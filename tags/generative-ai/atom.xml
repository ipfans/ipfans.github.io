<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Generative AI on ipfans's Blog</title><link>https://www.4async.com/tags/generative-ai/</link><description>Recent content in Generative AI on ipfans's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 28 Feb 2024 17:39:00 +0000</lastBuildDate><atom:link href="https://www.4async.com/tags/generative-ai/atom.xml" rel="self" type="application/rss+xml"/><item><title>使用 Ollama 快速部署本地开源大语言模型</title><link>https://www.4async.com/2024/02/deploy-local-llm-using-ollama/</link><pubDate>Wed, 28 Feb 2024 17:39:00 +0000</pubDate><guid>https://www.4async.com/2024/02/deploy-local-llm-using-ollama/</guid><description>&lt;img src="https://www.4async.com/2024/02/deploy-local-llm-using-ollama/cover.png" alt="Featured image of post 使用 Ollama 快速部署本地开源大语言模型" />如果你是第一次开始研究如何使用开源大语言模型(LLM)测试 GenerativeAI 时，一开始所有的信息一股脑在你的眼前，令人望而生畏。互联网上存在着来自许多不同来源的大量碎片信息，使得快速启动项目变得困难。
这篇文章的目标是提供一篇简易上手的文章，帮助你使用名为 Ollama 的模型的启动程序在本地设置和运行开源 AI 模型，当然，这些步骤也同样可以让你在你的服务器上运行它。
什么是 Ollama Ollama 是一款帮助我们在本地机器上运行大型语言模型的工具，它让你在本地测试 LLM 变得更加容易。Ollama 提供了本地命令行和 API 调用多种方式进行交互。如果你是想快速测试，那么 CLI 则是一个非常不错的方式；如果你是想开始一个产品，你也可以选择试用/api/chat进行开发一个应用。
Ollama 分为两个部分：一个是运行 LLM 的服务端，另外一个则是可选组件：用于和服务端和 LLM进行交互的 CLI。
安装 Ollama Ollama 官方提供了安装包用于 MacOS/Linux/Windows 下载。其中 Windows 支持截止到目前(2024/02/27)为止，还是预览支持，可能存在问题。因此这里我们演示使用 MacOS 安装这个应用。
下载安装 下载完成后，解压这个 zip 文件即可得到Ollama 的应用程序，你可以把它拖到系统的应用程序文件夹中，双击打开：
如果是第一次打开，会遇到安全提示，选择打开即可。
接下来需要安装 Ollama 的命令行工具，这样你就可以在命令行中访问LLM 了。
命令行安装 当然，如果你安装了[homebrew](https://brew.sh/)包管理工具，也可以使用下面的方式快速安装 Ollama：
brew install ollama 命令行方式安装时，需要额外注意我们无法像图形界面进行后台服务启动。你需要施工执行下面的命令启动Ollama 服务端：
ollama serve 试用 Ollama 现在你已经安装完成 Ollama 了，接下来让我们运行一个基础的模型试用一下。Ollama 支持了很多的开源模型，包括 Google 最新开源的 Gemma。不过这里我推荐使用Mistral 7B 模型。我们可以使用下面的命令快速安装对应模型：
ollama pull mistral 这会安装 Mistral 7B 模型，这是一个效率很高，效果不错的模型。这个命令通常会下载几个 G 到几十个 G 的模型到本地，请注意确保网速和本地磁盘空间足以安装模型。</description></item></channel></rss>