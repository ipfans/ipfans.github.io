[{"content":"一些在工作中经常使用的一些工具。如果有什么推荐的，也欢迎在评论中提供。这个列表后续会持续更新\nHTTP工具 curlie - httpie-like 工具，底层是curl lego - Let\u0026rsquo;s Encrypt证书工具 mkcert - 方便导入本地证书 paw.cloud - 原生的macOS HTTP调试工具，现在每年都会免费送，有兴趣关注一下 编译工具 go-task - 我用来替代Makefile，并无什么特殊必要，主要是不想写Makefile 代码质量 golangci-lint - 感觉无需介绍了，集成了很多实用工具，重复的就不列举了 pre-commit - 提交前检查代码质量，比如代码风格，缩进，空格等等 dcd - 查找代码中的重复代码 代码统计 scc - 高性能统计代码行数 图表工具 go-diagrams - 使用Go语言描述系统架构图 ndiag - 如果不想用Go描述，也可以选择用YAML描述系统架构 draft - 另外一个用YAML描述的工具，风格不一样 k8sviz - 你也可以从现成的K8s环境中生成系统架构图 archview - 通过代码中注释生成应用内部分层结构 go-plantuml - 根据Go代码生成结构体的PlantUML图 goplantuml - 另外一种生成PlantUML的工具 go-erd - 不想用PlantUML也可以换这种风格 asciiflow - 可以画ASCII图，ASCII图好处是可以放在代码里，如果你愿意的话 sequence - 嫌弃asciiflow比较原始，做时序图的时候可以用这个 mermaid-js - 方便集成在网页中，也可以导出成图片 kroki - 上面没提到的图类型的生成？看看这个 IaC pulumi - Terraform业界比较常用，不过要学习HCL比较蛋疼，我个人比较喜欢pulumi，可以选择自己的习惯的语言，tf-cdk目前还比较初级。 ","date":"2021-11-29T17:48:00Z","image":"https://www.4async.com/2021/11/awesome-toolkit/toolkit_hu46c2bb3a17f1f4690c7a5622eda6cdd0_215964_120x120_fill_box_smart1_3.png","permalink":"https://www.4async.com/2021/11/awesome-toolkit/","title":"一些实用工具列表"},{"content":"今天，Go的1.17版本终于正式发布，除了带来各种优化和新功能外，1.17正式在程序中提供了尝鲜的泛型支持，这一功能也是为1.18版本泛型正式实装做铺垫。意味着在6个月后，我们就可以正式使用泛型开发了。那在Go 1.18正式实装之前，我们在1.17版本中先尝鲜一下泛型的支持吧。\n泛型有什么作用？ 在使用Go没有泛型之前我们怎么实现针对多类型的逻辑实现的呢？有很多方法，比如说使用interface{}作为变量类型参数，在内部通过类型判断进入对应的处理逻辑；将类型转化为特定表现的鸭子类型，通过接口定义的方法实现逻辑整合；还有人专门编写了Go的函数代码生成工具，通过批量生成不同类型的相同实现函数代替手工实现等等。这些方法多多少少存在一些问题：使用了interface{}作为参数意味着放弃了编译时检查，作为强类型语言的一个优势就被抹掉了。同样，无论使用代码生成还是手工书写，一旦出现问题，意味着这些方法都需要重复生成或者进行批量修改，工作量反而变得更多了。\n在Go中引入泛型会给程序开发带来很多好处：通过泛型，可以针对多种类型编写一次代码，大大节省了编码时间。你可以充分应用编译器的编译检查，保证程序变量类型的可靠性。借助泛型，你可以减少代码的重复度，也不会出现一处出现问题需要修改多处地方的尴尬问题。这也让很多测试工作变得更简单，借助类型安全，你甚至可以少考虑很多的边缘情况。\nGo语言官方有详细的泛型提案文档可以在这里和这里查看详情。\n如何使用泛型 前面理论我们仅仅只做介绍，这次尝鲜还是以实践为主。让我们先从一个小例子开始。\n从简单的例子开始 让我们先从一个最简单的例子开始：\npackage main import ( \u0026#34;fmt\u0026#34; ) type Addable interface { type int, int8, int16, int32, int64, uint, uint8, uint16, uint32, uint64, uintptr, float32, float64, complex64, complex128, string } func add[T Addable](a, b T) T { return a + b } func main() { fmt.Println(add(1,2)) fmt.Println(add(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;)) } 这个函数可以实现任何需要使用+符号进行运算的类型，我们通过定义Addable类型，枚举了所有可能可以使用add方法的所有的类型。比如我们在main函数中就使用了int和string两种不同类型。\n但是如果这时我们使用简单的go run命令运行，会发现提示语法错误：\n$ go version go version go1.17 darwin/arm64 $ go run ~/main.go # command-line-arguments ../main.go:8:2: syntax error: unexpected type, expecting method or interface name ../main.go:15:6: missing function body ../main.go:15:9: syntax error: unexpected [, expecting ( 因为在Go 1.17中，泛型并未默认开启，你需要定义gcflags方式启用泛型：\n$ go run -gcflags=-G=3 ~/main.go 3 12 如果你觉得这种方式太过于复杂，每次都需要添加，也可以通过定义环境变量形式让每次都带此参数（不推荐，尤其是多版本环境时低版本Go中会报错）：\n$ export GOFLAGS=\u0026#34;-gcflags=-G=3\u0026#34; $ go run ~/main.go 3 12 在Go中，泛型可以做什么更多更复杂的事情吗？当然可以。除了最基础的算法实现以外，我们可以通过后面的几个场景看一下泛型可用的场景。\n实现类型安全的Map 在现实开发过程中，我们往往需要对slice中数据的每个值进行单独的处理，比如说需要对其中数值转换为平方值，在泛型中，我们可以抽取部分重复逻辑作为map函数：\npackage main import ( \u0026#34;fmt\u0026#34; ) func mapFunc[T any, M any](a []T, f func(T) M) []M { n := make([]M, len(a), cap(a)) for i, e := range a { n[i] = f(e) } return n } func main() { vi := []int{1,2,3,4,5,6} vs := mapFunc(vi, func(v int) int { return v*v }) fmt.Println(vs) } $ go run -gcflags=-G=3 main.go [1 4 9 16 25 36] 在这个例子中，我们定义了一个M类型，因此除了进行同样类型的转换外，也可以做不同类型的转换：\n- vs := mapFunc(vi, func(v int) int { - return v*v + vs := mapFunc(vi, func(v int) string { + return \u0026#34;\u0026lt;\u0026#34;+fmt.Sprint(v)+\u0026#34;\u0026gt;\u0026#34; $ go run -gcflags=-G=3 main.go [\u0026lt;1\u0026gt; \u0026lt;2\u0026gt; \u0026lt;3\u0026gt; \u0026lt;4\u0026gt; \u0026lt;5\u0026gt; \u0026lt;6\u0026gt;] 实现类型安全的Map/Filter 除了操作数据以外，我们通常还需要对数据进行筛选。在前面的例子上，我们可以通过实现filterFunc实现更好的通用逻辑：\npackage main import ( \u0026#34;crypto/rand\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;math/big\u0026#34; \u0026#34;strings\u0026#34; ) func mapFunc[T any, M any](a []T, f func(T) M) []M { n := make([]M, len(a), cap(a)) for i, e := range a { n[i] = f(e) } return n } func filterFunc[T any](a []T, f func(T) bool) []T { var n []T for _, e := range a { if f(e) { n = append(n, e) } } return n } func main() { vi := filterFunc( mapFunc([]int{1,2,3,4,5,6}, func(v int) int { return v*v }, ), func(v int) bool { return v \u0026lt; 40 }) fmt.Println(vi) vs := filterFunc( mapFunc([]string{\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;}, func(v string) string { // 需要使用crypto/rand增加随机性 n, _ :=rand.Int(rand.Reader, big.NewInt(5)) i := int(n.Int64())+1 return strings.Repeat(v, i) }, ), func(v string) bool { return len(v)\u0026gt;3 }) fmt.Println(vs) } $ go run -gcflags=-G=3 main.go [1 4 9 16 25 36] [aaaa dddd eeeee] 实现类型可靠的Worker Pool 除了上面这个例子，我们还可以通过泛型实现一个类型可靠的通用批量类型转换函数：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;sync\u0026#34; ) type T1 interface{} type T2 interface{} func ParallelMap(parallelism int, in []T1, f func(T1) (T2, error)) ([]T2, error) { var wg sync.WaitGroup defer wg.Wait() inc, outc, errc := make(chan T1), make(chan T2), make(chan error) donec := make(chan struct{}) defer close(donec) wg.Add(parallelism) for i := 0; i \u0026lt; parallelism; i++ { go func() { defer wg.Done() for x := range inc { y, err := f(x) if err != nil { select { case errc \u0026lt;- err: case \u0026lt;-donec: } return } select { case outc \u0026lt;- y: case \u0026lt;-donec: return } } select { case errc \u0026lt;- nil: case \u0026lt;-donec: } }() } go func() { for _, x := range in { inc \u0026lt;- x } close(inc) }() out := make([]T2, 0, len(in)) for rem := parallelism; rem \u0026gt; 0; { select { case err := \u0026lt;-errc: if err != nil { return nil, err } rem-- case y := \u0026lt;-outc: out = append(out, y) } } return out, nil } func main() { in := []T1{\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;5\u0026#34;} out, err := ParallelMap(4, in, func(x T1) (T2, error) { return strconv.Atoi(x.(string)) }) if err != nil { fmt.Println(\u0026#34;error: \u0026#34;, err) return } fmt.Println(out) in2 := []T1{1, 2, 3, 4, 5} out2, err := ParallelMap(4, in2, func(x T1) (T2, error) { return fmt.Sprintf(\u0026#34;\u0026lt;%d\u0026gt;\u0026#34;, x), nil }) if err != nil { fmt.Println(\u0026#34;error: \u0026#34;, err) return } fmt.Println(out2) } $ go run -gcflags=-G=3 main.go [3 5 2 4 1] [\u0026lt;1\u0026gt; \u0026lt;4\u0026gt; \u0026lt;5\u0026gt; \u0026lt;3\u0026gt; \u0026lt;2\u0026gt;] 其他应用 我们可以预见在Go 1.18版本中，多个标准库会被新增或者扩展，包括：类型定义库constraints，通用slice操作库slices，通用类型安全mapmaps等等。因为这些会进入标准库，大家可以先自行实现试用，真正线上使用建议等待标准库添加内容即可。\nGo泛型的实现原理 我们回归到最原始的例子快速看一下Go中是如何实现泛型的。为了方便分析，我们在所有func上添加go:noinline防止内联，然后编译程序进行分析。这里可能Go 1.17实现问题未能支持如go tool或go build -gcflags=all=-S之类的命令传递-G=3参数，因此这里我们选择第三方的反汇编工具看一下具体的实现：\n可以看到目前Go会根据类型将泛型展开成对应类型函数，这样也会小小的增加编译时间和编译后文件大小。因为我测试使用Apple Silicon平台，考虑大家可能不熟悉相关汇编，具体执行逻辑不再具体展示。\n其他注意事项 目前Go的泛型仍在开发过程中，即便在1.17beta到正式版过程中，很多泛型的corner case也正在完善过程中，比如在之前测试中我发现某些代码在beta版本无法正确编译，但是在RC中已可以正确编译。目前的泛型实现未必代表1.18版本中是相同的实现细节，甚至可能在1.18中提供更多的功能。同时，目前1.17泛型类型是无法在package中导出的，这导致在1.17版本中它的应用场景大大的受限。如果你仍有计划在某些场景中使用，我仍旧建议单元测试覆盖你使用的场景情况，防止出现版本迭代可能导致的问题。\n","date":"2021-08-17T17:48:00Z","image":"https://www.4async.com/2021/08/golang-117-generics/go_hu0a44c8ca350ed766a7653d0fae9076f0_60746_120x120_fill_box_smart1_3.png","permalink":"https://www.4async.com/2021/08/golang-117-generics/","title":"Go 1.17 泛型尝鲜"},{"content":" 作者：Miłosz Smółka 译者：Kevin 原文地址：https://threedots.tech/post/common-anti-patterns-in-go-web-applications/\n在我职业生涯的某个阶段，我对我所构建的软件不再感到兴奋。\n我最喜欢的工作内容是底层的细节和复杂的算法。在转到面向用户的应用开发之后，这些内容基本消失了。编程似乎是利用现有的库和工具把数据从一处移至另一处。到目前为止，我所学到的关于软件的知识不再那么有用了。\n让我们面对现实吧：大多数Web应用无法解决棘手的技术挑战。他们需要做到的是正确的对产品进行建模，并且比竞争对手更快的改进产品。\n这起初看起来似乎是那么的无聊，但是你很快会意识到实现这个目标比听起来要难。这是一项完全不同的挑战。即使它们技术上实现并没有那么复杂，但时解决它们会对产品产生巨大影响并且让人获得满足。\nWeb应用面临的最大挑战不是变成了一个无法维护的屎山，而是会减慢你的速度，让你的业务最终失败。\n这是他们如何在Go中发生和我是如何避免他们的。\n松耦合是关键 应用难以维护的一个重要原因是强耦合。\n在强耦合应用中，任何你尝试触动的东西都有可能产生意想不到的副作用。每次重构的尝试都会发现新的问题。最终，你决定字号从头重写整个项目。在一个快速增长的产品中，你是不可能冻结所有的开发任务去完成重写已经构建的应用的。而且你不能保证这次你把所有事都完成好。\n相比之下，松耦合应用保持了清晰的边界。他们允许更换一些损坏的部分不影响项目的其他部分。它们更容易构建和维护。但是，为什么他们如此罕见呢？\n微服务许诺了松耦合时实践，但是我们现在已经过了他们的炒作年代，而难以维护的应用仍旧存在。有些时候这反而变得更糟糕了：我们落入了分布式单体的陷阱，处理和之前相同的问题，而且还增加了网络开销。\n❌ 反模式：分布式单体 在你了解边界之前，不要将你的应用切分成为微服务。\n微服务并不会降低耦合，因为拆分服务的次数并不重要。重要的是如何连接各个服务。\n✅ 策略：松耦合 以实现松耦合的模块为目标。如何部署它们（作为模块化单体应用或微服务）是一个实现细节。\nDRY引入了耦合 强耦合十分常见，因为我们很早就学到了不要重复自己(Don\u0026rsquo;t Repeat Yourself, DRY)原则。\n简短的规则很容易被大家记住，但是简短的三个单词很难概括所有的细节。《程序员修炼之道: 从小工到专家》这本书提供了一个更长的版本：\n每条知识在系统中都必须有一个单一的、明确的、权威的表述。\n\u0026ldquo;每一条知识\u0026quot;这个说法相当极端。大多数编程困境的答案是看情况而定，DRY也不例外。\n当你让两个事物使用相同抽象的时候，你就引入了耦合。如果你严格遵循DRY原则，你就需要在这个抽象之前增加抽象。\n在Go中保持DRY 相比于其他现代语言，Go是清晰的，缺少很多特性，没有太多的语法糖来隐藏复杂性。\n我们习惯了捷径，所以一开始很难接受Go的冗长。就像我们已经开发出一种去寻找一种更加聪明的编写代码的方式的本能。\n最典型的例子就是错误处理。如果你有编写Go的经验，你会觉得下面的代码片段很自然\nif err != nil { return err } 但是对新手而言，一遍又一遍的重复这三行就是似乎在破坏DRY原则。他们经常想办法来规避这种样板方法，但是却没有什么好的结果。\n最终，大家都接受了Go的工作方式。它让你重复你自己，不过这并不是DRY告诉你的你要避免重复。\n单一数据模型带来的应用耦合 Go中有一个特性引入了强耦合，但会让你认为你自己在遵循DRY原则。这就是在一个结构体中使用多个标签。 这似乎是一个好主意，因为我们经常对不同的事物使用相似的模型。\n这里有一个流行的方式保存单个User模型的方法：\ntype User struct { ID int `json:\u0026#34;id\u0026#34; gorm:\u0026#34;autoIncrement primaryKey\u0026#34;` FirstName string `json:\u0026#34;first_name\u0026#34; validate:\u0026#34;required_without=LastName\u0026#34;` LastName string `json:\u0026#34;last_name\u0026#34; validate:\u0026#34;required_without=FirstName\u0026#34;` DisplayName string `json:\u0026#34;display_name\u0026#34;` Email string `json:\u0026#34;email,omitempty\u0026#34; gorm:\u0026#34;-\u0026#34;` Emails []Email `json:\u0026#34;emails\u0026#34; validate:\u0026#34;required,dive\u0026#34; gorm:\u0026#34;constraint:OnDelete:CASCADE\u0026#34;` PasswordHash string `json:\u0026#34;-\u0026#34;` LastIP string `json:\u0026#34;-\u0026#34;` CreatedAt *time.Time `json:\u0026#34;-\u0026#34;` UpdatedAt *time.Time `json:\u0026#34;-\u0026#34;` } type Email struct { ID int `json:\u0026#34;-\u0026#34; gorm:\u0026#34;primaryKey\u0026#34;` Address string `json:\u0026#34;address\u0026#34; validate:\u0026#34;required,email\u0026#34; gorm:\u0026#34;size:256;uniqueIndex\u0026#34;` Primary bool `json:\u0026#34;primary\u0026#34;` UserID int `json:\u0026#34;-\u0026#34;` } 完整代码：github.com/ThreeDotsLabs/go-web-app-antipatterns/01-coupling/01-tightly-coupled/internal/user.go\n这种方式通过很少的几行代码让你可以只维护单一的结构体实现功能。\n然而，在单一模型中拟合所有的内容需要很多技巧。API可能不需要保护某些字段，因此他们需要通过json:\u0026quot;-\u0026quot;隐藏起来。只有一个API使用到了Email字段，那么ORM就需要跳过它，并且需要在常规的JSON返回中通过omitempty进行隐藏。\n更重要的是，这个解决方案带来一个最糟糕的问题：API、存储和逻辑之间产生了强耦合。\n当你想要更新结构体中的任何东西时，你都不知道还有什么会发生修改。你会在更新数据库Schema或者更新验证规则时破坏API的约定。\n模型越复杂，你面临的问题就越多。\n比如，json标签表示JSON而不是HTTP。但是让你引入同样是格式化到JSON，但是格式与API不同的事件时会发生什么？你需要不停的添加hack让所有功能正常工作。\n最终，你的团队会避免对结构体的修改，因为在你动了结构体之后你无法确定会出现什么样的问题。\n❌ 反模式：单一模型 不要给一个模型多个责任。 每个结构字段不要使用多个标签。\n复制消除耦合 减少耦合最简单的方法是拆分模型。\n我们提取API使用的部分作为HTTP模型：\ntype CreateUserRequest struct { FirstName string `json:\u0026#34;first_name\u0026#34; validate:\u0026#34;required_without=LastName\u0026#34;` LastName string `json:\u0026#34;last_name\u0026#34; validate:\u0026#34;required_without=FirstName\u0026#34;` Email string `json:\u0026#34;email\u0026#34; validate:\u0026#34;required,email\u0026#34;` } type UpdateUserRequest struct { FirstName *string `json:\u0026#34;first_name\u0026#34; validate:\u0026#34;required_without=LastName\u0026#34;` LastName *string `json:\u0026#34;last_name\u0026#34; validate:\u0026#34;required_without=FirstName\u0026#34;` } type UserResponse struct { ID int `json:\u0026#34;id\u0026#34;` FirstName string `json:\u0026#34;first_name\u0026#34;` LastName string `json:\u0026#34;last_name\u0026#34;` DisplayName string `json:\u0026#34;display_name\u0026#34;` Emails []EmailResponse `json:\u0026#34;emails\u0026#34;` } type EmailResponse struct { Address string `json:\u0026#34;address\u0026#34;` Primary bool `json:\u0026#34;primary\u0026#34;` } 完整代码：github.com/ThreeDotsLabs/go-web-app-antipatterns/01-coupling/02-loosely-coupled/internal/http.go\n数据库相关部分作为存储模型：\ntype UserDBModel struct { ID int `gorm:\u0026#34;column:id;primaryKey\u0026#34;` FirstName string `gorm:\u0026#34;column:first_name\u0026#34;` LastName string `gorm:\u0026#34;column:last_name\u0026#34;` Emails []EmailDBModel `gorm:\u0026#34;foreignKey:UserID;constraint:OnDelete:CASCADE\u0026#34;` PasswordHash string `gorm:\u0026#34;column:password_hash\u0026#34;` LastIP string `gorm:\u0026#34;column:last_ip\u0026#34;` CreatedAt *time.Time `gorm:\u0026#34;column:created_at\u0026#34;` UpdatedAt *time.Time `gorm:\u0026#34;column:updated_at\u0026#34;` } type EmailDBModel struct { ID int `gorm:\u0026#34;column:id;primaryKey\u0026#34;` Address string `gorm:\u0026#34;column:address;size:256;uniqueIndex\u0026#34;` Primary bool `gorm:\u0026#34;column:primary\u0026#34;` UserID int `gorm:\u0026#34;column:user_id\u0026#34;` } 完整代码：github.com/ThreeDotsLabs/go-web-app-antipatterns/01-coupling/02-loosely-coupled/internal/db.go\n起初，看上去我们会在所有地方使用相同的User模型。现在，很明显我们过早的避免了重复。API和存储的结构很相似，但足够不同到需要拆分成不同的模型。\n在Web应用中，你API返回（读模型）与存储在数据库中的视图（写模型）并不相同。\n存储代码无需知道HTTP的模型，因此我们需要进行结构转换。\nfunc userResponseFromDBModel(u UserDBModel) UserResponse { var emails []EmailResponse for _, e := range u.Emails { emails = append(emails, emailResponseFromDBModel(e)) } return UserResponse{ ID: u.ID, FirstName: u.FirstName, LastName: u.LastName, DisplayName: displayName(u.FirstName, u.LastName), Emails: emails, } } func emailResponseFromDBModel(e EmailDBModel) EmailResponse { return EmailResponse{ Address: e.Address, Primary: e.Primary, } } func userDBModelFromCreateRequest(r CreateUserRequest) UserDBModel { return UserDBModel{ FirstName: r.FirstName, LastName: r.LastName, Emails: []EmailDBModel{ { Address: r.Email, }, }, } } 完整代码： github.com/ThreeDotsLabs/go-web-app-antipatterns/01-coupling/02-loosely-coupled/internal/http.go\n这就是所有你需要的代码：将一种类型映射到另一种类型的函数。编写这种平淡无奇的代码可能看起来十分无聊，但是它对解耦至关重要。\n创建一个使用序列化或者reflect实现用于映射结构体的通用解决方案看上去十分诱人。请抵制它。编写模版比调试映射的边缘情况会更节省时间和精力。 简单的函数对团队中每个人都更容易理解。魔法转换器会在一段时间后变得难以理解，即使对你而言也是如此。\n✅ 策略：模型单一责任。 通过使用单独的模型来实现松耦合。编写简单明了的函数用以在它们之间进行转换。\n如果你害怕太多的重复，请考虑一下最坏的情况。如果你最终多了几个随着应用程序增长不变的结构，你可以将它们合并回一个。与强耦合代码相比，修复重复代码是微不足道的。\n生成模版 如果你担心手写这些所有代码，有一个管用的方法可以规避。使用可以为你生成模版的库。\n你可以生成诸如：\n由OpenAPI定义的HTTP模型和路由（oapi-codegen或者其他库）。 由SQL schema定义的数据库模型和相关代码（[sqlboiler](https://github.com/volatiletech/sqlboiler和其他ORM）。 通过Protobuf文件生成gPRC模型。 生成的代码可以提供强类型保护，因此你无需在通用函数中传递interface{}类型的数据。你可以保证编译时检查的同时无需手写代码。\n下面是生成的模型的例子。\n// PostUserRequest defines model for PostUserRequest. type PostUserRequest struct { // E-mail Email string `json:\u0026#34;email\u0026#34;` // First name FirstName string `json:\u0026#34;first_name\u0026#34;` // Last name LastName string `json:\u0026#34;last_name\u0026#34;` } // UserResponse defines model for UserResponse. type UserResponse struct { DisplayName string `json:\u0026#34;display_name\u0026#34;` Emails []EmailResponse `json:\u0026#34;emails\u0026#34;` FirstName string `json:\u0026#34;first_name\u0026#34;` Id int `json:\u0026#34;id\u0026#34;` LastName string `json:\u0026#34;last_name\u0026#34;` } 完整代码：github.com/ThreeDotsLabs/go-web-app-antipatterns/01-coupling/03-loosely-coupled-generated/internal/http_types.go\ntype User struct { ID int64 `boil:\u0026#34;id\u0026#34; json:\u0026#34;id\u0026#34; toml:\u0026#34;id\u0026#34; yaml:\u0026#34;id\u0026#34;` FirstName string `boil:\u0026#34;first_name\u0026#34; json:\u0026#34;first_name\u0026#34; toml:\u0026#34;first_name\u0026#34; yaml:\u0026#34;first_name\u0026#34;` LastName string `boil:\u0026#34;last_name\u0026#34; json:\u0026#34;last_name\u0026#34; toml:\u0026#34;last_name\u0026#34; yaml:\u0026#34;last_name\u0026#34;` PasswordHash null.String `boil:\u0026#34;password_hash\u0026#34; json:\u0026#34;password_hash,omitempty\u0026#34; toml:\u0026#34;password_hash\u0026#34; yaml:\u0026#34;password_hash,omitempty\u0026#34;` LastIP null.String `boil:\u0026#34;last_ip\u0026#34; json:\u0026#34;last_ip,omitempty\u0026#34; toml:\u0026#34;last_ip\u0026#34; yaml:\u0026#34;last_ip,omitempty\u0026#34;` CreatedAt null.Time `boil:\u0026#34;created_at\u0026#34; json:\u0026#34;created_at,omitempty\u0026#34; toml:\u0026#34;created_at\u0026#34; yaml:\u0026#34;created_at,omitempty\u0026#34;` UpdatedAt null.Time `boil:\u0026#34;updated_at\u0026#34; json:\u0026#34;updated_at,omitempty\u0026#34; toml:\u0026#34;updated_at\u0026#34; yaml:\u0026#34;updated_at,omitempty\u0026#34;` R *userR `boil:\u0026#34;-\u0026#34; json:\u0026#34;-\u0026#34; toml:\u0026#34;-\u0026#34; yaml:\u0026#34;-\u0026#34;` L userL `boil:\u0026#34;-\u0026#34; json:\u0026#34;-\u0026#34; toml:\u0026#34;-\u0026#34; yaml:\u0026#34;-\u0026#34;` } 完整代码：github.com/ThreeDotsLabs/go-web-app-antipatterns/01-coupling/03-loosely-coupled-generated/models/users.go\n有时你可能会想要编写代码生成工具。这其实并不难，结果需要是每个人都可以阅读和理解的常规Go代码。常见的替代方案是使用reflect，但是这很难掌握和调试。当然，首先要考虑的是付出的努力是否值得。在大多数情况下，手写代码已经足够快了。\n✅ 策略：生成重复工作的部分 生成的代码为你提供强类型和编译时安全性。选择它而不是reflect。\n不要过度使用库 只将生成的代码用于它应该做的事情。如果你想避免手工编写模版，但仍需要保留一些专用的模型。不要以单一模型反模式作为结束。\n当你想遵循DRY原则时，很容易落入这个陷阱。\n例如，sqlc和sqlboiler都是从SQL查询中生成代码。sqlc允许在生成的模型上添加JSON标签，甚至允许让你选择camelCase还是snake_case。sqlboiler在所有模型上默认添加了json，toml和yaml标签。这显然是不是让用户仅仅把这个模型仅用于存储。\n看一下sqlc的issue列表，我发现很多开发者要求更多的灵活性，比如重命名生成的字段和整个跳过一些JSON字段。有人甚至提到他们需要某种在REST API中隐藏某些敏感字段的方法。\n所有这些都是鼓励在单一模型中担负更多职责。它可以让你写更少的代码，但是请务必考虑这种耦合是否值得。\n同样，需要注意结构体标签中隐藏的魔法，比如，gorm中提供的权限功能：\ntype User struct { Name string `gorm:\u0026#34;\u0026lt;-:create\u0026#34;` // 允许读取和创建 Name string `gorm:\u0026#34;\u0026lt;-:update\u0026#34;` // 允许读取和更新 Name string `gorm:\u0026#34;\u0026lt;-\u0026#34;` // 允许读取和写入（创建和更新） Name string `gorm:\u0026#34;\u0026lt;-:false\u0026#34;` // 允许读取，禁用写权限 Name string `gorm:\u0026#34;-\u0026gt;\u0026#34;` // 只读模式（除非单独配置，否则禁用写权限） Name string `gorm:\u0026#34;-\u0026gt;;\u0026lt;-:create\u0026#34;` // 允许读取和创建 Name string `gorm:\u0026#34;-\u0026gt;:false;\u0026lt;-:create\u0026#34;` // 只允许创建（禁止从数据库中读取） Name string `gorm:\u0026#34;-\u0026#34;` // 在读写模型时忽略这个字段 } 完整代码：gorm.io/docs/models.html#Field-Level-Permission\n你同样可以使用[validator]库进行复杂的比较，比如参考其他字段：\ntype User { FirstName string `validate:\u0026#34;required_without=LastName\u0026#34;` LastName string `validate:\u0026#34;required_without=FirstName\u0026#34;` } 它为你节省了一点编写代码的时间，但是这意味着你放弃了编译期检查。在结构体标签中很容易出现错别字，在验证和权限等敏感地方使用这种会带来风险。这同样也会让很多不那么熟悉库的语法糖的人感到困扰。\n我并不是指摘这些提到的库，他们都有自己的用途。但是这些示例展示了我们如何把DRY做到极致，这样我们就不用编写更多的代码了。\n❌ 反模式：选择魔法来节省编写代码的时间 不要过度使用库以避免冗余。\n避免隐式标签名 大多数库不要求标签必须存在，此时会默认使用字段名称。\n在重构项目时，有人可能会重命名字段，但是他没有想过编辑API返回或者数据模型。如果没有标签，这就会导致API约定或者数据存储过程被破坏。\n请始终填写所有标间，即使你必须兼容同一名称两次，这并不违反DRY原则。\n译者注：其实Go之前有个类似proposal提过在1.16中简化这一写法，但是后面发现存在一些问题被回滚了。\n❌反模式：省略结构标签 如果库使用它们，则不要跳过结构标签。\ntype Email struct { ID int `gorm:\u0026#34;primaryKey\u0026#34;` Address string `gorm:\u0026#34;size:256;uniqueIndex\u0026#34;` Primary bool UserID int } ✅战术：显式结构标签 始终填充结构标签，即使字段名称相同。\ntype Email struct { ID int `gorm:\u0026#34;column:id;primaryKey\u0026#34;` Address string `gorm:\u0026#34;column:address;size:256;uniqueIndex\u0026#34;` Primary bool `gorm:\u0026#34;column:primary\u0026#34;` UserID int `gorm:\u0026#34;column:user_id\u0026#34;` } 将逻辑与实现细节分开 通过生成模型将API与存储解耦是一个好的开始。但是，我们仍旧需要保留在HTTP处理中的验证过程。\ntype createRequest struct { Email string `validate:\u0026#34;required,email\u0026#34;` FirstName string `validate:\u0026#34;required_without=LastName\u0026#34;` LastName string `validate:\u0026#34;required_without=FirstName\u0026#34;` } validate := validator.New() err = validate.Struct(createRequest(postUserRequest)) if err != nil { log.Println(err) w.WriteHeader(http.StatusBadRequest) return } 完整代码：github.com/ThreeDotsLabs/go-web-app-antipatterns/01-coupling/03-loosely-coupled-generated/internal/http.go\n验证是你能在大多数Web应用中可以找到的业务逻辑中的一环。通常，他们会更加复杂，比如：\n仅在特定情况下显示字段 检查权限 取决于角色而隐藏字段 计算价格 根据几个因素采取行动 将逻辑和实现细节混在一起（比如将他们放在HTTP handler中）是一种快速交付MVP的方法。但是这也引入了最坏的技术债务。这就是为什么你会被供应商锁定，为什么你需要不停的添加hack拉支持新功能。\n❌ 反模式：将逻辑和细节混在一起 不要将你的应用程序逻辑与实现细节混在一起。\n商业逻辑需要单独的层。更改实现（数据库引擎、HTTP 库、基础架构、Pub/Sub 等）应是可能的，而无需对逻辑部件进行任何更改。\n你做这种分离并不是因为你想要更改数据库引擎，这种情况很少会发生。但是，关注点的分离可以让你的代码更容易理解和修改。你知道你在修改什么，并且有没有副作用。 这样就很难在关键部分引入bug。\n要分离应用层，我们需要添加额外的模型和映射。\ntype User struct { id int firstName string lastName string emails []Emailf } func NewUser(firstName string, lastName string, emailAddress string) (User, error) { if firstName == \u0026#34;\u0026#34; \u0026amp;\u0026amp; lastName == \u0026#34;\u0026#34; { return User{}, ErrNameRequired } email, err := NewEmail(emailAddress, true) if err != nil { return User{}, err } return User{ firstName: firstName, lastName: lastName, emails: []Email{email}, }, nil } type Email struct { address string primary bool } func NewEmail(address string, primary bool) (Email, error) { if address == \u0026#34;\u0026#34; { return Email{}, ErrEmailRequired } // A naive validation to make the example short, but you get the idea if !strings.Contains(address, \u0026#34;@\u0026#34;) { return Email{}, ErrInvalidEmail } return Email{ address: address, primary: primary, }, nil } 完整代码：github.com/ThreeDotsLabs/go-web-app-antipatterns/01-coupling/04-loosely-coupled-app-layer/internal/user.go\n这就是当我需要更新业务逻辑时我需要修改的代码。这明显很无聊，但是我知道我修改了什么。\n当我们添加另一个API（比如gRPC）或者外部系统（如Pub/Sub）时，我们需要同样的工作。每个部分都是用单独的模型，我们在应用层映射转换它们。\n因为应用层维护了所有的验证和其他商业逻辑，他会让我们无论是使用HTTP还是gRPC API都没什么区别。API只是应用的入口。\n✅ 策略：应用层 将产品最重要的代码划分成单独的层。\n上面的代码片段都来自于同一个代码库，并且实现了经典的用户域。所有示例都暴露相同的API并且使用相同的测试套件。\n以下是他们的比较：\n强耦合 松耦合 基于代码生成的松耦合 松耦合的应用层 耦合 强 中 中 弱 模版 手动 手动 生成 生成 代码行数 292 345 298 408 生成的代码 0 0 2154 2154 标准的Go项目结构 如果你看过这个仓库，你会发现在每个例子中只有一个包。\nGo目前没有官方的目录组织结构，不过你可以找到很多微服务例子或者REST模版仓库建议你如何拆分。他们通常有精心设计的目录机构，有人甚至提到他们遵循了简洁架构或者六边形架构。\n我一般第一件确认的事情是如何存储模型的。大多数情况下，他们使用了JSON和数据库标签混合的结构体。\n这是一种错觉：包看起来进行了很好的切分，但实际上他们仍旧通过一个模型被紧密的耦合在了一起。新人用来学习的很多流行例子中，这些问题也很常见。\n具有讽刺意味的是，标准的Go项目结构仍旧在社区中继续被讨论，然而模型耦合反模式却很常见。如果你的应用程序的类型耦合了，任何目录的组织形式都不会改变什么。\n在查看示例结构时，请记住他们可能是为另外一种不同类型的应用程序设计的。对于开源的基础设施工具、Web应用后端和标准库而言，没有一种方法同时对他们有效。\n包分层和切分微服务的问题非常类似。重要的不是如何划分他们，而是他们彼此之间如何连接。\n当你专注于松耦合时，目录结构就会变得更加清晰。你可以将实现细节与业务逻辑区分开。你把相互引用的事物分组，并将不互相引用的事物拆分开。\n在我准备的示例中，我可以轻松的将HTTP相关的代码和数据库相关的代码拆分至单独的包中。这会避免命名空间的污染。模型之间已经没有耦合，所以这些操作就变成了具体的细节。\n❌反模式：过度考虑目录结构 不要通过分割目录来启动项目。不管你怎么做，这是一个惯例。 你不太可能在编写代码之前把事情做好。\n✅策略：松耦合代码 重要的部分不是目录结构，而是包和结构是如何进行相互引用的。\n保持简单化 假设你想要创建一个用户，这个用户有一个ID字段。最简单的方法可以看起来像这样：\ntype User struct { ID string `validate:\u0026#34;required,len=32\u0026#34;` } func (u User) Validate() error { return validate.Struct(u) } 这段代码能够正常工作。但是，你无法判断该结构在任何时候都是正确的。你依靠一些额外东西来调用验证并处理错误。\n另一种方法是采用良好的旧式封装。\ntype User struct { id UserID } type UserID struct { id string } func NewUserID(id string) (UserID, error) { if id == \u0026#34;\u0026#34; { return UserID{}, ErrEmptyID } if len(id) != 32 { return UserID{}, ErrInvalidLength } return UserID{ id: id, }, nil } func (u UserID) String() string { return u.id } 此片段更清晰、更冗长。如果你创建了一个新的UserID并且没有收到任何错误，你可以确定创建是成功的。此外，你可以轻松地将错误映射到 API 的正确响应。\n无论你选择哪种方法，你都需要对用户ID的基本复杂性进行建模。从纯粹的实现的角度来看，将 ID 保持在字符串中是最简单的解决方案。\nGo应该很简单，但这并不意味着你应该只使用原始类型。对于复杂的行为，请使用反映产品工作方式的代码。否则，你最终会获得一个简化的模型。\n❌反模式：过度简化 不要用琐碎的代码来模拟复杂的行为。\n✅策略：编写明确的代码 保证代码是明确的，即使它很冗长。 使用封装来确保你的结构始终处于有效状态。\n即使所有字段都未导出，也可以在包外创建空结构。唯一要做的是在接受UserID作为参数时，你需要检查一下合法性。 你可以使用if id == UserID{}或编写专门的IsZero()方法来进行。\n从数据库Schema开始 假设我们需要添加一个用户创建和加入团队的功能。\n按照关系型方法，我们需要添加一个teams表和另外一个将用户和它进行关联的表。我们叫它membership。\n按照关系方法，我们将添加一张桌子和另一张加入它的表格。让我们称之为。teamsusersmembership\n我们已经有了UserStorage，所以很自然的添加两个新的结构体：TeamStorage和MembershipStorage。他们会为每个表格提供CRUD方法。\n添加新团队的代码可能看起来是这个样子的：\nfunc CreateTeam(teamName string, ownerID int) error { teamID, err := teamStorage.Create(teamName) if err != nil { return err } return membershipStorage.Create(teamID, ownerID, MemberRoleOwner) } 这种方法有一个问题：我们没有在事务中创建团队和成员记录。如果出现问题，我们可能最终拥有一支没有分配所有者的团队。\n首先想到的第一个解决方案是在方法之间传递事务。\nfunc CreateTeam(teamName string, ownerID int) error { tx, err := db.Begin() if err != nil { return err } defer func() { if err == nil { err = tx.Commit() } else { rollbackErr := tx.Rollback() if rollbackErr != nil { log.Error(\u0026#34;Rollback failed:\u0026#34;, err) } } }() teamID, err := teamStorage.Create(tx, teamName) if err != nil { return err } return membershipStorage.Create(tx, teamID, ownerID, MemberRoleOwner) } 但是，这样的话实现细节（事务处理）就会泄漏到了逻辑层。它通过基于defer的错误处理污染了一个可读的函数。\n下面是一个练习：考虑如何在文档数据库中对此进行建模。比如，我们可以将所有成员保留在团队文档中。\n在这种情况下，添加成员就可以在TeamStorage中完成，这样我们就不需要单独的MembershipStorage。但是切换数据库就变更了我们模型的假设，这不是很奇怪吗？\n现在很显然，我们通过引入\u0026quot;成员身份\u0026quot;概念泄露了实现细节。\u0026ldquo;创建新成员身份\u0026rdquo;，这只会困扰我们的销售或者客户服务同事。当你开始说一种不同于公司其他成员的语言时，这通常一个严重的危险信号。\n反模式❌：从数据库Schema开始 不要将模型建立在数据库模式的基础上。你最终会暴露实现细节。\nTeamStorage用于存储团队信息，但它不是与teams SQL表无关。这是关于我们产品的团队概念。\n每个人都明白创建一个团队需要一个所有者，我们可以为此暴露一个方法。这个方法会将所有的查询放在一个事务中执行查询。\nteamStorage.Create(teamName, ownerID, MemberRoleOwner) 同样，我们也可以有一个加入团队的方法。\nteamStorage.JoinTeam(teamID, memberID, MemberRoleGuest) membership表依旧存在，但是实现细节被隐藏在TeamStorage中。\n✅策略：从领域开始 你的存储方法应遵循产品的行为。不要他们的事务细节。\n你的网络应用程序不是单纯的CRUD 教程通常都是以\u0026quot;简单的 CRUD\u0026quot;为特色，因此它们似乎是任何 Web 应用的基础构建模块。这是个虚无缥缈的传说。如果你所有的产品需要的是CRUD，你就是在浪费时间和金钱从零开始构建。\n框架和无代码工具使得启动 CRUD 变得更容易，但我们仍然向开发人员支付构建自定义软件的费用。即使是 GitHub Copilot 也不知道你的产品除了模版之外是如何工作的。\n正是特殊的规则和奇怪的细节使你的应用程序与众不同。这不是你分散在四个CRUD操作之上的逻辑。 它是你销售的产品的核心。\n在 MVP 阶段，从 CRUD 开始快速构建可工作版本是很诱人的。但这就像使用电子表格而不是专用软件。一开始，你会获得类似的效果，但每个新功能都需要更多的hack。\n反模式❌：从 CRUD 开始 不要围绕四个 CRUD 操作的想法来设计你的应用程序。\n✅策略：了解你的领域 花时间了解你的产品是如何工作的，并在代码中建模。\n我描述的许多策略都是众所周知的模式背后的想法：\nSOLID中的单一责任原则（每个模型只有一项责任）。 简洁架构（松耦合的包，将逻辑与实现细节隔离）。 CQRS（使用不同的读取模型和写入模型）。 有些甚至接近域驱动设计：\n值对象（始终保持结构处于有效状态）。 聚合和仓库（无论数据库表的数量如何，都以事务方式保存领域对象）。 无处不在的语言（使用每个人都能理解的语言）。 这些模式似乎大多与企业级应用相关。但其中大多数是简单明了的核心思想，比如本文中的策略。它们同样适用于处理复杂的业务行为Web应用程序、\n你不需要阅读大量书籍或复制其他语言的代码来遵循这些模式。你可以通过实践检验的技术编写惯用的Go代码。如果你想了解更多关于他们的内容，可以看看我们的免费电子书。\n如果你想在反模式仓库中添加更多示例以及有关主题，请在评论中告知我们。\n","date":"2021-08-13T14:09:00Z","image":"https://www.4async.com/2020/02/2020-02-16-moving-towards-domain-driven-design-in-go/cover_huea63e6370dcf0c375755d886a5d0b9c6_97708_1600x0_resize_q75_box.jpg","permalink":"https://www.4async.com/2021/08/common-anti-patterns-in-go-web-applications/","title":"Go Web应用中常见的反模式"},{"content":"基本概念 事件建模（Event Modeling）是一种描述系统的方法，展示信息如何随时间变化的例子。具体来说，这种方式省略了瞬息万变的细节，而着眼于在任何特定的时间点上的持久化存储和用户所见数据的变化。这些时间轴上的事件，构成了对系统的描述。\n近年来，很多系统使用事件通过事件存储数据库或者使用特定方式使用常规数据库构建了状态和信息传播的模块。然而，大多数方法仍然依赖于通过SQL数据库、文档数据库或者其他技术实现严格意义上的当前时间点信息的视图。\n对很多系统而言，特别是对于非小型系统而言，随着系统复杂性的增加，变更成本将会随着时间的推移难度指数级上升。与现有的设计和建模方式对比，事件建模可以在短时间内创建一个基础蓝图，将返工工作量降到最低。\n从过去谈起 讲故事自古以来就是人类能够将知识传递给后代的方法，它在很大程度上依赖于我们如何存储记忆-无论是逻辑的、视觉的、听觉的还是其他的。这一点很重要，因为这与信息系统的构建方式有相似之处。 用具体的例子说明某物应该如何工作是一种常见的方式。这种方式可以在软件开发的成功实践中看到，如行为驱动开发。这种方式很有效，因为我们通过故事来沟通更有效。将它和讲故事联系在一起，是一种保持社会信息的方式。我们的大脑是为它而建的，而不是为流程图和其他格式而建的。 而事件建模模型就是遵循这种讲故事模式而建立的产品建模方式。\n事件建模模式 时间线是最好描述故事主线的方式，对我们的系统而言，时间线也是描述我们系统核心部分概念的重要组成部分。我们可以通过在一条时间线上，系统从开始到结束，在没有分支情况下应该做什么方式展示我们系统的一部分功能。这就是一个典型的事件模型的组成。我们可以用这种方式跟踪所有UI界面中字段值如何存储和如何展示的。比如在上面的示例图中，我们使用了3种不同模块的内容和传统的线框模型就展示了整个系统的模型。但是简单性是我们重要的一个目标，因此我们只依赖于4种模式构建这种模型图。\n保持简单性 当我们想采用某些做法或流程来帮助彼此理解和沟通时，它与个人为熟练掌握这些方法而进行的学习量成反比。换句话说，如果我们可以更快的掌握一个名叫X的方法时，我们就可以更好的通过这种方式进行知识分享和互动；反之，无论这种方法多么好，昂贵的学习成本总会搞砸一切。\n当一本书是团队中的必读书目时，每个人都会说他们读过；但事实上只有一半的人会真正读过；这些真正读过的人中一半的人会声称他们理解了这本书；而这些声称理解的人中只有一半的人真正的理解了这本书；而这些真正理解了这本书的人只有一半的人能够使用它。\n这就是为什么使用3个模块和基于2个想法的4种模式进行事件建模。因为这只需要几分钟就可以将所有的东西向所有人解释清楚。其他的学习则可以在实践中进行。即便理解出现了不足和错误，也可以很快在实践中得到纠正。\n事件 假设我们想为连锁酒店设计一个酒店网站，让我们的客户可以在线预订房间，并让我们安排清洁和任何其他酒店问题。 我们可以显示在该业务的年度时间线上存储了哪些事件。 我们可以假装我们已经有了这个系统，然后问自己随着时间的推移存储了哪些事件。\n线框图 让我们看一下在图片的最上面的部分的第一个模块。为了让讲故事这个事情更加可视化，我们可以在顶部显示功能的线框图或者网页模拟图。这也可以被具象化为具体的泳道图，以方便不同的人（也可以是系统）与我们的系统进行互动。这里一些自动化的内容可以用齿轮表示，同时说明系统正在做什么。通过这种方式，我们可以非常容易的展示出系统需要实现的功能列表，执行流程和项目完成标记。这里的图是示例了一个酒店的预订、支付和通知系统的过程，我们可以重点关注一下所有相关高亮显示的内容。\n借助这个模块，我们可以很方便的和设计师一起沟通设计系统，当然，这里需要注意在设计中，两个重要的内容需要添加到整个设计中：用户所拥有的权限和用户可以获取的信息。\n命令 大多数信息系统必须给用户一种影响系统的状态的能力，而这种能力就是命令。在我们的例子中，我们必须允许房间预订改变系统状态，这样我们就不会发生超额预订情况。当那个人在未来的预订日期到达时，他们就有一个为他们准备的房间。\n改变系统状态的意图会被封装在一个命令中。相对于简单地将表单数据保存到数据库中的一个表中，这可以让我们以非技术性的方式来显示意图，同时允许任何实现 - 尽管我们可以看到某些方法更具优势。\n从UI和UX的角度来看，这就是一个\u0026quot;命令响应式用户界面\u0026quot;，对帮助制作可组合的UI大有帮助。使用这种模式，从技术和商业的角度来看，交易的界限就更清楚了。以酒店入住为例，酒店的客人要么登记成功，要么没有。\n当命令成功的前提条件有细微的差别时，它们会在\u0026quot;Given-When-Then\u0026quot;风格的描述方式中进行阐述。这种方式也是行为测试模式惯用的描述方式，也是一种成功的讲述故事的方式。实际执行过程中，可能会有几个这样的故事来说明一个命令如何能成功执行和不能成功执行。\n这里我们可以用一个例子来描述一下：\nGiven：我们已经注册并添加了一个支付方式\nWhen：我们试图预订一个房间\nThen：一个房间被预订了\n这种描述方式也通常被叫做“安排、行动、断言”，在UI/UX的世界中，也被称为“情景、统计、价值”。 在图中我们也可以发现，所有的命令都是用蓝色进行标记的。\n视图（或者叫读模型） 任何信息系统的一个重要能力是将系统中保存的状态告知用户。我们的酒店客人需要知道他们感兴趣的某些类型的房间在哪一天可以入住。这通常有很多种情况，需要支持信息系统的多个模型。\n随着这些新事件的存储，系统中的视图也会一直变化。在我们的酒店系统中，这个日历视图随着影响库存的新事件的发生而被更新。其他视图中清洁团队可以在客户离店事件存储后在其他视图中看到房间已经可以被清理了。\n指定视图的行为方式与我们指定接受命令的方式非常相似，但有一处不同。视图是被动的，并且不能在事件被存储到系统中之后撤销事件。\n举个例子：\nGiven：酒店设置了12间海景房，海景房从4月4日到12日被预订\nThen：日历上应该显示除4月4日到12日以外的所有海景房的日期\n从上面图中我们也可以注意到，所有的读都是用绿色进行标记的。\n集成 我们刚刚介绍了描述大多数系统所需的 4 种模式中的前 2 种模式。 系统可以从其他系统获取信息并且将信息发送到其他系统。 强迫这 2 个模式成为前 2 个模式的扩展并共享相同的空间是很诱人的选择。 然而这会让交流变得更加困难，因为它们没有人类可见的方面，并且需要一些更高级别的模式。\n翻译 当我们有一个为我们提供信息的外部系统时，将这些信息转换成我们自己系统中的更熟悉的形式会很有帮助。 在我们的酒店系统中，如果选择让我们的清洁人员反应更加灵敏，我们可以从客人的 GPS 坐标中获取事件。 我们不想使用经度和纬度对作为事件来指定我们系统中的先决条件。 我们宁愿选择对我们有意义的活动，例如“客人离开酒店”、“客人回到酒店房间”。\n通常，翻译很简单，可以表示为从外部事件中获取信息的视图。如果我们不将它们用作测试的任何“Given”部分，则它们存储在该视图模型中的值仅在我们的状态更改测试中的命令参数中表示。\n自动化 我们的系统一般都需要与外部服务进行通信。当我们酒店的客人在退房时支付住宿费用时，我们的系统会调用付款处理程序。我们可以通过系统中某个处理程序的“待办事项列表”的概念来了解这是如何发生的。这个待办事项列表显示了我们需要完成的任务。例如，我们的处理程序会不时查看该列表（可能是几毫秒或几天）并向外部系统发送命令以处理付款。然后将来自外部系统的回复转换为我们存储回系统中的事件。通过这种方式，我们将系统中使用的构建块保留为对我们有意义的东西。\n我们通过在具有线框的蓝图顶部放置一个处理程序来展示这一点。这表明这些东西在屏幕上无法看到但在后台发生。由于需要完成后台任务，用户可能期望旋转图标提示延迟。此规范的形式为“Given：要执行的任务的视图，When：每个项目启动此命令时，Then：这些事件会返回。”\n实际上，这些可以通过许多不同的方式实现，例如队列、响应式或实时构造。它们甚至可能是我们使用的待办事项列表。这里的目标是传达我们的系统在需要影响外部时是如何与之沟通的。\n落地实现 事件建模分7个步骤进行。我们已经解释了最终的目标。因此，让我们倒退到开头，展示如何建立起蓝图。\n头脑风暴 我们让某人解释项目的目标和其他信息。然后，参与者设想系统的外观和行为是什么样的。他们写下所有他们能想象到的发生的事件。在这里，我们只需要明确改变了状态的事件。这些所有的事件一定要意味着某些状态的变更，如果没有发生变更，那么他们就不是事件。比如，有人会说出\u0026quot;客人查看了房间的可用日历\u0026quot;，那么这个就不是事件。\n阐述剧情 现在的任务是用这些事件创造一个合理的故事。因此，它们被排列成一条线（也就是时间线），每个人都检查这条线，以了解这条时间线作为按顺序发生的事件是合理的。\n制作故事 接下来，需要制作故事的框线图或模拟图，用可视化帮助初学者上手。更重要的是，每个领域都必须被表示出来，这样系统的蓝图就可以从用户的角度来表示信息的来源和目的地。\nUX并行 线框图一般被放在蓝图的顶部。如果有超过一个用户的话，它们可以被分成独立的泳道，以显示每个用户看到的东西。这里需要注意我们需将系统的每个变化转化为蓝图的单独一列，因此屏幕不能出现垂直重叠的情况。不同的排序可以显示在各种详情中。如果它是系统的核心或非常重要的交互，则需要在蓝图中添加备用的工作流程。这是显示部分的最后一步，但如果有帮助的话，可以提前完成。\n鉴别输入 从前面的部分我们看到，我们需要展示我们如何让用户改变系统的状态。这通常是我们引入蓝框的步骤。每次由于用户的操作而存储了一个事件时，我们通过一个命令将其与UI联系起来，如果是一个网页程序，这个命令展示了我们从屏幕上获取或隐式的从客户端状态得到的信息。\n鉴别输出 再次回头看一下我们的蓝图目标，我们现在必须通过视图（又称读模型）将通过存储事件积累的信息链接到用户界面。这些可能是像酒店系统中的日历视图一样的东西，当用户想要预订房间时，它将显示房间的可用性。\n应用康威定律 现在我们知道了所有的信息是如何进出我们的系统的，我们可以开始研究将事件本身组织成泳道。我们需要这样做，以便让系统的模块作为一组独立团队可以拥有的自主部分而存在。这允许专业化发生在我们控制的水平上，而不是脱离团队的组成。详见梅尔康威的康威法则。\n精心设计场景 每个工作流程步骤都与一个命令或一个视图/读模型相联系。前面已经解释了这些规范。我们如何制定它们，仍然是与所有参与者合作进行。一个Give-When-Then或Given-Then可以在快速的构建的同时被多个角色进行审查的。这使得传统上由专门的产品负责人以文本格式单独完成的用户故事写作，可以在极短的时间内以可视化的方式协作完成。更为关键的是，每个定义都与一个命令或视图相关联。\n完整性校验 事件模型应该有每一个字段的说明。所有的信息都必须有一个起点和一个终点。事件会促进这种转换，并包含必要的字段来实现这一目标。这种严格性是使用该技术的最大好处。\n这方面的一个变种是，我们不做这个最后的检查，在某些情况下，这也是可以的。\n项目管理 如果完成的足够良好，我们制作出的是一组包含每个工作流程所有场景定义的足够小的组合。他们可以直接转化为开发人员的开发内容和测试内容，并且只与约定部分和相邻工作流程相耦合。\n明确的约定 许多项目管理、业务和协调问题都因为我们对工作流的某一步骤开始时的信息形状和完成时的数据形状做了明确的约定而得到缓解。这些事前和事后的条件是允许工作在相对孤立的情况下完成，并在之后按照设计与相邻的步骤结合在一起。\n平坦的成本曲线 使用事件建模的最大影响是可以建立一个平坦的成本曲线。这是由于建立每个工作流程步骤不受其他工作流程开发的影响。需要理解的一件事是，如果一个工作流程步骤使用了相同的命令或视图，那么它就可以被认为是在事件模型上的重复。\n这一点的影响是非常深远的，因为它是将软件开发重新变为工程实践的原因。它使创建一个信息系统的工作像建造房子一样。功能可以以任何顺序被创建。传统的开发不能依靠估算，因为功能是否在项目的早期和后期所需的工作量会随着系统的复杂度而被影响。重新确定工作的优先次序使得以前的估算变得不可靠。\n真正的完成就是要正确的完成 当一个工作流程步骤被实现后，实现任何其他工作流程步骤的行为都不会导致需要重新调整这个已经完成的工作流程步骤。这也是恒定特征成本曲线能够实现的原因。\n没有估算的估算 有了恒定成本曲线，一个团队的开发可以简单地通过许多功能在一段时间内进行衡量。这是一种相对公平的方式，可以凭经验确定团队的速度。这些数字在之后可以被用来确定未来项目的范围、进度和成本。\n关于测试驱动开发 这是行业内采用敏捷实践带来的影响，就像将创可贴贴在缺乏设计的核心问题上。因为现在每个团队需求的范围都是按工作流程步骤进行的，所以TDD的重构步骤不会影响事件模型中的其他工作流程步骤。当我们没有事件模型的时候，重构就可以不受限制地进行，以前完成的工作片段就需要进行调整。已经完成的工作越多，在我们构建解决方案的过程中，每一个新增加的工作都会被审查和调整。\n项目外包 恒定成本曲线给了制定固定成本项目的机会。一旦有了一个团队的开发速度，你就可以明确团队的软件的成本。有了这个数字，你现在就可以为你愿意给外包厂商完成的每个工作流程步骤的报酬定价了。\n产出保证 由于每个工作流程步骤都受到保护，不受其他工作流程步骤的影响，因此，任何缺陷都将由谁为它们提供非计费工作来保证。因此，如果一个外包商只是为了快速完成更多的可计费项目而做得不好，他们就必须让接下来的工作时间专门用来修复之前已经完成的工作的缺陷。这就平衡了他们的有效工资率，因为他们不是在做新的应交付的工作。\n这可以通过不同的绩效检查点来提供这些指标，在员工参与的过程中长期进行下去。\n由于有效薪酬可以根据个人的能力进行自我调整，这也是一种让新员工入职的方式，在他们处于试用阶段的时候，对他们进行公平的支付。这种从合同到雇佣的过程消除了技术职位的主观和基本无效的面试过程。\n确定优先顺序 在不改变每个项目的估计成本的情况下，在时间表上移动工作，即哪些步骤要先实施。这确保了工作的优先次序对总成本没有影响。恒定成本曲线的要求是允许这种重新确定优先次序的 \u0026ldquo;敏捷性\u0026rdquo;。\n变更管理 当计划改变时，我们只需调整事件模型。这通常是通过直接复制当前的内容并进行调整来实现的。这样，我们就可以直观的看到差异在哪里。如果一个事件中加入了新的信息，这就构成了创建该事件的工作流程的新版本。视图也一样。如果这些还没有开发，那么这就不会改变我们的计划。如果它们已经开发了，它们就会给我们的开发计划增加另一个实现单元，它被认为是对原有流程的一种替换。围绕这一点还有一些规则。最终的结果是为变更管理提供一个明确的指南。\n安全性 有了事件模型，该方案可以准确地显示敏感数据在哪里，同样重要的是，敏感数据何时越界。在传统的审计中，与员工面谈的次数很耗时，而且有可能错过重要的领域。当应用程序有一个事件模型作为参考时，安全问题就能够得到最有效的保障。\n遗留系统 大部分组织所面临的情况是大多数系统已经存在了。处理一个因为复杂和缺乏理解而难以管理的系统的主要方法是重写它或在它运行时重构它。这两种做法的代价都是非常昂贵的。\n其实还有第三个风险较小的选择。冻结旧系统。在适当的支持下，团队可以不再考虑变更现有的系统。相反，处理错误和增加新的功能是作为sidecar方式来完成的。\n可以从旧系统的数据库中收集事件，并制作该状态的视图\u0026ndash;采用前面描述的翻译模式。用户行动的Y型阀重定向可以在侧面解决方案中增加新的功能。一个修复了一个错误的例子（注意，我们采用了外部集成模式，并对旧系统进行了扩展，增加了资料图片，如图。）\n这种模式可以让团队停止将精力浪费在现有的系统上，并能够通过实现事件模型的模式解除遗留系统对快速交付价值的阻碍。\n","date":"2021-07-06T11:41:00Z","image":"https://www.4async.com/2021/07/introducing-event-modeling/event-modeling-tutorial_hucf7c873ea7f3b2f557ada81568c30cdb_177241_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.4async.com/2021/07/introducing-event-modeling/","title":"什么是事件建模Event Modeling?"},{"content":"在上一篇文章中，吐槽了拖延症的危害，因此这次我来分享一下我最新推送到dapr的最新的一个新的绑定组件，通过这个来看一下如何实现自己的绑定组件。\n文中提到的PR可以在 dapr/components-contrib#872 查看对应的具体代码。\n什么是 dapr 的绑定组件？ 在dapr中，绑定是用于使用外部系统功能（比如事件或者接口）的扩展组件。它的优势在于：\n免除连接到消息传递系统(如队列和消息总线)并进行轮询的复杂性； 聚焦于业务逻辑，而不是如何与系统交互的实现细节； 使代码不受 SDK 或库的跟踪； 处理重试和故障恢复； 在运行时在绑定之间切换； 构建具有特定于环境的绑定的可移植应用程序，不需要进行代码更改； 在官方文档中，也提到了一个具体的例子：以twilio发送短信为例，一般开发过程中应用程序需要依赖Twilio SDK才可以实现功能，但是借助绑定组件，你可以将SDK的绑定转移至dapr程序领域内，在本身应用程序中不再绑定对应的SDK，不用担心未来SDK过期或者变更带来的重复工作（仅需要更新dapr即可）。\n根据订阅的进出方向，绑定组件也分为输入绑定和输出绑定。这些绑定均是通过yaml文件描述类型和元数据，通过HTTP/gRPC进行调用。\n如何实现自己的绑定组件？ 官方例子中提供了一个基础的介绍，上一节中我们也提到了在程序中，根据进出方向可以把绑定组件分为输出绑定和输入绑定。你可以通过官方教程中的例子提供查看：\n在这个例子用，你可以看到，根据方向吧dapr发布消息到Kafka作为输出组件，把Kafka读取消息到dapr作为输入组件。\n绑定的声明yaml文件的规范则如下：\napiVersion: dapr.io/v1alpha1 kind: Component metadata: name: \u0026lt;NAME\u0026gt; namespace: \u0026lt;NAMESPACE\u0026gt; spec: type: bindings.\u0026lt;TYPE\u0026gt; version: v1 metadata: - name: \u0026lt;NAME\u0026gt; value: \u0026lt;VALUE\u0026gt; 其中metadata.name则是绑定置名称，spec.metadata.name和spec.metadata.value则是配置的属性和对应值。这个值我们可以通过实现接口InputBinding或者OutputBinding实现输入绑定和输出绑定.\ntype InputBinding interface { Init(metadata Metadata) error Read(handler func(*ReadResponse) ([]byte, error)) error } type OutputBinding interface { Init(metadata Metadata) error Invoke(req *InvokeRequest) (*InvokeResponse, error) Operations() []OperationKind } 接下来需要实现一个生成对象的方法，比如说我们需要实现一个飞书推送Webhook的绑定组件，则可以：\ntype FeishuWebhook struct { logger logger.Logger // 这个是dapr的日志接口，输出日志可以使用这个 settings Settings // 具体配置信息 httpClient *http.Client // 请求HTTP } func NewFeishuWebhook(l logger.Logger) *FeishuWebhook { // See guidance on proper HTTP client settings here: // https://medium.com/@nate510/don-t-use-go-s-default-http-client-4804cb19f779 dialer := \u0026amp;net.Dialer{ //nolint:exhaustivestruct Timeout: 5 * time.Second, } var netTransport = \u0026amp;http.Transport{ //nolint:exhaustivestruct DialContext: dialer.DialContext, TLSHandshakeTimeout: 5 * time.Second, } httpClient := \u0026amp;http.Client{ //nolint:exhaustivestruct Timeout: defaultHTTPClientTimeout, Transport: netTransport, } return \u0026amp;FeishuWebhook{ //nolint:exhaustivestruct logger: l, httpClient: httpClient, } } 在绑定组件生命周期中，init会在初始化是进行调用，传入我们之前在yaml文件中定义的配置文件，因此我们可以在这里实现具体的配置获取：\ntype Settings struct { URL string `mapstructure:\u0026#34;url\u0026#34;` // Webhook地址 Secret string `mapstructure:\u0026#34;secret\u0026#34;` // 加密消息的密钥 } func (s *Settings) Decode(in interface{}) error { return config.Decode(in, s) } func (s *Settings) Validate() error { if s.ID == \u0026#34;\u0026#34; { return errors.New(\u0026#34;webhook error: missing webhook id\u0026#34;) } if s.URL == \u0026#34;\u0026#34; { return errors.New(\u0026#34;webhook error: missing webhook url\u0026#34;) } return nil } // Init performs metadata parsing func (t *FeishuWebhook) Init(metadata bindings.Metadata) error { var err error if err = t.settings.Decode(metadata.Properties); err != nil { return fmt.Errorf(\u0026#34;feishu configuration error: %w\u0026#34;, err) } if err = t.settings.Validate(); err != nil { return fmt.Errorf(\u0026#34;feishu configuration error: %w\u0026#34;, err) } return nil } 接下来在具体的Read(handler func(*ReadResponse) ([]byte, error)) error和Invoke(req *InvokeRequest) (*InvokeResponse, error)方法中，我们可以分别实现读取传入消息和发送传出消息的功能。代码根据不同实现而不同，这里就不做区分了。\n总结 我这里根据实际的绑定组件例子介绍了给dapr实现绑定组件的功能，是不是手痒希望试试了？快点加入到贡献大军吧XD。\n","date":"2021-05-15T18:38:00Z","image":"https://vip1.loli.io/2022/03/26/uJdVWNeXC6Z3Hq8.png","permalink":"https://www.4async.com/2021/05/building-your-own-dapr-binding/","title":"构建属于你自己的dapr绑定组件"},{"content":"写在最前: 这篇文章其实算是马后炮了，因为一直拖延症的问题，顺带过了一个五一假期，结果发现已经有社区贡献者提供了Consul的服务发现实现，于是本来写了一半的文章只能进行调整了。拖延症害人啊！几个草稿的文章看来要尽快赶出来了🤦‍♂️\n在上一篇文章中，我其实遗留了一个问题：如何定义dapr的服务发现呢？其实在后面阅读dapr的源码之后也前一篇文章的评论中提到了答案：目前dapr提供了内置两种服务发现模式：K8s模式和用于独立部署的mDNS模式。mDNS模式在某些网络环境下可能存在问题（比如跨机房），不过没有关系，dapr同时提供了可扩展能力，可以通过定义自主的服务发现能力扩展dapr的边界。\n从 NameResolution 到 Resolver 接口 在 pkg/components/nameresolution/registry.go 文件中，dapr定义了一个 NameResolution 结构体用于服务注册和发现：\ntype ( // NameResolution is a name resolution component definition. NameResolution struct { Name string FactoryMethod func() nr.Resolver } // Registry handles registering and creating name resolution components. Registry interface { Register(components ...NameResolution) Create(name, version string) (nr.Resolver, error) } nameResolutionRegistry struct { resolvers map[string]func() nr.Resolver } ) 其中真正的服务解析则是依靠 components-contrib 中实现了 Resolver 接口的具体实现执行。\n// Resolver is the interface of name resolver. type Resolver interface { // Init initializes name resolver. Init(metadata Metadata) error // ResolveID resolves name to address. ResolveID(req ResolveRequest) (string, error) } 其中 Init 会在 Runtime 初始化时被调用，而 ResolveID 则会在服务查询时调用。比如在 pkg/messaging/direct_messaging.go 的方法 getRemoteApp 中进行服务的解析：\nfunc (d *directMessaging) getRemoteApp(appID string) (remoteApp, error) { id, namespace, err := d.requestAppIDAndNamespace(appID) if err != nil { return remoteApp{}, err } request := nr.ResolveRequest{ID: id, Namespace: namespace, Port: d.grpcPort} address, err := d.resolver.ResolveID(request) if err != nil { return remoteApp{}, err } return remoteApp{ namespace: namespace, id: id, address: address, }, nil } 当然，事实上这样并不完全足够，还需要把这个服务注册放入dapr支持的服务中去：\nruntime.WithNameResolutions( nr_loader.New(\u0026#34;mdns\u0026#34;, func() nr.Resolver { return nr_mdns.NewResolver(logContrib) }), nr_loader.New(\u0026#34;kubernetes\u0026#34;, func() nr.Resolver { return nr_kubernetes.NewResolver(logContrib) }), nr_loader.New(\u0026#34;consul\u0026#34;, func() nr.Resolver { return nr_consul.NewResolver(logContrib) }), ), 上面的这些是设定的dpar目前支持的一些服务发现功能，而我们之前服务发现也一直使用的 Consul 实现，已经满足我们的需求了\u0026hellip;😓拖延症害人啊！\n从原理到实现 上面提到了我们需要实现一个 Resolver 接口的实现，我们可以预见到我们大概会需要这么一个东西:\ntype resolver struct {} // NewResolver creates Consul name resolver. func NewResolver() nr.Resolver // Init will configure component. It will also register service or validate client connection based on config func (r *resolver) Init(metadata nr.Metadata) error // ResolveID resolves name to address via consul func (r *resolver) ResolveID(req nr.ResolveRequest) (string, error) 接下来就需要一个 client *consul.Client 去实现服务的注册：\ntype resolver struct { client *consul.Client } func (r *resolver) Init(metadata nr.Metadata) error { // ... if err := r.client.Agent().ServiceRegister(regData); err != nil { return fmt.Errorf(\u0026#34;failed to register consul service: %w\u0026#34;, err) } // ... } 注册服务完成后，在调用具体的服务时，我们需要获取具体的服务地址：\nfunc (r *resolver) ResolveID(req nr.ResolveRequest) (string, error) { // ... services, _, err := r.client.Health().Service(req.ID, \u0026#34;\u0026#34;, true, cfg.QueryOptions) // ... } 当然上面的演示代码只是部分核心功能代码，如果需要拓展更多的实现细节内容，需要查看具体的官方接收社区贡献的实现：components-contrib/nameresolution/consul\n","date":"2021-05-08T18:38:00Z","image":"https://vip1.loli.io/2022/03/26/uJdVWNeXC6Z3Hq8.png","permalink":"https://www.4async.com/2021/05/building-your-own-dapr-service-discovery/","title":"构建属于你自己的dapr服务发现"},{"content":"前一段时间一直关注的dapr正式发布了v1.0版本(实际上本文发布时还更新了v1.0.1)，代表dapr在某些程度上进入稳定状态，可以尝试在实际中进行运用。作为我一直关注的项目，在第一时间中进行了尝试，并试图引入实际项目中，本文则是针对这些的一些先期测试内容.\n什么是dapr？ dapr最早是由微软开源的(不愧是你)，一个可移植的、事件驱动的程序运行时，它使任何开发者都能轻松地构建运行在云和边缘的弹性、无状态/有状态的应用程序，并且可以灵活支持多种开发语言。换而言之，在我看来，dapr可以作为一个serverless落地方案看待和处理，对程序而言，只关注提供的store和消息队列接口，无需关心架构层面更多内容。\n不过在官方的示例教程中，使用的环境为容器环境部署和管理dapr。实际上，除了在容器环境或者容器集群环境下，dapr可以配置为在本地机器上以自托管模式运行。\n本地安装 dapr安装可以通过官方的dapr-cli实现，dapr-cli可以通过一键安装命令快速安装：\n# wget -q https://raw.githubusercontent.com/dapr/cli/master/install/install.sh -O - | /bin/bash Your system is linux_amd64 Dapr CLI is detected: main: line 86: 43656 Segmentation fault $DAPR_CLI_FILE --version Reinstalling Dapr CLI - /usr/local/bin/dapr... Getting the latest Dapr CLI... Installing v1.0.0 Dapr CLI... Downloading https://github.com/dapr/cli/releases/download/v1.0.0/dapr_linux_amd64.tar.gz ... dapr installed into /usr/local/bin successfully. CLI version: 1.0.0 Runtime version: n/a To get started with Dapr, please visit https://docs.dapr.io/getting-started/ 可以通过输入dapr命令确认dapr-cli程序是否被正常安装成功。\n接下来使用dapr-cli安装所有的runtime等应用。\n# dapr init --slim ⌛ Making the jump to hyperspace... ↘ Downloading binaries and setting up components... Dapr runtime installed to /root/.dapr/bin, you may run the following to add it to your path if you want to run daprd directly: export PATH=$PATH:/root/.dapr/bin ✅ Downloaded binaries and completed components set up. ℹ️ daprd binary has been installed to /root/.dapr/bin. ℹ️ placement binary has been installed to /root/.dapr/bin. ✅ Success! Dapr is up and running. To get started, go here: https://aka.ms/dapr-getting-started # dapr --version CLI version: 1.0.0 Runtime version: 1.0.1 在官方文档中，如果选择使用init命令初始化，dapr-cli将会自动尝试使用容器环境管理相关程序，只有添加--slim参数才会选择本地化运行。更多用法可以参考dapr help init帮助。默认程序相关内容会安装在$HOME/.dapr目录下，这里因为我为了简便使用了root用户，因此程序命令所在目录为/root/.dapr/bin，共安装了如下命令：\n# ls ~/.dapr/bin daprd dashboard placement web 从文件名可以看出来daprd是deamon进程，dashboard就是管理面板，placement是用于管理actor分布方案和密钥范围的工具。官方文档中提到在安装后会使用Reids作为默认的存储和pub/sub组件，但是我实际安装下来其实是并没没有的，不知道是不是文档有些过期导致的。这时如果按照官方文档的例子进行操作启动程序并尝试在存储中保存数据，则会出现报错的情况：\n// 第一个session中执行： # dapr run --app-id myapp --dapr-http-port 3500 // 第二个session中执行： # curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;[{ \u0026#34;key\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Bruce Wayne\u0026#34;}]\u0026#39; http://localhost:3500/v1.0/state/statestore {\u0026#34;errorCode\u0026#34;:\u0026#34;ERR_STATE_STORES_NOT_CONFIGURED\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;state store is not configured\u0026#34;} 不过实际上添加组件在dapr中也是比较简单的，可以通过在$HOME/.dapr/components下添加对应yaml文件实现。\n添加Redis作为组件 我们可以在官方文档中找到一个Redis组件配置模版，可以快速使用：\n# redis-store.yml apiVersion: dapr.io/v1alpha1 kind: Component metadata: name: redis-store namespace: default spec: type: state.redis version: v1 metadata: - name: redisHost value: 127.0.0.1:6379 - name: redisPassword value: \u0026#34;\u0026#34; 当然我们也可以使用Redis Stream功能做pub/sub功能，虽然这个功能已经GA，但是介于Redis Stream的特点，你需要谨慎使用这个功能，这里只是因为是演示所以无所谓：\n# redis-pubsub.yml apiVersion: dapr.io/v1alpha1 kind: Component metadata: name: redis-pubsub namespace: default spec: type: pubsub.redis version: v1 metadata: - name: redisHost value: 127.0.0.1:6379 - name: redisPassword value: \u0026#34;\u0026#34; - name: consumerID value: \u0026#34;myGroup\u0026#34; 这里我们定义了一个store名叫做redis-store，所以我们要把上面的命令修改一下：\n# curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;[{ \u0026#34;key\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Bruce Wayne\u0026#34;}]\u0026#39; http://localhost:3500/v1.0/state/redis-store // 获取存储内容 # curl http://localhost:3500/v1.0/state/redis-store/name \u0026#34;Bruce Wayne\u0026#34; 同时也可以通过redis-cli获取Redis中存储的内容：\n# redis-cli 127.0.0.1:6379\u0026gt; keys * 1) \u0026#34;myapp||name\u0026#34; 127.0.0.1:6379\u0026gt; hgetall \u0026#34;myapp||name\u0026#34; 1) \u0026#34;data\u0026#34; 2) \u0026#34;\\\u0026#34;Bruce Wayne\\\u0026#34;\u0026#34; 3) \u0026#34;version\u0026#34; 4) \u0026#34;1\u0026#34; 我们在添加Redis作为存储时还额外添加了Redis支持发布/订阅功能，这个功能如何实现呢？这里可能就需要编写额外程序实现了。我们这里采用官方的例子进行。订阅在dapr中有两种形式，一种是采用yaml声明组件形式，另外一种则可以通过编写代码形式实现。当然第一种方式和第二种方式互有优劣，前者更适合无缝集成，后者方便开发控制。这里为了演示直观性直接采用了编写代码方式实现。\npackage main import ( \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func main() { r := gin.Default() r.GET(\u0026#34;/dapr/subscribe\u0026#34;, func(ctx *gin.Context) { ctx.JSON(http.StatusOK, []map[string]string{ { \u0026#34;pubsubname\u0026#34;: \u0026#34;redis-pubsub\u0026#34;, \u0026#34;topic\u0026#34;: \u0026#34;deathStarStatus\u0026#34;, \u0026#34;route\u0026#34;: \u0026#34;dsstatus\u0026#34;, }, }) }) r.POST(\u0026#34;/dsstatus\u0026#34;, func(c *gin.Context) { b, _ := io.ReadAll(c.Request.Body) defer c.Request.Body.Close() log.Println(string(b)) c.JSON(http.StatusOK, map[string]interface{}{\u0026#34;success\u0026#34;: true}) }) r.Run(\u0026#34;127.0.0.1:5000\u0026#34;) } 使用如下命令启动编译后的daprdemo，注意指定文件名时需要填写路径或者在$PATH中:\n# dapr --app-id subapp --app-port 5000 run ~/daprdemo 在程序启动日志中我们可以看到dapr会尝试访问一些默认的endpoint读取可能的配置：\nINFO[0000] application discovered on port 5000 app_id=subapp instance=127.0.0.1 scope=dapr.runtime type=log ver=1.0.1 == APP == [GIN] 2021/03/11 - 10:45:02 | 404 | 949ns | 127.0.0.1 | GET \u0026#34;/dapr/config\u0026#34; INFO[0000] application configuration loaded app_id=subapp instance=127.0.0.1 scope=dapr.runtime type=log ver=1.0.1 INFO[0000] actor runtime started. actor idle timeout: 1h0m0s. actor scan interval: 30s app_id=subapp instance=127.0.0.1 scope=dapr.runtime.actor type=log ver=1.0.1 == APP == [GIN] 2021/03/11 - 10:45:02 | 200 | 540.891µs | 127.0.0.1 | GET \u0026#34;/dapr/subscribe\u0026#34; INFO[0000] app is subscribed to the following topics: [deathStarStatus] through pubsub=redis-pubsub app_id=subapp instance=127.0.0.1 scope=dapr.runtime type=log ver=1.0.1 WARN[0000] redis streams: BUSYGROUP Consumer Group name already exists app_id=subapp instance=127.0.0.1 scope=dapr.contrib type=log ver=1.0.1 INFO[0000] dapr initialized. Status: Running. Init Elapsed 49.674504ms app_id=subapp instance=127.0.0.1 scope=dapr.runtime type=log ver=1.0.1 接下来我们尝试使用dapr-cli对我们之前启动的myapp发送消息：\ndapr publish --publish-app-id myapp --pubsub redis-pubsub --topic deathStarStatus --data \u0026#39;{\u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;}\u0026#39; 在程序日志中获取到的输出为：\n== APP == [GIN] 2021/03/11 - 10:45:05 | 200 | 122.15µs | 127.0.0.1 | POST \u0026#34;/dsstatus\u0026#34; == APP == 2021/03/11 10:45:05 {\u0026#34;id\u0026#34;:\u0026#34;9c237504-7cab-4a13-8582-92d9130fd016\u0026#34;,\u0026#34;source\u0026#34;:\u0026#34;myapp\u0026#34;,\u0026#34;pubsubname\u0026#34;:\u0026#34;redis-pubsub\u0026#34;,\u0026#34;traceid\u0026#34;:\u0026#34;00-fba669a086f84650e882e3cadc55082c-ea466c080e359e68-00\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;status\u0026#34;:\u0026#34;completed\u0026#34;},\u0026#34;specversion\u0026#34;:\u0026#34;1.0\u0026#34;,\u0026#34;datacontenttype\u0026#34;:\u0026#34;application/json\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;com.dapr.event.sent\u0026#34;,\u0026#34;topic\u0026#34;:\u0026#34;deathStarStatus\u0026#34;} 当然，除了pub/sub方式，我们也可以借助dapr提供的路由功能，直接进行服务调用：\n# curl http://127.0.0.1:3500/v1.0/invoke/subapp/method/dsstatus -X POST {\u0026#34;success\u0026#34;:true} 其他的组件功能则可以参考官方文档中描述进行配置即可。\n总结 dapr是一个功能强大的serverless运行时，除了上面提到的面向消息和请求存储的功能以外，还可以控制程序的HTTP请求与gRPC请求等等。除了这些功能外，还包含了服务的管理，还有可观测性支持等功能，是一个非常有潜力的运行时选择。\n","date":"2021-03-11T18:38:00Z","image":"https://vip1.loli.io/2022/03/26/uJdVWNeXC6Z3Hq8.png","permalink":"https://www.4async.com/2021/03/2021-03-11-running-dapr-without-container/","title":"在非容器(集群)环境下运行dapr"},{"content":"在我吐槽了无数次之后，NATS JetStream终于结束了beta阶段正式进入RC阶段。终于官方也在最近刚刚正式回复了我正式版本在处理几个问题之后就会正式发布。那么在这个比较重要的NATS-Server特性发布之际聊一下NATS产品本身区别和新特性的使用，还有更多的潜在的区别。\n概念区分：NATS-Server / NATS Streaming Server / NATS JetStream NATS-Server NATS-Server（或者叫nats）是一个开源的、云原生的、高性能的消息传递系统，是NATS的最基础的产品。它的核心是一个发布/订阅（Pub/Sub）系统，客户端可以在不同集群中的服务间nats进行通讯，而不需要关注具体的消息在哪个服务上。换而言之，客户端可以在任意一个集群的服务端上发布消息，同时在任意集群客户端上尝试读取消息。在官方与其他同类消息队列产品功能对比中，我们也可以管窥一下产品的功能列表。nats支持多流多服务进行pub/sub，负载均衡，保障消息最多/最少一次送达，多租户和用户认证等功能。虽然看上去优点很多，但是nats不是一个应用很广的消息队列的重要原因是，它缺少了一些对消息队列而言很最重要的产品特性，比如持久化支持，比如消息确保一次送达。这意味着当你的消息发送出去之后，你的消息是在处理过程中可能丢失的，甚至是可能送达不到的。\nNATS Streaming Server NATS Streaming Server（或者叫stan）是用于尝试解决上面提到的nats的已存在问题的。stan添加了持久化功能和消息送达策略支持。stan中自带了nats服务端，但是在使用过程中，nats和stan不能进行混用。在官方文档中，是这么描述stan和nats之间的关系的：\nNATS客户端和NATS Streaming Server客户端之间不能相互交换数据。也就是说，如果一个NATS Streaming Server客户端在foo上发布消息，在同一主题上订阅的NATS客户端将不会收到消息。NATS Streaming Server消息是由protobuf组成的NATS消息。NATS Streaming Server要向生产者发送ACK，并接收消费者的ACK。如果与NATS客户端自由交换消息，就会引起问题。\nstan的具体架构如下图：\n但是stan虽然提供了持久化和消息传递策略支持，但是在架构设计上却出现了问题，导致在最开始设计时遗留了很多问题，比如当你确定stan集群是固定的不能无限制水平扩容(#999)，比如不支持多租户功能(#1122)，比如客户端无法主动拉取消息只能被推送等等\nNATS JetStream NATS JetStream（或者叫JetStream）是NATS基于Raft算法实现的最新的架构设计尝试解决上述问题的新方案。在区别于原有的stan功能上，提供了新的持久化功能和消息送达策略，同时支持水平扩容。同时，新的JetStream也为大消息做了一些优化，不再将这特性功能作为nats的客户端存在而是嵌入NATS Server中作为其中的一个功能存在。也就是说，如果在对这几项技术进行选择时，JetStream应该是最应该被选择的方案。更多详细情况具体可以查看官方的指导文档。\nNATS JetStream使用 理论介绍过了，接下来说说实际使用的事情。现在JetStream还是RC阶段，\n编译和启动客户端 下载nats-server源码，解压之后执行：\ncd nats-server-master go build -o nats-server -ldflags=\u0026#34;-s -w -buildid=\u0026#34; . ./nats-server -js 这样就可以启动一个支持JetStream功能的服务端了。\n[54738] 2021/03/02 18:27:02.605197 [INF] Starting nats-server [54738] 2021/03/02 18:27:02.605236 [INF] Version: 2.2.0-RC.2 [54738] 2021/03/02 18:27:02.605238 [INF] Git: [not set] [54738] 2021/03/02 18:27:02.605239 [INF] Name: NAFWRGQTR2CHMBIKNPE6R3ZTW2BWV2FWPAZREMHI24IYVM6FVHMVIYLQ [54738] 2021/03/02 18:27:02.605240 [INF] ID: NAFWRGQTR2CHMBIKNPE6R3ZTW2BWV2FWPAZREMHI24IYVM6FVHMVIYLQ [54738] 2021/03/02 18:27:02.605658 [INF] Starting JetStream [54738] 2021/03/02 18:27:02.606062 [WRN] _ ___ _____ ___ _____ ___ ___ _ __ __ [54738] 2021/03/02 18:27:02.606076 [WRN] _ | | __|_ _/ __|_ _| _ \\ __| /_\\ | \\/ | [54738] 2021/03/02 18:27:02.606077 [WRN] | || | _| | | \\__ \\ | | | / _| / _ \\| |\\/| | [54738] 2021/03/02 18:27:02.606078 [WRN] \\__/|___| |_| |___/ |_| |_|_\\___/_/ \\_\\_| |_| [54738] 2021/03/02 18:27:02.606079 [WRN] [54738] 2021/03/02 18:27:02.606080 [WRN] _ _ [54738] 2021/03/02 18:27:02.606081 [WRN] | |__ ___| |_ __ _ [54738] 2021/03/02 18:27:02.606082 [WRN] | \u0026#39;_ \\/ -_) _/ _` | [54738] 2021/03/02 18:27:02.606083 [WRN] |_.__/\\___|\\__\\__,_| [54738] 2021/03/02 18:27:02.606084 [WRN] [54738] 2021/03/02 18:27:02.606084 [WRN] JetStream is a Beta feature [54738] 2021/03/02 18:27:02.606085 [WRN] https://github.com/nats-io/jetstream [54738] 2021/03/02 18:27:02.606092 [INF] [54738] 2021/03/02 18:27:02.606093 [INF] ----------- JETSTREAM ----------- [54738] 2021/03/02 18:27:02.606095 [INF] Max Memory: 12.00 GB [54738] 2021/03/02 18:27:02.606096 [INF] Max Storage: 35.79 GB [54738] 2021/03/02 18:27:02.606098 [INF] Store Directory: \u0026#34;/var/folders/5s/8rczg1gs4wb59y9s22nc3f_r0000gn/T/nats/jetstream\u0026#34; [54738] 2021/03/02 18:27:02.606099 [INF] --------------------------------- [54738] 2021/03/02 18:27:02.606399 [INF] Listening for client connections on 0.0.0.0:4222 [54738] 2021/03/02 18:27:02.606512 [INF] Server is ready 编写JetStream DEMO 接下来我们看一下如何使用JetStream进行消息发布/订阅功能：\n// 连接到nats的服务器 conn, err := nats.Connect(\u0026#34;nats://127.0.0.1:4222\u0026#34;) if err != nil { log.Panic(err) } defer conn.Close() // 初始化JetStream功能 js, err := conn.JetStream() if err != nil { log.Panic(err) } // 判断Stream是否存在，如果不存在，那么需要创建这个Stream，否则会导致pub/sub失败 stream, err := js.StreamInfo(streamName) if err != nil { log.Println(err) // 如果不存在，这里会有报错 } if stream == nil { log.Printf(\u0026#34;creating stream %q and subject %q\u0026#34;, streamName, subject) _, err = js.AddStream(\u0026amp;nats.StreamConfig{ Name: streamName, Subjects: []string{subject}, MaxAge: 3 * 24 * time.Hour, }) if err != nil { log.Panicln(err) } } // 订阅消息 sub, err := js.Subscribe(subject, cbHandle, nats.AckAll(), nats.DeliverNew()) if err != nil { log.Panic(err) return } defer sub.Unsubscribe() // 发送消息 js.Publish(subject, []byte(\u0026#34;Hello World! \u0026#34;+time.Now().Format(time.RFC3339))) time.Sleep(5 * time.Second) log.Println(\u0026#34;Exiting...\u0026#34;) 在这个例子中，有个值得注意的功能需要额外强调一下，在Subscribe消息时，我们在这里特别声明了nats.DeliverNew()这个选项。如果不声明，则默认为nats.DeliverAll()；除了这两个参数，还有一个nats.DeliverLast()参数，这分别对应了3种开始订阅时的方式：默认方式nats.DeliverAll()是会读取有效生命周期内的所有消息，甚至包含已被处理的消息；nats.DeliverLast()是会包含消息队列中的最后一条消息，即使被处理过的消息；nats.DeliverNew()则只处理订阅之后的新消息。\n","date":"2021-03-02T18:38:00Z","image":"https://vip1.loli.io/2022/03/26/Cv92HUNpYyjDTVK.png","permalink":"https://www.4async.com/2021/03/2021-03-02-nats-server-usage/","title":"NATS-Server(JetStream)和NATS Streaming Server对比"},{"content":"原文地址：https://blog.golang.org/context-and-structs 原文作者：Jean de Klerk, Matt T. Proud 译者：Kevin 介紹 在许多Go API中，尤其是现代的API中，函数和方法的第一个参数通常是context.Context。上下文（Context）提供了一种方法，用于跨API边界和进程之间传输截止时间、调用者取消和其他请求范围的值。当一个库与远程服务器（如数据库、API等）直接或间接交互时，经常会用到它。\n在context的文档中写道。\n上下文不应该存储在结构类型里面，而是传递给每个需要它的函数。 本文对这一建议进行了扩展，用具体例子解析为什么传递上下文而不是将其存储在其他类型中很重要。它还强调了一种罕见的情况，即在结构类型中存储上下文可能是有意义的，以及如何安全地这样做。\n倾向于将上下文作为参数传递 为了深入理解不在结构中存储上下文的建议，我们来考虑一下首选的上下文作为参数的方法。\ntype Worker struct { /* … */ } type Work struct { /* … */ } func New() *Worker { return \u0026amp;Worker{} } func (w *Worker) Fetch(ctx context.Context) (*Work, error) { _ = ctx // 每次调用中ctx用于取消操作，截止时间和元数据。 } func (w *Worker) Process(ctx context.Context, w *Work) error { _ = ctx // A每次调用中ctx用于取消操作，截止时间和元数据。 } 在这个例子中，(*Worker).Fetch和(*Worker).Process方法都直接接受上下文。通过这种通过参数传递的设计，用户可以设置每次调用的截止时间、取消和元数据。而且，很清楚传递给每个方法的context.Context将如何被使用：没有期望传递给一个方法的context.Context将被任何其他方法使用。这是因为上下文的范围被限定在了小范围的必须操作内，这大大增加了这个包中上下文的实用性和清晰度。\n将上下文存储在结构中会导致混乱 让我们再次使用上下文存储在结构体中这种方式审视一下上面的Worker例子。它的问题是，当你把上下文存储在一个结构中时，你会向调用者隐藏它的生命周期，甚至可能的是把两个不同的作用域以不可预料的方式互相干扰：\ntype Worker struct { ctx context.Context } func New(ctx context.Context) *Worker { return \u0026amp;Worker{ctx: ctx} } func (w *Worker) Fetch() (*Work, error) { _ = w.ctx // 共享的w.ctx用于取消操作，截止时间和元数据。 } func (w *Worker) Process(w *Work) error { _ = w.ctx // 共享的w.ctx用于取消操作，截止时间和元数据。 } (*Worker).Fetch和(*Worker).Process方法都使用存储在Worker中的上下文。这防止了Fetch和Process的调用者（它们本身可能有不同的上下文）在每次调用的基础上指定截止日期、请求取消和附加元数据。例如：用户无法只为(*Worker).Fetch提供截止日期，也无法只取消(*Worker).Process的调用。调用者的生命期与共享上下文交织在一起，上下文的范围是创建Worker的生命周期。\n与上下文作为参数的方法相比，该API也更容易让用户感到疑惑。用户可能会问自己：\n既然New需要一个context.Context，那么构造函数是否在做取消或截止时间控制的工作？ New传递进来的context.Context是否适用于(*Worker).Fetch和(*Worker).Process？都不适用？有一个而没有另一个？ API需要大量的文档来明确告诉用户context.Context到底是用来做什么的。用户可能还需要阅读代码，而不是能够依靠API结构获得信息。\n最后，如果设计一个生产级服务器，其每个请求没有上下文，从而不能充分重视取消操作，这可能是相当危险的。如果没有能力设置每个调用的截止日期，你的进程可能会积压资源并导致资源耗尽（如内存）!\n规则的例外：保存向后的兼容性 当引入context.Context的Go 1.7发布时，大量的 API 必须以向后兼容的方式添加上下文支持。例如，net/http的Client方法，如Get和Do，就是很好的上下文取消操作的应用。每一个用这些方法发送的外部请求都会受益于context.Context带来的截止时间、取消和元数据支持。\n有两种方法可以以向后兼容的方式添加对context.Context的支持：将上下文包在一个结构中，正如我们稍后将看到的那样；复制函数，复制的函数接受context.Context作为参数，并将Context作为其函数名的后缀。复制的方法应该比在结构体中嵌入上下文的方式更可取，在保持模块的兼容性中会进一步讨论。然而，在某些情况下，这是不切实际的：例如，如果你的API暴露了大量的函数，那么复制所有的函数可能是不可行的。\nnet/http包选择了上下文存储在结构体方式，这提供了一个有用的案例研究。让我们看看net/http的Do方法。在引入context.Context之前，Do的定义如下：\nfunc (c *Client) Do(req *Request) (*Response, error) 在Go 1.7之后，如果不考虑破坏向后的兼容性的问题，Do可能看起来像下面这样：\nfunc (c *Client) Do(ctx context.Context, req *Request) (*Response, error) 但是，保留向后的兼容性，遵守Go 1的兼容性承诺对于标准库来说是至关重要的。所以，维护者选择在http.Request结构上添加一个context.Context，以便在不破坏向后兼容性的情况下支持context.Context：\ntype Request struct { ctx context.Context // ... } func NewRequestWithContext(ctx context.Context, method, url string, body io.Reader) (*Request, error) { // 为了本文演示需要做了简化。 return \u0026amp;Request{ ctx: ctx, // ... } } func (c *Client) Do(req *Request) (*Response, error) 当改造你的API以支持上下文时，像上面那样在一个结构中添加一个context.Context可能是有意义的。但是，你需要首先考虑复制你的函数，这样可以在不牺牲实用性和理解性的前提下，向后兼容地改造context.Context。例如：\nfunc (c *Client) Call() error { return c.CallContext(context.Background()) } func (c *Client) CallContext(ctx context.Context) error { // ... } 总结 上下文使得重要的跨库和跨API信息很容易在调用栈中传播。但是，为了保持可理解性、易调试性和有效性，必须统一清晰地使用它。\n当作为方法中的第一个参数而不是存储在结构类型中时，用户可以充分利用它的可扩展性，以便通过调用栈建立一个强大的取消、截止日期和元数据信息树。而且，最重要的是，当它作为一个参数传递进来时，它的范围被清晰的理解，从而导致堆栈上下的理解更加清晰和调试更加容易。\n当设计一个带有上下文的API时，请记住这样的建议：将context.Context作为一个参数传递进来，不要将它存储在结构体中。\n","date":"2021-02-25T10:30:00Z","image":"https://www.4async.com/2021/02/2021-02-25-context-and-structs/cover_huc83bc740bc3d410b7e22e27a8fb6bc1c_61028_120x120_fill_box_smart1_3.png","permalink":"https://www.4async.com/2021/02/2021-02-25-context-and-structs/","title":"上下文Context与结构体Struct"},{"content":"其实很早之前就希望切换至Hugo引擎驱动博客了，但是体验下来一圈，最大的感觉是Hugo的主题都很一言难尽\u0026hellip;所以拖着一直没有切换。结果今天想发布新文章时突然发现我无法正常编译Hexo的项目了，于是干脆趁此良机切换至Hugo引擎了。\n本站目前使用的主题是stack主题，相当简洁，推荐想切换至Hugo的小伙伴了解一下。\n另外提供一下快速进行Github Action部署的方案（私有化仓库代码/公有化仓库网站展示），PERSONAL_TOKEN是个人token，需要从个人设置页面里获取填写至仓库Secrets中：\nname: github pages on: push: branches: - master jobs: deploy: runs-on: ubuntu-20.04 steps: - uses: actions/checkout@v2 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: personal_token: ${{ secrets.PERSONAL_TOKEN }} external_repository: 你的用户名/你的仓库 publish_dir: ./public cname: 你的域名 ","date":"2021-02-19T16:27:00Z","image":"https://www.4async.com/2021/02/2021-02-19-move-to-hugo/cover_hu7bbb3f3ebdb843801f79bded2b036297_73881_120x120_fill_box_smart1_3.png","permalink":"https://www.4async.com/2021/02/2021-02-19-move-to-hugo/","title":"切换至Hugo引擎驱动"},{"content":"原文地址：https://blog.golang.org/go116-module-changes 原文作者：Jay Conrod 译者：Kevin 希望您喜欢Go 1.16! 这个版本有很多新功能，特别是对Module而言。发行说明中简要介绍了这些变化，但让我们深入发掘一下其中的一些变化。\nModule功能默认开启 go命令现在默认以module-aware模式构建包，即使没有go.mod文件存在。这是向在所有项目中使用Module功能迈出的一大步。\n通过设置GO111MODULE环境变量为off，仍然可以在GOPATH模式下构建包。你也可以将GO111MODULE设置为auto，只有当当前目录或任何父目录中存在go.mod文件时才启用module-aware模式。这在以前是默认的。请注意，您可以使用go env -w来永久地设置GO111MODULE和其他变量。\ngo env -w GO111MODULE=auto 我们计划在 Go 1.17 中放弃对GOPATH模式的支持。换句话说，Go 1.17将忽略GO111MODULE。如果您的项目没有以module-aware模式构建，现在是时候迁移了。如果有问题妨碍您迁移，请考虑提交问题或体验报告。\n不会自动更改 go.mod 和 go.sum 在之前的版本中，当go命令发现go.mod或go.sum有问题时，比如缺少require指令或缺少sum，它会尝试自动修复问题。我们收到了很多反馈，认为这种行为是出乎大家意料的，尤其是对于像go list这样通常不会产生副作用的命令。自动修复并不总是可取的：如果一个导入的包没有被任何需要的Module提供，go命令会添加一个新的依赖关系，可能会触发普通依赖关系的升级。即使是拼写错误的导入路径也会导致（失败的）网络查找。\n在 Go 1.16 中，module-aware命令在发现go.mod或go.sum中的问题后会报告一个错误，而不是尝试自动修复问题。在大多数情况下，错误信息建议使用命令来修复问题。\n$ go build example.go:3:8: no required module provides package golang.org/x/net/html; to add it: go get golang.org/x/net/html $ go get golang.org/x/net/html $ go build 和之前一样，如果存在vendor目录，go命令可能会使用该目录（详见Vendoring）。像go get和go mod tidy这样的命令仍然会修改go.mod和go.sum，因为它们的主要目的是管理依赖关系。\n在特定版本上安装可执行文件 go install命令现在可以通过指定@version后缀来安装特定版本的可执行文件。\ngo install golang.org/x/tools/gopls@v0.6.5 当使用这种语法时，go install命令会从该Module的制定版本安装，而忽略当前目录和父目录中的任何 go.mod 文件。(如果没有@version后缀，go install会像往常一样继续运行，使用当前Module的go.mod中列出的版本要求和替换来构建程序。)\n我们曾经推荐使用go get -u程序来安装可执行文件，但这种使用方式对go.mod中添加或更改Module版本需求的意义造成了太多的混淆。而为了避免意外修改go.mod，人们开始建议使用更复杂的命令，比如：\ncd $HOME; GO111MODULE=on go get program@latest 现在我们都可以用go install program@latest来代替。详情请看go install。\n为了消除使用哪个版本的歧义，当使用这种安装语法时，对程序的go.mod文件中可能存在的指令有一些限制。特别是，至少在目前，替换和排除指令是不允许的。从长远来看，一旦新的go install program@version在足够多的用例中运行良好，我们计划让go get停止安装命令二进制文件。详情请参见issue 43684。\nModule撤回 您是否曾经在Module版本准备好之前不小心发布过？或者您是否在版本发布后就发现了一个需要快速修复的问题？发布的版本中的错误是很难纠正的。为了保持Module构建的确定性，一个版本在发布后不能被修改。即使你删除或更改了版本标签，proxy.golang.org和其他代理可能已经有了原始版本的缓存。\nModule作者现在可以使用go.mod中的retract指令撤回Module版本。撤回的版本仍然存在，并且可以被下载（所以依赖它的构建不会中断），但在解析@latest这样的版本时，go命令不会自动选择它，go get和go list -m -u会打印关于现有使用版本的警告。\n例如，假设一个流行库example.com/lib的作者发布了v1.0.5，然后发现了一个新的安全问题。他们可以在他们的go.mod文件中添加如下指令。\n// Remote-triggered crash in package foo. See CVE-2021-01234. retract v1.0.5 接下来，作者可以标记并推送v1.0.6版本，即新的最高版本。在这之后，已经依赖v1.0.5的用户在检查更新或升级依赖的软件包时，就会被通知版本撤回。通知信息可能会包含来自retract指令上方注释的文字。\n$ go list -m -u all example.com/lib v1.0.0 (retracted) $ go get . go: warning: example.com/lib@v1.0.5: retracted by module author: Remote-triggered crash in package foo. See CVE-2021-01234. go: to switch to the latest unretracted version, run: go get example.com/lib@latest 关于交互式的、基于浏览器的使用指南，请查看play-with-go.dev上的Retract Module Versions。可以查看retract指令文档以了解语法细节。\n用GOVCS控制版本管理工具 go命令可以从proxy.golang.org这样的镜像中下载Module源代码，或者直接从使用git、hg、svn、bzr或fossil的版本管理仓库中下载。直接的版本控制访问是很重要的，特别是对于那些在代理上无法使用的私有Module，但这也是一个潜在的安全问题：版本控制工具中的一个bug可能会被恶意服务器利用，运行非预期的代码。\nGo 1.16引入了一个新的配置变量GOVCS，让用户可以指定哪些Module可以使用特定的版本控制工具。GOVCS接受一个以逗号分隔的pattern:vcslist规则列表。pattern是一个path.Match模式，匹配一个Module路径的一个或多个前缀元素。特殊模式public和private匹配公共和私有Module（private被定义为由GOPRIVATE中的模式匹配的Module；public是其他所有Module）。vcslist是一个以管道符分隔的允许的版本控制命令列表，或关键字all或off。\n例如\nGOVCS=github.com:git,evil.com:off,*:git|hg 在此设置下，路径在github.com上的Module可以使用git下载；路径在evil.com上的Module不能使用任何版本管理程序下载，其他所有路径（*匹配所有）可以使用git或hg下载。\n如果没有设置GOVCS，或者一个Module不符合任何模式，go命令就会使用这个默认值：公共Module允许使用git和hg，私有Module允许使用所有工具。只允许Git和Mercurial的理由是，这两个系统作为不受信任的服务器的客户端运行的问题最受关注。相比之下，Bazaar、Fossil和Subversion主要是在受信任的、经过认证的环境中使用，作为攻击面的审查程度不高。也就是说，默认的设置是\nGOVCS=public:git|hg,private:all 更多细节请参见使用GOVCS控制版本管理工具。\n下一步？ 我们希望您觉得这些功能很有用。我们已经在努力为Go 1.17开发新的Module功能，特别是懒惰Module加载，这将使Module加载过程更快、更稳定。和以往一样，如果您遇到新的bug，请在问题跟踪上告诉我们。Happy coding!\n","date":"2021-02-19T10:16:00Z","image":"https://www.4async.com/2021/02/2021-02-19-go116-module-changes/cover_huf957eb7cc0803733802fcc4099cfd7cb_47470_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.4async.com/2021/02/2021-02-19-go116-module-changes/","title":"Go 1.16 中Module功能新变化"},{"content":"今天早些时候，OpenTelemetry正式进入Beta版本阶段，这标志着OpenTelemetry的基本模型已经正式确定，可以开始将OpenTelemetry集成到应用程序和客户端库中，以捕获应用程序级指标和分布式跟踪。\nOpenTelemetry介绍 本章节主要介绍什么是OpenTelemetry，如果你已经了解，则可以跳过本节。\n对于软件而言，可观测性是非常重要的指标之一。OpenTelemetry项目提供了一组特定于语言的API，SDK，代理和其他组件，可用于从应用程序中收集分布式跟踪，metrics和相关应用Metadata。借助OpenTelemetry，开发人员几乎可以从零开始从应用程序中捕获关键的可观察性数据。OpenTelemetry不仅仅是可以作为分布式追踪工具，也可以作为metrics收集的相关工具。支持的后端存储包括：Prometheus，Jaeger，Zipkin，Azure Monitor，Dynatrace，Google Cloud Monitoring + Trace，Honeycomb，Lightstep，New Relic和Splunk等。\nOpenTelemetry项目是作为OpenTracing和OpenCensus的融合项目，进入CNCF sandbox项目。对于OpenTelemetry项目的一个目标就是，兼容OpenTracing和OpenCensus。\nOpenTelemetry实战 我们这里演示最常用的追踪功能开始演示。OpenTelemetry支持多种后端，这里为了简化功能演示，我们最开始选择最为简单的标准输出作为后端输出，例子均采用Go语言实现。\n假设我们已创建一个名叫demo的基于Go Module的项目。接下来创建对应的main.go:\npackage main import ( “context” “log” “go.opentelemetry.io/otel/api/global” “go.opentelemetry.io/otel/exporters/trace/stdout” sdktrace “go.opentelemetry.io/otel/sdk/trace” ) func initTracer() { exporter, err := stdout.NewExporter(stdout.Options{PrettyPrint: true}) if err != nil { log.Fatal(err) } tp, err := sdktrace.NewProvider(sdktrace.WithConfig(sdktrace.Config{DefaultSampler: sdktrace.AlwaysSample()}), sdktrace.WithSyncer(exporter)) if err != nil { log.Fatal(err) } // 设置全局 global.SetTraceProvider(tp) } func main() { initTracer() tracer := global.Tracer(“4async.com/demo”) tracer.WithSpan(context.Background(), “foo”, func(ctx context.Context) error { tracer.WithSpan(ctx, “bar”, func(ctx context.Context) error { tracer.WithSpan(ctx, “baz”, func(ctx context.Context) error { return nil }, ) return nil }, ) return nil }, ) } 运行上面程序之后，我们可以获取到标准输出中输出了对应的tracing信息：\n{ \u0026#34;SpanContext\u0026#34;: { \u0026#34;TraceID\u0026#34;: \u0026#34;9495716100dbdd51e77edda2d39d8f28\u0026#34;, \u0026#34;SpanID\u0026#34;: \u0026#34;c9e68a06fbe116e0\u0026#34;, \u0026#34;TraceFlags\u0026#34;: 1 }, \u0026#34;ParentSpanID\u0026#34;: \u0026#34;e3e88597058566b3\u0026#34;, \u0026#34;SpanKind\u0026#34;: 1, \u0026#34;Name\u0026#34;: \u0026#34;baz\u0026#34;, \u0026#34;StartTime\u0026#34;: \u0026#34;2020-03-31T10:53:55.497818+08:00\u0026#34;, \u0026#34;EndTime\u0026#34;: \u0026#34;2020-03-31T10:53:55.497821142+08:00\u0026#34;, \u0026#34;Attributes\u0026#34;: null, \u0026#34;MessageEvents\u0026#34;: null, \u0026#34;Links\u0026#34;: null, \u0026#34;StatusCode\u0026#34;: 0, \u0026#34;StatusMessage\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;HasRemoteParent\u0026#34;: false, \u0026#34;DroppedAttributeCount\u0026#34;: 0, \u0026#34;DroppedMessageEventCount\u0026#34;: 0, \u0026#34;DroppedLinkCount\u0026#34;: 0, \u0026#34;ChildSpanCount\u0026#34;: 0, \u0026#34;Resource\u0026#34;: null } { \u0026#34;SpanContext\u0026#34;: { \u0026#34;TraceID\u0026#34;: \u0026#34;9495716100dbdd51e77edda2d39d8f28\u0026#34;, \u0026#34;SpanID\u0026#34;: \u0026#34;e3e88597058566b3\u0026#34;, \u0026#34;TraceFlags\u0026#34;: 1 }, \u0026#34;ParentSpanID\u0026#34;: \u0026#34;db446b6a4e579c04\u0026#34;, \u0026#34;SpanKind\u0026#34;: 1, \u0026#34;Name\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;StartTime\u0026#34;: \u0026#34;2020-03-31T10:53:55.497817+08:00\u0026#34;, \u0026#34;EndTime\u0026#34;: \u0026#34;2020-03-31T10:53:55.498188044+08:00\u0026#34;, \u0026#34;Attributes\u0026#34;: null, \u0026#34;MessageEvents\u0026#34;: null, \u0026#34;Links\u0026#34;: null, \u0026#34;StatusCode\u0026#34;: 0, \u0026#34;StatusMessage\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;HasRemoteParent\u0026#34;: false, \u0026#34;DroppedAttributeCount\u0026#34;: 0, \u0026#34;DroppedMessageEventCount\u0026#34;: 0, \u0026#34;DroppedLinkCount\u0026#34;: 0, \u0026#34;ChildSpanCount\u0026#34;: 1, \u0026#34;Resource\u0026#34;: null } { \u0026#34;SpanContext\u0026#34;: { \u0026#34;TraceID\u0026#34;: \u0026#34;9495716100dbdd51e77edda2d39d8f28\u0026#34;, \u0026#34;SpanID\u0026#34;: \u0026#34;db446b6a4e579c04\u0026#34;, \u0026#34;TraceFlags\u0026#34;: 1 }, \u0026#34;ParentSpanID\u0026#34;: \u0026#34;0000000000000000\u0026#34;, \u0026#34;SpanKind\u0026#34;: 1, \u0026#34;Name\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;StartTime\u0026#34;: \u0026#34;2020-03-31T10:53:55.497813+08:00\u0026#34;, \u0026#34;EndTime\u0026#34;: \u0026#34;2020-03-31T10:53:55.498301313+08:00\u0026#34;, \u0026#34;Attributes\u0026#34;: null, \u0026#34;MessageEvents\u0026#34;: null, \u0026#34;Links\u0026#34;: null, \u0026#34;StatusCode\u0026#34;: 0, \u0026#34;StatusMessage\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;HasRemoteParent\u0026#34;: false, \u0026#34;DroppedAttributeCount\u0026#34;: 0, \u0026#34;DroppedMessageEventCount\u0026#34;: 0, \u0026#34;DroppedLinkCount\u0026#34;: 0, \u0026#34;ChildSpanCount\u0026#34;: 1, \u0026#34;Resource\u0026#34;: null } 这里我们借助TraceID、SpanID和ParentSpanID就可以快速梳理调用链路。\n当然，实际上在实际项目中不可能使用这种方式进行追踪，一般来说我们会有专门的系统存储和展示追踪链路信息。因此可以通过修改上面程序中对应的go.opentelemetry.io/otel/exporters/trace/stdout内容快速替换成你需要的对应的后端存储，比如我们在线上使用的Jaeger后端，则引用go.opentelemetry.io/otel/exporters/trace/jaeger，修改initTracer()方法快速替换：\n// initTracer creates a new trace provider instance and registers it as global trace provider. func initTracer() func() { // Create and install Jaeger export pipeline _, flush, err := jaeger.NewExportPipeline( jaeger.WithCollectorEndpoint(“http://localhost:14268/api/traces”), jaeger.WithProcess(jaeger.Process{ ServiceName: “trace-demo”, Tags: []core.KeyValue{ key.String(“exporter”, “jaeger”), key.Float64(“float”, 312.23), }, }), jaeger.RegisterAsGlobal(), jaeger.WithSDK(\u0026amp;sdktrace.Config{DefaultSampler: sdktrace.AlwaysSample()}), ) if err != nil { log.Fatal(err) } return func() { flush() } } 至于其他后端支持，你可以从pkg.go.dev中获取到所有的支持后端列表。\n","date":"2020-03-31T11:10:00Z","image":"https://www.4async.com/2020/03/2020-03-31-intro-opentelemetry/cover_hu2c1469b7ee57dc12d272d4e1f195b2aa_29271_120x120_fill_box_smart1_3.png","permalink":"https://www.4async.com/2020/03/2020-03-31-intro-opentelemetry/","title":"OpenTelemetry入门"},{"content":"原文：Moving Towards Domain Driven Design in Go\n本文的目的是帮助演示当应用程序随着时间不断推移不断演化时，我们如何利用领域驱动设计帮我们解决可能遇到的问题。为了实现这个目标，我们会通过一个琐碎的项目研究项目是如何随着时间一步步演化的。这不是一个完整的项目，示例代码并不能够直接编译，甚至有些导入以来没有全部列出。这只是一个简单的示例，也就是说，如果出现什么问题，你可以随时与我联系，我们将对问题进行调整或者你的问题及时解答（如果可以的话）。\n首先，让我们讨论一下这个项目的背景。想象一下你在工作，老板要求你创建一种通过GitHub API对用户进行身份验证的方法。具体上说，你需要利用用户个人访问令牌，查找该用户及其所有组织。 这样，你以后就可以根据他们所属的组织来限制他们的访问。\n注意：我们这里使用访问令牌来简化示例。\n这听起来很容易，所以你启动编辑器并实现提供此功能的github包。\npackage github type User struct { ID string Email string OrgIDs []string } type Client struct { Key string } func (c *Client) User(token string) (User, error) { // 与Github API进行交互，并返回用户。如果他们在一个组织中，返回组织的c.OrgID } 注意：这里不是使用真实的Github API - 这只是一个演示例子。\n接下来，你需要给github包编写一些中间件，这样你就可以在需要保护的HTTP handler中使用这个包。在中间件中，你通过头部中的basic auth中获得用户的访问令牌，然后调用Github的代码查找用户，检查他们是否是所提供组织的一部分，然后相应地授予权限或拒绝访问。\npackage mw func AuthMiddleware(client *github.Client, reqOrgID string, next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { token, _, ok := r.BasicAuth() if !ok { http.Error(w, “Unauthorized”, http.StatusUnauthorized) return } user, err := client.User(token) if err != nil { http.Error(w, “Unauthorized”, http.StatusUnauthorized) return } permit := false for _, orgID := range user.OrgIDs { if orgId == reqOrgID { permit = true break } } if !permit { http.Error(w, “Unauthorized”, http.StatusUnauthorized) return } // 用户已认证，放他们进来 next.ServeHTTP(w, r) }) } 你将这些代码给你的同事看，他们担心缺乏测试。具体的说，没有一种方法能够验证AuthMiddleware能否按照预期工作。“没有问题”，你说，“我们可以使用interface保证我们可以方便的测试它！”\npackage mw type UserService interface { User(token string) (github.User, error) } func AuthMiddleware(us UserService, reqOrgID string, next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { token, _, ok := r.BasicAuth() if !ok { http.Error(w, “Unauthorized”, http.StatusUnauthorized) return } user, err := us.User(token) if err != nil { http.Error(w, “Unauthorized”, http.StatusUnauthorized) return } permit := false for _, orgID := range user.OrgIDs { if orgId == reqOrgID { permit = true break } } if !permit { http.Error(w, “Unauthorized”, http.StatusUnauthorized) return } // user is authenticated, let them in next.ServeHTTP(w, r) }) } 现在，你可以使用模拟用户服务测试此代码。\npackage mock type UserService struct { UserFn func(token string) (github.User, error) } func (us *UserService) Authenticate(token string) (github.User, error) { return us.UserFn(token) } 创建模拟用户服务有很多方式，但是这个通常需要测试认证正常通过时和认证出现问题时的不同情况。\n现在，我们对认证中间件完成测试、发布，生活似乎很愉快。\n然后，悲剧发生了。你的CEO听说哥斯拉（译者注：哥斯拉毁灭世界？）正在前往旧金山，你的公司无法在未来不确定的情况下继续依赖Github。如果Github整个办公室被摧毁了，没有工程师继续维护该产品怎么办？不，完全不能接受！\n幸运的是，还有一家名为GitLab的替代公司，似乎做了很多类似GitHub的功能，不同的是他们是一个远程工作团队。这意味着哥斯拉永远无法同时消灭所有工程师，对吗？ 🎉\n高层似乎认同这个逻辑，他们开始进行迁移。你的工作？你的任务是保证所有认证代码能够在新系统上正常运行！\n你花了一些时间查看GitLab API文档，好消息是，看起来他们采用了和Github类似的策略。GitLab同样也有个人访问令牌，组织，你只需要重新实现客户端即可。中间件中的代码完全不需要更改，因为你是一个聪明人，你使用了接口！ 😁 现在你只需要创建一个Github客户端…\npackage gitlab type User struct { ID string Email string OrgIDs []string } type Client struct { Key string } func (c *Client) User(token string) (User, error) { // 与Gitlab API交互，返回用户和对应的组织c.OrgID } 现在，把这个功能放入AuthMiddleware中，但是，它却不能正常工作了！\n事实证明，即使接口也可能成为耦合的受害者。在这种情况下，这是因为你的接口期望通过User方法返回github.User 。\ntype UserService interface { User(token string) (github.User, error) ^^^^^^^^^^^ } 怎么办？老板希望明天就要发布了！\n现在，你有几种选择：\n修改中间件，以便UserService接口返回gitlab.User而不是github.User； 创建一个新的专门用于GitLab的身份验证中间件。 创建一个通用的用户类型，能够在AuthMiddleware中可以自由切换你的github和gitlab实现。 如果你确信自己的公司以后继续使用GitLab，则方案1就有意义。当然，你需要同时更改用户服务接口和mock，但是如果是一次性的修改，那么这种方案并不坏。\n当然，你并不真正知道你们是否会坚持使用GitLab。也许哥斯拉会改变它的攻击计划？\n如果程序中很多代码都依赖此程序包返回的github.User类型，这个方案也会带来很多麻烦。\n方案2也是可行的，但是它看起来有点傻傻的。当逻辑不变时，为什么我们需要重写一遍所有的代码和测试？当然，一定有一种接口可以按照你最初的意图实现功能。毕竟，中间件不关心查询的用户，我们只是需要处理其中一些重要信息。\n现在，你决定尝试一下方案3。你创建了一个mw包，定义了一个User类型，然后准备编写一个适配器将其与Github和Gitlab客户端连接。\npackage mw type User struct { OrgIDs []string } type UserService interface { User(token string) (User, error) } 当你编写代码时，你意识到，你实际上并不关心用户ID或者电子邮件等等其他信息，因此你完全可以从mw.User中删除这些字段，你只需要制定你关心的字段，这样就可以让代码更容易维护和测试。棒极了！\n接下来，你需要创建一个适配器，以便你可以对其进行操作。\n// Package adapter probably isn’t a great package name, but this is a // demo so deal with it. package adapter type GitHubUserService struct { Client *github.Client } func (us *GitHubUserService) User(token string) (mw.User, error) { ghUser, err := us.Client.User(token) if err != nil { return mw.User{}, err } return mw.User{ OrgIDs: ghUser.OrgIDs, }, nil } type GitLabUserService struct { Client *gitlab.Client } func (us *GitLabUserservice) User(token string) (mw.User, error) { glUser, err := us.Client.User(token) if err != nil { return mw.User{}, err } return mw.User{ OrgIDs: glUser.OrgIDs, }, nil } 你也需要更新你的mock，不过这是一个相当快的修改。\npackage mock type UserService struct { UserFn func(token string) (mw.User, error) } func (us *UserService) Authenticate(token string) (mw.User, error) { return us.UserFn(token) } 现在，如果你想将我们的AuthMiddleware与GitHub或GitLab一起使用，则可以使用如下代码进行操作：\nvar myHandler http.Handler var us mw.UserService us = \u0026amp;GitLabUserservice{ Client: \u0026amp;gitlab.Client{ Key: “abc-123”, }, } // This protects your handler myHandler = mw.AuthMiddleware(us, “my-org-id”, myHandler) 我们终于有了一个完全解耦的解决方案。我们可以轻松地在GitHub和GitLab之间切换，并且当新的源码管理公司出现时，我们也可以很快跟上潮流。\n寻找中间方案 在前面的示例中，我们一步步看到了代码从紧密耦合到完全解耦的过程。我们使用adapter包来完成处理这些解耦后的mw和github/gitlab包之间的转换。\n这样做的主要好处是我们在最后的时候看到的：我们可以在自由选择使用GitHub或GitLab认证策略，而不需要修改handlers和认证中间件。\n尽管这些好处非常棒，但在不考虑成本的情况下探索这些好处是不公平的。 所有这些更改提供了越来越多的代码，如果你回过头去查看gitlab和mw软件包的原始版本，你会发现它们比需要使用adapter软件包的最终版本要简单得多。这样也会也可能导致更多的设置项，因为在我们代码的某个地方，我们需要实例化所有这些适配器并将它们连接在一起。\n如果沿这条路线继续演进，我们可能会很快发现我们也需要许多不同的User类型。例如，我们可能需要在GitHub（或GitLab）等服务中将内部用户类型与外部用户ID关联。 这可能导致在我们的数据库包中定义一个ExternalUser ，然后编写一个适配器将github.User转换为这种类型，以便我们的数据库代码与我们正在使用的服务解耦。\nI actually tried doing this on one project with my HTTP handlers just to see how it turned out. Specifically, I isolated every endpoint in my web application to its own package with no external dependencies specific to my web application and ended up with packages like this: 实际上，为了证明这个结果，我在一个项目上的HTTP处理程序上进行过尝试。具体来说，我将Web应用程序中的每个端点隔离到其自己的程序包中，而没有特定于Web应用程序的外部依赖关系，最终得到了像这样的程序包：\n// Package enroll provides HTTP handlers for enrolling a user into a new // course. // This package is entirely for demonstrative purposes and hasn’t been tested, // but if you do see obvious bugs feel free to let me know and I’ll address // them. package enroll import ( “io” “net/http” “github.com/gorilla/schema” ) // Data defines the data that will be provided to the HTML template when it is // rendered. type Data struct { Form Form // Map of form fields with errors and their error message Errors map[string]string User User License License } // License is used to show the user more info about what they are enrolling in. // Eg if they URL query params have a valid key, we might show them: // // “You are about to enroll in Gophercises - FREE using the key `abc-123`” // ^ ^ ^ // Course Package Key // type License struct { Key string Course string Package string } // User defines a user that can be enrolled in courses. type User struct { ID string // Email is used when rendering a navbar with the user’s email address, among // other areas of an HTML page. Email string Avatar string // … } // Form defines all of the HTML form fields. It assumes the Form will be // rendered using struct tags and a form package I created // (https://github.com/joncalhoun/form), but it isn’t really mandatory as // long as the form field names match the `schema` part here. type Form struct { License string `form:\u0026#34;name=license;label=License key;footer=You can find this in an email sent over by Gumroad after purchasing a course. Or in the case of Gophercises it will be in an email from me (jon@calhoun.io).\u0026#34; schema:\u0026#34;license\u0026#34;` } // Handler provides GET and POST http.Handlers type Handler struct { // Interfaces and function types here serve roughly the same purpose. funcs // just tend to be easier to write adapters for since you don’t need a // struct type with a method. UserFn func(r *http.Request) (User, error) LicenseFn func(key string) (License, error) // Interface because this one is the least likely to need an adapter Enroller interface { Enroll(userID, licenseKey string) error } // Typically satisfied with an HTML template Executor interface { Execute(wr io.Writer, data interface{}) error } } // Get handles rendering the Form for a user to enroll in a new course. func (h *Handler) Get() http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { user, err := h.UserFn(r) if err != nil { // redirect or render an error return } var data Data data.User = user var form Form err := r.ParseForm() if err != nil { // maybe log this? We can still render } dec := schema.NewDecoder() dec.IgnoreUnknownKeys(true) err = dec.Decode(\u0026amp;form, r.Form) if err != nil { // maybe log this? We can still render } data.Form = form if form.License != “” { lic, err := h.LicenseFn(form.License) data.License = lic if err != nil { data.Errors = map[string]string{ “license”: “is not valid”, } } } h.Executor.Execute(r, data) } } // Post handles processing the form and enrolling a user. func (h *Handler) Post() http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { user, err := h.UserFn(r) if err != nil { // redirect or render an error return } var data Data data.User = user var form Form err := r.ParseForm() if err != nil { // maybe log this? We can still render } dec := schema.NewDecoder() dec.IgnoreUnknownKeys(true) err = dec.Decode(\u0026amp;form, r.Form) if err != nil { // maybe log this? We can still render } data.Form = form err = h.Enroller.Enroll(user.ID, form.License) if err != nil { data.Errors = map[string]string{ “license”: “is not valid”, } // Re-render the form h.View.Execute(r, data) return } http.Redirect(w, r, “/courses”, http.StatusFound) } } 从理论上讲，这个想法听起来很酷。 现在，我可以单独定义所有HTTP处理程序，不必担心应用程序的其余部分的影响。 每个包都可以轻松测试，当我编写这些独立的作品时，我发现自己的工作效率非常高。 我甚至有一个名为Executor的接口，谁不想在他们的代码中使用自定义的执行器？！\n在实践中，这个想法对我的特定用例而言太糟糕了。是的，这种方法确实有好处，但是它们并没有超过编写所有这些代码的成本。 在创建enroll和类似软件包的内部组件时，我的工作效率很高，但是我花了很多时间编写适配器并将各个部分连接在一起，这使我的整体生产率下降了。 我找不到一种无需编写自定义UserFn ， LicenseFn的快速方法就可以将其引入到代码中的方案，而且我发现自己为带有HTTP处理程序的每个程序包编写了一堆几乎相同的UserFn变种。\n这引出了本节的主题- 是否有办法实现可接受的中间方案？\n我喜欢将我的代码与第三方依赖项解耦。我也喜欢编写可测试的代码。但是我不喜欢通过加倍的编码工作来实现这一目标。当然，这里必然有一个中间方案，可以在没有所有额外代码的情况下为我们提供大多数好处，对吗？\n是的，是有中间方案的，找到它的关键不是消除所有耦合，而是有意地选择代码所耦合的内容。\n让我们回到github和gitlab包的原始示例。在我们的第一个版本（紧密耦合版本）中，我们有一个mw包所依赖的github.User类型。 它满足使用，并且我们甚至可以围绕它构建接口，但是我们仍然与github包紧密耦合。\n在第二个版本（解耦版本）中，我们有一个github.User ，gitlab.User和mw.User。 这使我们能够解耦所有内容，但是我们必须创建将这些解耦的代码连接在一起的适配器。\nThe middle ground, and the third version we will explore, is to intentionally define a User type that every package is allowed to be tightly coupled to. By doing this, we are intentionally choosing where that coupling happens and can make that decision in a way that still makes it easy to test, swap implementations, and do everything else we desire from decoupled code. 我们即将探索的第三种中间方案，是有意允许定义每个包紧密耦合的User类型。通过这种操作，我们刻意选择发生耦合的位置，并且保证易于测试、方便实现交换和易于其他为解耦代码实现的一切方式。\n首先是我们的User类型。 这将在domain包中创建，我们应用中的任何其他包都可以导入。\npackage domain type User struct { ID string Email string OrgIDs []string } 接下来，我们将重写github和gitlab软件包以利用此domain.User类型。 这些基本相同，因为我去除了所有的真实逻辑，只展示其中一个。\npackage gitlab // github is the same basically type Client struct { Key string } // Note the return type is domain.User here - this code is now coupled to our // domain. func (c *Client) User(token string) (domain.User, error) { // … interact with the gitlab API, and return a user if they are in an org with the c.OrgID } 最后，我们有了mw软件包。\npackage mw type UserService interface { User(token string) (domain.User, error) } func AuthMiddleware(us UserService, reqOrgID string, next http.Handler) http.Handler { // unchanged } 我们甚至可以使用此编写一个模拟包。\npackage mock type UserService struct { UserFn func(token string) (domain.User, error) } func (us *UserService) Authenticate(token string) (domain.User, error) { return us.UserFn(token) } 领域驱动设计 到目前为止，我一直努力避免使用任何令人困惑的术语，因为我发现它们通常会使问题变得复杂而不是简化它们。如果你不相信我，请尝试阅读有关域驱动设计（DDD）的任何文章，书籍或其他资源。他们几乎总是使你在代码中实际实现想法带来更多的疑问和更少的明确答案。\n我不是在说明DDD没有用，也不是在建议你不要读那些书。我的意思是，我的很多（大多数）读者在这里是为了寻求有关如何改进其代码的更实用建议，而不是讨论软件开发理论。\n从实际的执行角度来看，领域驱动设计的主要好处是编写可以随时间变化而变化的软件。我发现在Go中实现此目标的最佳方法是清楚地定义你的领域类型，然后编写依赖于这些类型的实现。这仍然会导致代码耦合，但是由于你的领域与问题紧密相关，因此你解决这种耦合并不困难。实际上，我经常发现，需要对领域模型进行清晰的定义是有启发性的，而不是麻烦的。\n注意：定义具体领域类型并将代码耦合到它们的想法并不是独创的或新颖的。本·约翰逊在2016年撰写的 这篇文章，对于任何新入门的Gopher来说仍然是非常有价值的文章。\n回到前面的示例，我们看到在domain包中定义了我们的领域 ：\npackage domain type User struct { ID string Email string OrgIDs []string } 再进一步，我们甚至可以开始定义基本的构建块，而我们的其余应用可以实现或者在不与实现细节耦合的情况下使用。 例如我们的UserService ：\npackage domain type User struct { … } type UserService interface { User(token string) (User, error) } 这是由github和gitlab包实现的接口，并在mw包中使用。 它也可以被我们代码中的其他软件包所使用，而不用关心它是如何实现的。而且由于它是在领域中定义的，因此我们不必担心每个实现的返回类型都会有所变化-它们都基于一个通用的定义可以构建。\n随着应用程序的发展和变化，这种定义通用接口的想法变得更加强大。 例如，假设我们有一个稍微复杂的UserService ：也许它会处理创建用户，验证用户，通过令牌查找用户，通过密码重置令牌，更改密码等操作。\npackage domain type UserStore interface { Create(NewUser) (*User, RememberToken, error) Authenticate(email, pw string) (*User, RememberToken, error) ByToken(RememberToken) (*User, error) ResetToken(email string) (ResetToken, error) UpdatePw(pw string, tok ResetToken) (RememberToken, error) } 我们可能首先使用纯SQL代码和本地数据库来实现：\npackage sql type UserStore struct { DB *sql.DB } func (us *UserStore) Create(newUser domain.NewUser) (*domain.User, domain.RememberToken, error) { // … } // … and more methods 即便当我们只有一个应用程序时，但是也许我们会成长为另一个Google，我们决定一个集中的用户管理系统，这样我们所有单独的应用都可以使用这套系统。\n如果我们将代码耦合到sql实现，这基本很难实现，但是由于大多数代码都耦合到domain.UserService我们可以编写一个新的实现并使用。\npackage userapi type UserStore struct { HTTPClient *http.Client } func (us *UserStore) Create(newUser domain.NewUser) (*domain.User, domain.RememberToken, error) { // interact with a third party API instead of a local SQL database } // … 一般而言，耦合到领域而不是特定的实现方式使我们不必担心诸如以下的细节：\n我们正在与微服务或本地数据库进行交互吗？ 无论我们的用户管理系统是本地SQL数据库还是微服务，我们都可以在合理的时间内完成代码编写。 *我们是否通过JSON，GraphQL，gRPC或其他方式与用户API通信？ 尽管我们的实现需要知道如何与用户API进行通信，但是无论我们使用哪种特定技术，我们其余的代码都将继续以相同的方式运行。 其他更多… 从根本上讲，这就是我认为领域驱动设计的主要好处。这不是花哨的术语，色彩鲜艳的图形，也不是在同事面前炫耀聪明。纯粹是设计能够不断发展以满足不断变化的需求的软件。\n为什么我们不从领域驱动设计开始？ 显而易见的后续问题是：“如果它是如此出色，为什么我们不只是从领域驱动设计开始呢？”\n任何有使用模型-视图-控制器（MVC）经验的人都会告诉你，它很容易发生紧密耦合。几乎我们所有的应用程序都将需要依赖于我们的模型，而我们只是探讨了这可能会带来问题。那有什么呢？\n从公用领域进行构建虽然很有用，但如果滥用，也可能是一场噩梦。领域驱动设计具有相当陡峭的学习曲线：并不是因为这些想法特别难于掌握，而是因为在项目发展到合理规模之前，你很少了解在应用这些想法时哪里出错了。结果是可能要花几年的时间才能真正掌握所有涉及的动态演化。我已经写了很长一段时间的软件了，但我仍然感觉不到我对所有可能出错或变得复杂的方式都拥有完全的了解。\n*注意：这是我花这么长时间发布本文的重要原因之一。在很多时候，我仍然对分享保有疑虑，我不觉得自己是这个主题的专家。但是，我最终决定分享，因为我相信其他人可以从我的有限理解中学到东西，并且我相信随着与其他开发人员进行讨论，本文会随着时间的推移而发展和改进。因此，随时欢迎与我进行讨论 -jon@calhoun.io *\nMVC为你提供了组织代码的合理起点。数据库交互在模型层，http处理程序在控制器层，渲染代码在视图层。这可能会导致紧密耦合，但可以让你快速入门。\n与MVC不同，领域驱动设计不会为你提供组织代码的合理起点。实际上，从DDD开始与从MVC开始几乎完全相反：不是直接进入构建控制器并查看模型如何发展，相反，你必须花大量时间预先确定领域应该是什么。这可能包括模拟一些想法并让同事进行审查，讨论什么是正确/不正确的，几个迭代周期，然后才可以投入到编写一些代码中。 你可以在 Ben Johnson的WTF Dial项目 中看到这一点，他在该 项目 中创建了PR，并与Peter Bourgon，Egon Elbre和Marcus Olsson讨论了领域相关内容。\n这并不是一件坏事，但正确起来也不容易，它需要大量的前期工作。 因此，如果你有一个更大的团队，每个人都需要在某个共同领域上达成共识，然后才能开始开发，那么我通常会发现这种方法最有效。\n鉴于我经常在较小的团队中（或由我自己）进行编码，所以我发现，如果我从简单的事情开始，我的项目就会自然发展。也许这是一个 平铺结构 ，也许是 一个mvc结构 ，或者也许完全是其他东西。 只要我对代码的发展保持开放的态度，我就不会太着迷于那些细节。这使它最终可以采用DDD之类的形式，但是不需要我从那里开始。如前所述，对于大型组织（每个人都一起开发同一应用程序）而言，这样做可能会比较困难，因此通常需要进行更多的前期设计讨论。\n在我们的示例应用程序中，我们做了与“让它不断发展”的概念非常相似的事情。每一步都是为特定目的而采取的：我们添加了UserService接口，因为我们需要测试身份验证中间件。 当我们开始从GitHub迁移到GitLab时，我们意识到我们的接口不够用，因此我们探索了其他选择。 围绕这一点，我认为一种让变得有意义DDD的方法，而不是猜测User和UserService接口，因为我们有一些实际的实现基础进行验证。\n以DDD开头的另一个潜在问题是类型定义不足，因为我们经常在拥有具体用例之前就定义它们。例如，我们可能决定对用户进行身份验证如下所示：\ntype UserAuthenticator interface { Authenticate(email, pw string) (error) } 但是后来，我们意识到，实际上，每当我们对用户进行身份验证时，我们确实希望返回该用户（或记住的令牌），并且通过预先定义此接口，而之前我们错过了这一细节。现在，我们需要引入第二种方法来检索此信息，或者我们需要更改UserAuthenticator类型并重构实现利用此功能的任何代码。\n同样的情况适用于你的模型。在实际实现github和gitlab软件包之前，我们可能会认为，我们在User模型上唯一需要的识别信息是Email字段，但是我们稍后可能会在实现这些服务时意识到电子邮件地址可以更改的，而我们还需要ID字段用于唯一标识用户。\n在使用领域模型之前定义是一项挑战。除非我们已经非常了解我们正在研究的领域，否则我们极不可能知道我们需要做什么和不需要做什么。是的，这意味着我们必须在以后重构代码，但是如果你错误定义了领域，不使用DDD会很多比重构整个代码库更容易。 这是为什么我不介意从紧密耦合的代码开始并在以后进行重构的另一个原因。\n最后，并非所有代码都需要这种解耦，它并不能总是能够带来好处，并且在某些情况下（例如DB），我们很少利用这种解耦。\n对于不继续研发的项目，你可能不需要花费所有时间来解耦代码。如果代码没有继续研发，那么更改的可能性将大大降低，而为代码做准备而付出的额外努力可能只是浪费了。\n此外，解耦并不总是能提供它所承诺的好处，我们也不会总是利用这种解耦的优势。 正如Mat Ryer所指出的那样 ，我们很少会换掉数据库实现。 即使我们确实实现了所有功能的分离，即使我们碰巧只在少数正在迁移数据库的应用程序中，这种迁移通常也需要彻底重新考虑我们与数据存储之间的交互方式。 毕竟，NoSQL数据库的行为与SQL数据库完全不同，要真正利用两者，我们必须编写所使用数据库的特定代码。最终结果是这些抽象并不总是为我们提供我们想要的神奇的“实现无关”的结果。\n这并不意味着DDD不能提供优势，而是意味着我们不应该简单地 顶礼膜拜 并期待神奇效果。我们需要停下来自我反思。\n综上所述 在本文中，我们直接研究了代码紧密耦合时遇到的问题，并探讨了定义领域类型和接口如何帮助改善这种耦合。 我们还讨论了为什么让我们的代码随着时间的推移而发展，而不是从开始进行这种分离的设计的原因。\n在本系列的下一篇文章中，我希望展开讲解使用领域驱动设计编写Go代码的想法。 具体来说，我想讨论：\n接口测试如何帮助确保实现可以毫无问题地互换。 子域如何衍生自不同的上下文。 使用传统的DDD六边形来形象化这一切，以及像第三方库这样的代码如何适配。 我还想强调，本文绝不是硬性规定。这只是我微薄的尝试，试图分享一些帮助我改进Go软件的见解和想法。\n我也不是第一个在Go中讨论或探索DDD和设计模式的人。 你应该查看以下内容以获得更全面的理解：\nGoDDD by Marcus Olsen - 这是一个GitHub存储库，其中包含文章和演讲，Marcus Olsen试图将传统的DDD Java示例应用程序移植到Go中。 WTF Dial by Ben Johnson - 我在文中链接了Ben的Standard Pacakage Layout文章； 本系列应用了Ben在该文章中讨论的内容。我还建议查看附带的PR，并仔细阅读评论。 How Do You Structure Your Go Apps by Kat Zien - 在演讲中，Kat通过多种方法构建了Go应用程序。附带了演讲的代码仓库和幻灯片。 Design Philosophy On Packaging by Bill Kennedy - 虽然不专门涉及结构化应用程序，但本系列讨论了与结构紧密联系的包设计。 ","date":"2020-02-16T21:52:00Z","image":"https://www.4async.com/2020/02/2020-02-16-moving-towards-domain-driven-design-in-go/cover_huea63e6370dcf0c375755d886a5d0b9c6_97708_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.4async.com/2020/02/2020-02-16-moving-towards-domain-driven-design-in-go/","title":"译：在Go中转向领域驱动设计"},{"content":"我们通常会遇到线上甚至测试中代码出现问题，这些问题可能来自于我们开发过程中的引入的BUG，有些来自于我们的功能未得到理想结果的，甚至有一些问题来自于运行环境的。很多事情可能未必能够足够可控，尤其是上线之后才发现出现了问题。除了我们前面一篇文章中介绍了一些测试相关的内容，虽然可以解决一部分问题，但是这些并不能完全杜绝所有问题在线上一定不会出现任何问题。因此我们需要建立对发布/预发环境一套相对完善的监控、诊断机制，保证我们可以尽快进行故障的分析和溯源。\n系统的可观察性 为了应对目前软件开发的复杂度带来的相关问题，可观察性(Observability) 这个概念被引入软件领域。传统的监控和报警主要关注系统的异常情况和失败因素，可观察性更关注的是从系统自身出发，去展现系统的运行状况，更像是一种对系统的自我审视。一个可观察的系统中更关注应用本身的状态，而不是所处的机器或者网络这样的间接证据。我们希望直接得到应用当前的吞吐和延迟信息，为了达到这个目的，我们就需要合理主动暴露更多应用运行信息。在当前的应用开发环境下，面对复杂系统我们的关注将逐渐由点到点线面体的结合，这能让我们更好的理解系统，不仅知道What，更能回答Why。\n可观察性目前主要包含以下三大支柱：\n度量(Metrics)：Metric 往往是一些聚合的信息，相比 Logging 丧失了一些具体信息，但是占用的空间要比完整日志小的多，可以用于监控和报警。\n分布式追踪(Tracing)：Tracing 介于 Logging 和 Metric 之间， 以请求的维度，串联服务间的调用关系并记录调用耗时，即保留了必要的信息，又将分散的日志事件通过 Span 串联， 帮助我们更好的理解系统的行为、辅助调试和排查性能问题。\n日志(Logging)：Logging 主要记录一些离散的事件，应用往往通过将定义好格式的日志信息输出到文件，然后用日志收集程序收集起来用于分析和聚合。\n故障发现：监控设施 监控设施虽然对程序而言各种各样，但是在我们实际落地过程中，我们也会对网络环境/物理设备/业务系统等等各种可能存在潜在问题的系统进行监控和报警。这里监控系统和报警系统一般是属于监控设施部分，这一部分的选型一般是公司整体选型设计的，这里不额外讨论。这里讨论的具体事项可能需要根据各公司情况自行判定。\n我们选型时则是选择了基于Prometheus+Grafana进行业务系统监控，因此需要对应的客户端进行输出结果。Prometheus采用了pull模式，需要开启对应的HTTP端口方便进行采集。官方有一个基础的入门教程，可以用以参考。\n故障追溯：分布式追踪 分布式追踪我们目前采用的是Jaeger系统作为分布式追踪系统，我们可以根据用户反馈查看对应系统的请求，发现请求流经的系统和对应的处理时间。\nJaeger提供了各种语言的丰富客户端格式，作为Go编写的应用，自然也提供了Go语言的客户端库。\n故障追溯：日志 日志采集系统大家常见的包含ELK(EFK)，后面更有性能更强资源占用更少的ClickHouse等等作为选择。这里我们重点不讨论外部系统的选择，更重要的将核心集中于常见库的选择中。\n在常见日志记录中，同分布式追踪相同，我们通常需要对请求的相关ID进行记录，保证在对对应数据进行分析时，能够尽快筛选对应的请求和相关数据内容。这里我推荐使用rs/zerolog作为日志记录的日志库。虽然我们历史系统中大量的使用了zap作为日志输出驱动，但是在实际开发中，借助zerolog提供的WithContext方法，我们可以有针对性的，对每次请求，部分请求、请求内参数等各方面进行详细定制，方便我们在诊断具体内容时动态调整日志级别、附带每次请求ID等等的需求。\n以常见Web框架Gin为例，我们可以将一些具体的请求数据保存至context.Context中，方便我们后续使用：\nfunc RequestID(logger zerolog.Logger) gin.HandlerFunc { return func(c *gin.Context) { var reqID string // 可接受外部RequestID，常见于系统间互相调用 if c.Request.Header.Get(\u0026#34;X-Request-Id\u0026#34;) == \u0026#34;\u0026#34; { reqID = uuid.New().String() } else { reqID = c.Request.Header.Get(\u0026#34;X-Request-Id\u0026#34;) } ctx := c.Request.Context() logger = logger.With().Str(\u0026#34;requestID\u0026#34;, reqID).Logger() // 绑定到context.Context ctx = logger.WithContext(ctx) c.Request = c.Request.WithContext(ctx) c.Next() c.Header(\u0026#34;X-Request-Id\u0026#34;, reqID) } } 在使用过程中，我们需要动态调节日志等级，可以通过中间件或者其他形式控制：\nfunc DebugLevel(config *viper.Viper) gin.HandlerFunc { return func(c *gin.Context) { if config.GetBool(\u0026#34;debug\u0026#34;) { logger := log.Ctx(c.Request.Context()).Level(zerolog.DebugLevel) ctx := logger.WithContext(c.Request.Context()) c.Request = c.Request.WithContext(ctx) } c.Next() } } ","date":"2020-02-05T18:30:00Z","permalink":"https://www.4async.com/2020/02/2020-02-05-golang-debug-instrumental/","title":"从 Go 语言的依赖库讲起（2）监控、分布式追踪和日志"},{"content":"对开发而言，测试的重要性相信对每个开发者而言是老生常谈的事情。虽然我们很有可能在开发过程中由于各种原因会希望后续补全，然而事实上我更建议采用“Tests that fail then pass”原则去处理在实际开发过程中遇到的问题。\n在我们开发过程的初期阶段，开发质量的保持更多依赖开发人员自身素质保持。但是对一个团队而言，未必能够一直保持人员的高素质开发。在这个过程中，人员的变动，新老编码习惯的冲突，人员能力的残次不齐都有可能导致代码的腐化。在测试过程中，我们选择引入测试保障代码的质量\nGo本身提供了基础的测试功能，但是这个功能在实际使用过程中仍有使用起来功能较弱的问题。比如我们在使用过程中，需要使用额外的库让测试代码更佳高效。在实际实践过程中，我推荐使用Ginkgo、testify和GoMock工具。\nGoMock GoMock工具是Golang官方提供的针对接口的代码生成测试工具。在实际的单元测试过程中，通常会选择Mock掉数据库（DB/KV）、外部服务调用操作部分，将这部分功能留在集成测试中完成。\n比如我们将数据操作类型抽象成接口Creator、Updater、Deleter等，借助接口的组合功能，针对我们需要的功能进行组合开发。在测试过程中，我们可以借助GoMock工具生成对应的测试辅助代码。\n以对最简单的io.ReadeCloser使用代码为例：\npackage tdd import \u0026#34;io\u0026#34; func Read(r io.ReadCloser, buf []byte) (n int, err error) { n, err = io.ReadFull(r, buf) return } 生成对应的mock方法，这里为了方便，我们使用-package参数定义包名，为了区分生成文件，添加了_ten_test.go后缀。\n# 指定生成io.ReadCloser的mock方法 # 如果有专门的文件定义对应接口定义，则可以通过-source方法指定一次性提取所有接口 mockgen -package tdd io ReadCloser \u0026gt; reader_gen_test.go 接下来就是使用这个方法进行操作了，我们可以在reader_test.go文件中进行：\npackage tdd import ( \u0026#34;io\u0026#34; \u0026#34;reflect\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;github.com/golang/mock/gomock\u0026#34; ) func TestRead(t *testing.T) { ctrl := gomock.NewController(t) defer ctrl.Finish() r := NewMockReadCloser(ctrl) r.EXPECT(). Read(gomock.AssignableToTypeOf([]byte{})). SetArg(0, []byte{0x0, 0x1, 0x2, 0x3, 0x4}). // 设置参数值 Return(5, io.EOF). // 设置返回值 AnyTimes() // 执行次数 buf := make([]byte, 5) Read(r, buf) want := []byte{0x0, 0x1, 0x2, 0x3, 0x4} if !reflect.DeepEqual(want, buf) { t.Errorf(\u0026#34;Read() failed. want=%v, got=%v.\u0026#34;, want, buf) } } testify 我们在上面的例子中，会发现使用reflect.DeepEqual方式对比，然后调用t.Errorf方式输出错误信息。但是这里面其实相对来说要麻烦一点，另外一个则是对数据而言，如果内容较多，我们没办法一一对比可能出现的内容，这种情况下testify工具则可以提供一种更便捷的方式帮助我们进行测试的管理。\n为了方便对比这个测试内容，我们把上面DeepEqual的判断条件取反，获取的错误的内容对比验证一下：\n# DeepEqual === RUN TestRead --- FAIL: TestRead (0.00s) /Users/kevin/Desktop/tdd/reader_test.go:26: Read() failed. want=[0 1 2 3 4], got=[0 1 2 3 4]. FAIL 现在，我们将测试文件替换为testify方式进行：\npackage tdd import ( \u0026#34;io\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;github.com/golang/mock/gomock\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; ) func TestRead(t *testing.T) { ctrl := gomock.NewController(t) defer ctrl.Finish() r := NewMockReadCloser(ctrl) r.EXPECT(). Read(gomock.AssignableToTypeOf([]byte{})). SetArg(0, []byte{0x0, 0x1, 0x2, 0x3, 0x4}). // 设置参数值 Return(5, io.EOF). // 设置返回值 AnyTimes() // 执行次数 buf := make([]byte, 5) Read(r, buf) want := []byte{0x0, 0x1, 0x2, 0x3} if !assert.Equal(t, want, buf, \u0026#34;Read failed\u0026#34;) { return } } 获取测试结果：\n=== RUN TestRead --- FAIL: TestRead (0.00s) /Users/kevin/Desktop/tdd/reader_test.go:25: Error Trace:\treader_test.go:25 Error: Not equal: expected: []byte{0x0, 0x1, 0x2, 0x3} actual : []byte{0x0, 0x1, 0x2, 0x3, 0x4} Diff: --- Expected +++ Actual @@ -1,3 +1,3 @@ -([]uint8) (len=4) { - 00000000 00 01 02 03 |....| +([]uint8) (len=5) { + 00000000 00 01 02 03 04 |.....| } Test: TestRead Messages: Read failed FAIL coverage: 100.0% of statements 另外，在testify工具中，还提供了assert.JSONEq等等非常有用的函数，可以自行研究一下。同时，testify工具还提供了Testsuite功能，用于方便的设置Setup和Teardown函数。\n你会发现testify工具还提供了mock功能，不过在实际过程中，不太建议使用该功能。\nGinkgo Ginkgo是针对Go程序进行BDD开发的工具，虽然它默认搭配使用gomega工具，不过我们还是建议你选择testify工具。你可以使用下面的方法快速接入testify：\npackage foo_test import ( . \u0026#34;github.com/onsi/ginkgo\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; ) var _ = Describe(func(\u0026#34;foo\u0026#34;) { It(\u0026#34;should testify to its correctness\u0026#34;, func(){ assert.Equal(GinkgoT(), foo{}.Name(), \u0026#34;foo\u0026#34;) }) }) Ginkgo工具提供了完善的文档介绍，你可以参考工具官方文档了解具体的使用。另外一个Ginkgo非常有用的是它可以方便接入已有的测试日志捕获程序，比如你是JUnit的用户，你可以选择将日志格式输出成JUnit XML格式：\npackage foo_test import ( . \u0026#34;github.com/onsi/ginkgo\u0026#34; . \u0026#34;github.com/onsi/gomega\u0026#34; \u0026#34;github.com/onsi/ginkgo/reporters\u0026#34; \u0026#34;testing\u0026#34; ) func TestFoo(t *testing.T) { RegisterFailHandler(Fail) junitReporter := reporters.NewJUnitReporter(\u0026#34;junit.xml\u0026#34;) RunSpecsWithDefaultAndCustomReporters(t, \u0026#34;Foo Suite\u0026#34;, []Reporter{junitReporter}) } 总结 文章总结了一些常见的涉及测试的工具，希望对你在实践过程中有所帮助。顺带，我还没忘记要完成这个系列。:D\n","date":"2020-01-10T17:30:00Z","permalink":"https://www.4async.com/2020/01/2020-01-10-golang-test-driven-toolkit/","title":"从 Go 语言的依赖库讲起（1）Ginkgo、testify和GoMock"},{"content":"缘起 今年是Go语言的10年生日，准确的说应该是Go开放于2009年，当时虽然公开了，但是其实真正的可用性并不是很好。甚至Windows的支持都没有。当时我的主要环境还是在Windows平台，因此没有太关注Go语言。在2012年Go语言正式发布1.0版本之后，我当时工作环境转移到了Linux/macOS/iOS相关的产品上，因此重新学习了一下Go语言。\n后来在Go的1.3版本发布后，我在支付宝的内部系统中第一次上线了一个基于Go语言的边缘服务，也是第一次将Go语言应用到了实际的生产环境中。后来在云柚科技创业过程中，正式将Go语言作为我们物联网环境的第一优选语言（也基本上是唯一语言），大规模的批量部署Go语言项目和代码。\n聊聊如何开发更方便维护的Go语言程序 对Go语言而言，开发似乎是简单的事情：Go本身的语法比较简单，关键词不多，可以快速上手。如果基本功能而言，能采用的花式解法不多，导致Go语言的上手比较简单，对我们创业期间的公司而言，享受到了很多这种方面带来的很多红利：我们工程师基本上都是其他语言/岗位转型过来的全(quan)栈(gan)工程师，他们也为我们业务的快速发展提供了原生的动力。\n然而，当我们把时间稍微放长一点，我们会发现当时我们仍旧遗留下了很多历史性的技术债，这些内容包含了我们当时对于代码质量的妥协，由其他语言习惯带过来的洋玩意但有点水土不服，一些过度的设计，一些库选择上和我们后续的最佳实践相违背等等问题。\n这里面，一些软性的东西其实我们可以通过一些非编码环节去解决：对开发过的代码要求提供单元测试和开发后的Code Review；在技术开发之前进行沟通，规避可能出现的过度设计问题等等。不过，我想了想，这些更多属于规范性的问题，各个公司自有自身的特色在，那么还有什么内容可以更适合初中级开发一块聊一聊的呢？\n那么毫无疑问就是对依赖库的选择。这部分的内容直接与我们在开发过程中会采用的实际最佳实践相关，无论是多人合作项目，还是单枪匹马的开源项目开发，各种依赖库无法避免：它们可以帮我们有效提升开发效率，帮我们落地最佳实践，让我们更快的更高效的完成工作。因此，我打算从这个系列中，一方面去聊聊我们未来会介绍的依赖库的使用；另外一方面，也想通过这个系列，聊聊如何是使用依赖库的过程中，将各种最佳实践一一落地。我们可能会介绍为什么选择这个依赖库，这个依赖库会带给我们的实践会是怎么样的。\n不过在这里仍旧提醒一句，这里提到的所有的功能和实践均为在实际项目中总结的内容，部分实践则可根据个人情况进行实际选择。所有的实践并非全部都是最优解，仍需根据实际项目情况进行对应的调整和抉择。\n最后，希望你们能够喜欢这个系列。：）\n","date":"2019-11-23T22:43:00Z","permalink":"https://www.4async.com/2019/11/2019-11-23-learning-go-from-3rd-library/","title":"从Go语言的依赖库讲起（0）：让我们聊聊如何开发更方便维护的Go语言程序"},{"content":"原文：Using Go Modules 作者：Tyler Bui-Palsulich、Eno Compton\n介绍 Go 1.11和1.12包含了初步的modules支持，Go的新版本管理系统用于依赖版本信息描述和更方便的管理。这篇博客是一个关于开始使用modules的基础操作指引教程。后续文章会介绍发布一个其他人可以使用的modules。\nmodules是Go包的集合，保存在顶层目录一个名叫go.mod的文件中。go.mod文件定义了模块的路径，这个会作为根目录引用路径；同时文件中也包含了能够正常构建的其他包依赖需求。每个依赖需求同样以模块路径方式标示，同时根据语义化版本方式进行标记。\n在Go 1.11开始，go命令行会再当前目录或者上层目录中存在go.mod文件并且在 $GOPATH/src目录外时自动启用modules功能。（当目录位于$GOPATH/src中时，出于兼容性考虑，go命令仍旧采用GOPATH模式，即便存在go.mod文件。具体请参考Go命令行文档）。从Go 1.13版本开始，modules功能将会在所有开发过程中默认开启。\n这篇博客会演示使用modules开发Go代码的一系列的常用操作：\n创建一个模块 添加依赖 升级依赖 添加一个依赖的新主版本 升级一个依赖到新主版本 移除无用依赖 创建一个新的模块 让我门从创建一个新模块开始。\n在$GOPATH/src外创建一个新的空文件夹，使用cd切换进入这个目录，然后创建一个名叫hello.go的新源码文件：\npackage hello func Hello() string { return \u0026#34;Hello, world.\u0026#34; } 让我们同样创建一个名叫hello_test.go的测试文件：\npackage hello import \u0026#34;testing\u0026#34; func TestHello(t *testing.T) { want := \u0026#34;Hello, world.\u0026#34; if got := Hello(); got != want { t.Errorf(\u0026#34;Hello() = %q, want %q\u0026#34;, got, want) } } 现在，这个目录包含了一个包，但是它并不是一个模块，因为还没有go.mod文件。如果你文件创建在 /home/gopher/hello目录下，执行go test命令时，我们可以看到结果：\n$ go test PASS ok _/home/gopher/hello\t0.020s $ 最后一行表示了整个包测试的汇总信息。因为我们现在在 $GOPATH外，并且不属于任何模块，因此 go命令不知道当前目录的引用路径，因此采用当前文件夹名生成了一个伪路径。\n现在让我们使用go mod init命令将当前目录设置成为模块的根目录，然后重新试试go test命令：\n$ go mod init example.com/hello go: creating new go.mod: module example.com/hello $ go test PASS ok example.com/hello\t0.020s $ 恭喜你！你已经编写和测试了你的第一个模块。\ngo mod init命令会创建一个go.mod文件：\n$ cat go.mod module example.com/hello go 1.12 $ go.mod文件仅出现在模块的根目录下。子目录中的包引用路径会使用模块的引用路径加上子目录路径的形式。举个例子，如果我们创建了一个名叫world的子目录，我们不需要在子目录中使用go mod init命令。包会自动识别作为example.com/hello 中的一部分，引用路径为example.com/hello/world。\n添加一个依赖 Go modules 功能的主要动机就是提升使用其他开发者代码(或者说添加一个依赖项)时的体验。\n让我们更新一下hello.go，引入rsc.io/quote并且使用它实现Hello：\npackage hello import \u0026#34;rsc.io/quote\u0026#34; func Hello() string { return quote.Hello() } 现在让我们再次执行测试：\n$ go test go: finding rsc.io/quote v1.5.2 go: downloading rsc.io/quote v1.5.2 go: extracting rsc.io/quote v1.5.2 go: finding rsc.io/sampler v1.3.0 go: finding golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c go: downloading rsc.io/sampler v1.3.0 go: extracting rsc.io/sampler v1.3.0 go: downloading golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c go: extracting golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c PASS ok example.com/hello\t0.023s $ go命令会自动处理go.mod中制定的依赖版本。当包中import指向的模块没有在 go.mod文件中， go命令会自动搜索这个模块，并且将最新版本添加到 go.mod文件中。（“最新版本”是指最新的标签非预发布的稳定版本。）在我们的例子中，go test解析了新的引用路径rsc.io/quote为对应的模块，版本为v1.5.2。它同样下载了rsc.io/quote中使用的两个依赖，名为rsc.io/sampler和golang.org/x/text。不过，只有直接使用的依赖会被记录在go.mod文件中。\n$ cat go.mod module example.com/hello go 1.12 require rsc.io/quote v1.5.2 $ 第二次执行go test命令时不会重复这个工作，因为 go.mod 已经最新并且所有的下载模块都会缓存到本地（保存在$GOPATH/pkg/mod目录中）：\n$ go test PASS ok example.com/hello\t0.020s $ 值得注意的是，虽然go命令添加新依赖简单便捷，但是它并不是没有成本的。你的模块递归依赖新的依赖时可能会有正确性、安全性和非正当授权等等问题。更多的思考，可以参考Russ Cox的博客我们的软件依赖问题。\n如同我们上面看到的那样，添加一新的依赖通常会带来新的间接依赖。命令go list -m all会列出虽有的当前依赖和他们的依赖\n$ go list -m all example.com/hello golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c rsc.io/quote v1.5.2 rsc.io/sampler v1.3.0 $ 在go list输出中，当前模块，或者叫做主模块，通常是第一行，接下来是根据依赖路径排序的依赖。\ngolang.org/x/text的版本v0.0.0-20170915032832-14c0d48ead0c是伪版本的例子，是go命令的版本语法用于标记未打标签的提交。\n同时，go.mod和go命令维护了一个名叫go.sum的文件包含了指定模块版本的期望的加密hash：\n$ cat go.sum golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c h1:qgOY6WgZO... golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c/go.mod h1:Nq... rsc.io/quote v1.5.2 h1:w5fcysjrx7yqtD/aO+QwRjYZOKnaM9Uh2b40tElTs3... rsc.io/quote v1.5.2/go.mod h1:LzX7hefJvL54yjefDEDHNONDjII0t9xZLPX... rsc.io/sampler v1.3.0 h1:7uVkIFmeBqHfdjD+gZwtXXI+RODJ2Wc4O7MPEh/Q... rsc.io/sampler v1.3.0/go.mod h1:T1hPZKmBbMNahiBKFy5HrXp6adAjACjK9... $ go命令使用go.sum文件保证之后的模块下载会下载到跟第一次下载相同的文件内容，保证你的项目依赖不会发生预期外的恶意修改、意外问题和其他问题。go.mod 和 go.sum都需要放进版本管理中。\n升级依赖 在Go modules中，版本采用语义化版本标签标记版本。语义化版本包含三部分：主要版本、次要版本和修订版本。举一个例子，比如，v0.1.2，主要版本为0，次要版本为1，修订版本为2.\n让我们在这一节中先升级一下次要版本，下一节中我们再尝试升级一下主要版本。\n从go list -m all的输出中，我们可以看到我们使用了 golang.org/x/text的一个没有标签的版本，让我们吧这个版本升级成最新版本，然后测试一下所有的功能是否正常工作：\n$ go get golang.org/x/text go: finding golang.org/x/text v0.3.0 go: downloading golang.org/x/text v0.3.0 go: extracting golang.org/x/text v0.3.0 $ go test PASS ok example.com/hello\t0.013s $ 哇哦，所有的功能都正常工作。\n让我们看一下go list -m all和go.mod文件：\n$ go list -m all example.com/hello golang.org/x/text v0.3.0 rsc.io/quote v1.5.2 rsc.io/sampler v1.3.0 $ cat go.mod module example.com/hello go 1.12 require ( golang.org/x/text v0.3.0 // indirect rsc.io/quote v1.5.2 ) $ golang.org/x/text文件被更新到了最新的标签版本v0.3.0。 go.mod文件中的记录也被更新成了v0.3.0。indirect 注释标记了依赖不是被当前模块直接使用的，只是在其他依赖项中被间接引用。具体内容可以通过go help modules查看更多介绍。\n现在，我们可以尝试升级rsc.io/sampler的小版本，同样的，我们使用go get指令后执行测试：\n$ go get rsc.io/sampler go: finding rsc.io/sampler v1.99.99 go: downloading rsc.io/sampler v1.99.99 go: extracting rsc.io/sampler v1.99.99 $ go test --- FAIL: TestHello (0.00s) hello_test.go:8: Hello() = \u0026#34;99 bottles of beer on the wall, 99 bottles of beer, ...\u0026#34;, want \u0026#34;Hello, world.\u0026#34; FAIL exit status 1 FAIL\texample.com/hello\t0.014s $ 噢，不！测试显示最新版的 rsc.io/sampler不兼容我们的使用方式，让我们看一下这个模块所有可用的标签版本：\n$ go list -m -versions rsc.io/sampler rsc.io/sampler v1.0.0 v1.2.0 v1.2.1 v1.3.0 v1.3.1 v1.99.99 $ 我们在使用的版本是v1.3.0，看上去v1.99.99明显不能使用，或许我们可以使用v1.3.1版本：\n$ go get rsc.io/sampler@v1.3.1 go: finding rsc.io/sampler v1.3.1 go: downloading rsc.io/sampler v1.3.1 go: extracting rsc.io/sampler v1.3.1 $ go test PASS ok example.com/hello\t0.022s $ 注意go get命令中我们使用了@v1.3.1作为参数。通常情况下go get可以指定参数标记使用的版本，这个默认值为@latest，对应会尝试使用最新的版本。\n添加一个依赖的主要版本 让我们添加一个新的函数到我们的包中：func Proverb 会返回Go的并发箴言，这个功能是在rsc.io/quote/v3模块中通过调用 quote.Concurrency实现的。\n现在，我们来更新hello.go 添加新的函数：\npackage hello import ( \u0026#34;rsc.io/quote\u0026#34; quoteV3 \u0026#34;rsc.io/quote/v3\u0026#34; ) func Hello() string { return quote.Hello() } func Proverb() string { return quoteV3.Concurrency() } 现在，我们来添加对应的测试：\nfunc TestProverb(t *testing.T) { want := \u0026#34;Concurrency is not parallelism.\u0026#34; if got := Proverb(); got != want { t.Errorf(\u0026#34;Proverb() = %q, want %q\u0026#34;, got, want) } } 现在让我们测试一下：\n$ go test go: finding rsc.io/quote/v3 v3.1.0 go: downloading rsc.io/quote/v3 v3.1.0 go: extracting rsc.io/quote/v3 v3.1.0 PASS ok example.com/hello\t0.024s $ 注意，我们同时使用了rsc.io/quote和rsc.io/quote/v3：\n$ go list -m rsc.io/q... rsc.io/quote v1.5.2 rsc.io/quote/v3 v3.1.0 $ 每个不同主要版本（v1, v2等等）可以使用不用的引用路径：路径采用主要版本结尾。在这个例子中，我们使用的rsc.io/quote的v3 版本不再是rsc.io/quote，而是rsc.io/quote/v3。这个叫做语义化引用版本，可以让不兼容的包（通常是不同主版本）拥有不同的路径。在差异中rsc.io/quote的v1.6.0版本需要向前兼容v1.5.2，这样就可以重用rsc.io/quote名称。（在前一节中， rsc.io/sampler的v1.99.99应该需要兼容rsc.io/sampler的v1.3.0版本，但是Bug或者其他原因都可能导致这种问题的出现。）\ngo命令允许一个构建包含一个指定模块路径的最多一个版本，以为着每个主要版本都可以包含最多一个版本：一个rsc.io/quote，一个rsc.io/quote/v2和一个rsc.io/quote/v3等等。\n这规定了模块作者一个清晰的单一模块路径的规则：可以一个程序既可以在rsc.io/quote的v1.5.2和v1.6.0版本下构建通过。同时，允许不同的模块主版本（因为有不同路径）让模块的消费者能够增量升级至新主版本中。在这个例子中，我们希望使用rsc/quote/v3 v3.1.0中的quote.Concurrency，但是我们没做好合并我们rsc.io/quote v1.5.2的使用方式时，这个能力可以帮助到我们。这个能力在大型代码的项目中的增量更新中尤为重要。\n升级一个依赖到新主版本 现在，让我们完成将使用rsc.io/quote转换为仅使用rsc.io/quote/v3。因为主版本的变化，我们可以预计到一些API已经删除，改名或者发生了一些不兼容的变化。阅读一下文档我们可以知道，Hello现在已经变为了HelloV3：\n$ go doc rsc.io/quote/v3 package quote // import \u0026#34;rsc.io/quote\u0026#34; Package quote collects pithy sayings. func Concurrency() string func GlassV3() string func GoV3() string func HelloV3() string func OptV3() string $ （这里输出中有一个已知BUG，显示内容中引用路径错误的丢弃了/v3。）\n我们可以更新我们hello.go文件中的quote.Hello()为quoteV3.Hello()：\npackage hello import quoteV3 \u0026#34;rsc.io/quote/v3\u0026#34; func Hello() string { return quoteV3.Hello() } func Proverb() string { return quoteV3.Concurrency() } 这时，我们已经不再需要重命名引用了，我们可以移除掉这些：\npackage hello import \u0026#34;rsc.io/quote/v3\u0026#34; func Hello() string { return quote.Hello() } func Proverb() string { return quote.Concurrency() } 现在我们重新运行一下测试，确保所有的功能都能够正常运行。\n$ go test PASS ok example.com/hello 0.014s 移除无用依赖 我们已经移除了所有rsc.io/quote的使用，但是当我们执行go list -m all命令时，我们仍旧可以看到它出现在我们的go.mod文件中。\n$ go list -m all example.com/hello golang.org/x/text v0.3.0 rsc.io/quote v1.5.2 rsc.io/quote/v3 v3.1.0 rsc.io/sampler v1.3.1 $ cat go.mod module example.com/hello go 1.12 require ( golang.org/x/text v0.3.0 // indirect rsc.io/quote v1.5.2 rsc.io/quote/v3 v3.0.0 rsc.io/sampler v1.3.1 // indirect ) $ 为什么呢？因为当我们构建一个单独的包时，如执行go build或者go test时，很容易得知哪些包遗失了或者需要被添加，但是很难确认哪些包可以被安全移除。只有在检查完模块所有的包和可能的构建标签组合之后，才能移除一个依赖。普通的构建命令不会夹在这些信息，因此无法安全的移除依赖。\ngo mod tidy命令则可以帮助清理无用依赖：\n$ go mod tidy $ go list -m all example.com/hello golang.org/x/text v0.3.0 rsc.io/quote/v3 v3.1.0 rsc.io/sampler v1.3.1 $ cat go.mod module example.com/hello go 1.12 require ( golang.org/x/text v0.3.0 // indirect rsc.io/quote/v3 v3.1.0 rsc.io/sampler v1.3.1 // indirect ) $ go test PASS ok example.com/hello\t0.020s $ 总结 Go modules功能是Go中依赖管理的未来。模块功能现在已经在目前受技术支持的Go版本中可用（目前是Go1.11和Go1.12）。\n这篇文章介绍了使用Go modules的一些工作流：\ngo mod init创建一个新的模块，初始化go.mod文件。 go build和go test命令和其他一些包构建命令添加必要新依赖到go.mod文件。 go list -m all打印当前模块依赖。 go get命令修改依赖的版本（或者新增依赖）。 go mod tidy移除无用依赖。 我们鼓励你从现在开始在你的本地开发中启用modules功能，并且添加go.mod 和 go.sum文件到项目中。你可以给我们发送BUG反馈或者是体验报告，反馈和帮助Go依赖管理的未来演进。\n感谢你们的反馈帮助提升modules功能。\n","date":"2019-03-20T14:22:00Z","permalink":"https://www.4async.com/2019/03/2019-03-20-using-go-modules/","title":"使用Go Modules"},{"content":"在升级到Python 3.5+版本之后，最大的项目管理优化来自于PEP-484 Type Hint的引入。借助Type Hint，我们可以进一步提升Python代码的类型标注，保障在重构过程中避免出现一些低级失误。\n我们可以通过高版本Python新加的新语法启用这项特性，然后通过mypy等工具检查：\ndef greeting(name: str) -\u0026gt; str: return \u0026#39;Hello \u0026#39; + name 然而，在实际实践过程中，也往往存在一些问题，这些问题来自于很多方面：\n你难免有一些历史性代码，这些代码需要同时支持Python2和Python3。 你使用的Python 3 Only的第三方库没有Type Hint定义，作者看起来并不打算近期支持或者某些原因（比如你正在用的某个老版本不维护了）导致你不能贡献代码。 这些时候你往往是不能直接使用Type Hint带来的便利的，不过PEP-484中也考虑到了这种情况，包含了一种stub file的定义形式，可以通过第三方文件.pyi文件，定义函数参数类型与返回值类型。\n比如官方文档中就定义了一个比较简单的stub例子。假设我们对一个名叫HttpRequest 的类进行类型标记：\n# request.py class HttpRequest: \u0026#34;\u0026#34;\u0026#34;A basic HTTP request.\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.META = {\u0026#34;SERVER_NAME\u0026#34;: \u0026#34;hello.com\u0026#34;} def get_raw_host(self): \u0026#34;\u0026#34;\u0026#34; Return the HTTP host using the environment or request headers. Skip allowed hosts protection, so may return an insecure host. \u0026#34;\u0026#34;\u0026#34; # Reconstruct the host using the algorithm from PEP 333. host = self.META[\u0026#34;SERVER_NAME\u0026#34;] return host 接下来，我们可以在当前文件的文件夹下新建一个名叫bytes.pyi的文件，填写如下内容：\n# request.py class HttpRequest: def get_raw_host(self) -\u0026gt; str: ... 让我们先构建一个存在类型混淆的例子，如果你使用了mypy之类的工具进行检查，则可以快速发现下面例子中本来为int型的i被重新赋值了str类型。\n通过这样的方法，可以快速将已有的代码库快速添加类型。不过实际中并不是这么简单。我们之前也说过，很多项目并不是由我们自行维护的，这个时候，我们很难去修改上游仓库时，应该如何做呢？\n这里，我们以mypy工具和Django这个Python常用为例去演示一下如何配置Django项目的类型检查。\n这里有一个第三方提供的Django类型标注库django-stubs，我们可以通过pip命令进行安装：\npip install django-stubs 在安装完成之后，因为这个标注文件时单独发布的包，因此我们需要修改mypy的指向，让mypy使用这个包的标注。需要修改mypy.ini文件用于定义。关于这个文件的具体配置，则可以参考mypy工具的官方文档。\n[mypy] plugins = mypy_django_plugin.main 接下来就可以使用mypy针对Django部分进行检查了。\n","date":"2019-01-29T14:30:00Z","permalink":"https://www.4async.com/2019/01/2019-01-28-python-typing-with-stub-files/","title":"利用Stub File标注Python文件"},{"content":"今天早些时候，golang/x/exp中默默的更新了一个名曰xerrors的包，这个包和同样处于golang/x/exp下的另一个名叫errors的包名字十分相似，就连介绍也都一致：\nPackage errors implements functions to manipulate errors. This package implements the Go 2 draft designs for error inspection and printing 从目前的情况来看，基本上错误的处理形式基本已经定型，处理方式则是类似于之前的另一个github.com/pkg/errors包，但是具体细节不尽相同。\n如何处理error？ 在之前介绍文章中提到过github.com/pkg/errors包的设计思路，那么在Go团队的实现中，这种思路也得到了继承。先从一个小例子开始：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;golang.org/x/exp/xerrors\u0026#34; ) func raiseError() error { return xerrors.New(\u0026#34;a new error\u0026#34;) } func main() { err := xerrors.Errorf(\u0026#34;raiseError: %w\u0026#34;, raiseError()) fmt.Println(err) } 输出结果：\nraiseError: a new error 看起来非常类似于之前github.com/pkg/errors的显示内容。而其中xerrors.Errorf则充当了之前errors.Wrap的功能。 其中值得一提的是%w，这个用于包装错误，后续验证错误中也会用到其中的值。\n同时，这个包中也包含了几个非常有用的辅助函数，分别是：验证错误类型方法Is、错误类型转换方法As、错误关系链解除方法Opaque和提取内层错误方法Unwrap。我们可以用一个简单的演示来说明这种关系：\nvar ( ErrBase = xerrors.New(\u0026#34;a new error\u0026#34;) ) func main() { err := xerrors.Errorf(\u0026#34;raiseError: %w\u0026#34;, ErrBase) fmt.Println(ErrBase == ErrBase) // 地址相同 fmt.Println(err == ErrBase) // 基于ErrBase包装之后不同 fmt.Println(xerrors.Is(err, ErrBase)) // 验证是否为基于ErrBase fmt.Println(xerrors.Opaque(err) == err) // 解除关系链之后不为相同地址 fmt.Println(xerrors.Is(xerrors.Opaque(err), ErrBase)) // 解除关系链之后无法确定关系 fmt.Println(xerrors.Unwrap(err) == ErrBase) // 获取内层错误为原错误，地址相同 } 输出结果为：\ntrue false true false false true 即便是在包裹多层之后，错误类型也能通过Is方法正确识别：\nfunc main() { err := xerrors.Errorf(\u0026#34;raiseError: %w\u0026#34;, ErrBase) err2 := xerrors.Errorf(\u0026#34;wrap#01: %w\u0026#34;, err) err3 := xerrors.Errorf(\u0026#34;wrap#02: %w\u0026#34;, err2) fmt.Println(xerrors.Is(err, ErrBase)) fmt.Println(xerrors.Is(err3, ErrBase)) // 能够正确识别关系，打印为true } 如果需要打印详细的调用链路，则可以通过标准库fmt相关功能进行输出，比如Printf或者Sprintf等。\nfunc main() { err := xerrors.Errorf(\u0026#34;raiseError: %w\u0026#34;, ErrBase) err2 := xerrors.Errorf(\u0026#34;wrap#01: %w\u0026#34;, err) err3 := xerrors.Errorf(\u0026#34;wrap#02: %w\u0026#34;, err2) fmt.Printf(\u0026#34;%+v\\n\u0026#34;, err3) } 输出链路如下：\nwrap#02: main.main /.../error.go:16 - wrap#01: main.main /.../error.go:15 - raiseError: main.main /.../error.go:14 - a new error: main.init /.../error.go:10 但是注意，使用%w进行包装时，仅能单次包装单个错误，比如下面这种两个错误同时包装时，是会无法识别的：\nvar ( ErrBase = xerrors.New(\u0026#34;a new error\u0026#34;) ErrBase2 = xerrors.New(\u0026#34;another new error\u0026#34;) ) func main() { err := xerrors.Errorf(\u0026#34;raiseError: %w, %w\u0026#34;, ErrBase2, ErrBase2) fmt.Println(xerrors.Is(err, ErrBase)) fmt.Println(xerrors.Is(err, ErrBase2)) } 落地使用 在具体项目实现中，如对外提供的库文件中，我建议可以参考其他语言的实现方式，定义一个特殊的基础错误类型ErrBase，所有其他错误类型均由此类型派生，则后续外部在使用过程中，只需通过xerros.Is判定错误类型，则可以快速判定错误是否来自于本包，并且可以借助此功能统一单独处理来自于此第三方包的异常。\n如果你有其他的建议和疑问，也欢迎在留言区中留言讨论。另外，根据实际实现代码来看，其中还有一些边缘情况未考虑或者未做处理，由于是试验性库，本库在使用过程中需自行承担所带来的风险。\n","date":"2019-01-25T23:20:00Z","image":"https://www.4async.com/2019/01/2019-01-25-go-new-xerrors/cover_hue4ddd2aac593f64f6c4b0db3c64deb1c_32610_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.4async.com/2019/01/2019-01-25-go-new-xerrors/","title":"聊聊新的Go语言错误处理方案"},{"content":"Gin框架是一个Go语言框架中非常受欢迎的一款。今天我们开放了一个gini库，结合了我们实际开发中的使用，描述一下我们为什么开发这个库，在开发过程中的权衡。\ngini简介 gini库目前发布了v0.1.0版本，支持几个常见的功能：\n提供请求数据和返回数据打印记录中间件 提供可Mock化Bind方式 提供统一输出格式管理 目前gini库功能已经实际落地在开发过程中，在采用gin框架开发的程序中获得了应用。\n设计思路 在开发调试过程中，对于前后端分离模式开发过程中部分初级开发者无法进行方便的调试，尤其是在某些特殊场景（如：蓝牙通讯场景）下。借助gini.DumpReqAndResp可以非常方便的打印各种信息。不过非常值得注意的是，这个操作存在严重的数据泄漏风险，是严禁发布至线上版本中运行的。\nr := gin.New() r.Use(gini.DumpReqAndResp()) 在实际开发测试中，通常我们进行HTTP请求测试一般会采用如下的形式编写代码：\nr := gin.New() ... w := httptest.NewRecorder() req, _ := http.NewRequest(\u0026#34;GET\u0026#34;, \u0026#34;xxx\u0026#34;, nil) r.ServeHTTP(w, req) assert.Equal(t, 200, w.Code) ... 但是在实际使用过程中，我们觉得在实际过程中，gin由于设计原因，无法进行更好的进行HandlerFunc分离处理。所以我们选择使用gini.Bind和gini. JSONRenderWrap分离请求处理流程与返回数据流程。让业务代码无需关心核心数据，将HTTP请求转化为类似函数式的测试方式。以一个简单的内容为例\npackage main import ( \u0026#34;errors\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/ipfans/gini\u0026#34; ) var ( ErrNotValid = errors.New(\u0026#34;请求数据不正确\u0026#34;) ) // Echo data. func Echo(c *gin.Context) error { var data map[string]string err := gini.Bind(c, \u0026amp;data) if err != nil { return ErrNotValid } c.Set(\u0026#34;data\u0026#34;, data) return nil } func main() { gin.SetMode(gin.ReleaseMode) gini.RegisterError(ErrNotValid, 400, ErrNotValid.Error()) r := gin.Default() r.Use(gini.DumpReqAndResp()) r.GET(\u0026#34;/echo\u0026#34;, gini.JSONRenderWrap(Echo)) r.Run() } 测试过程中，借助gini.Bind可以替换的特性，我们可以直接进行数据的定义。比如，我们可以这样编写测试代码：\nfunc TestEcho(t *testing.T) { type args struct { reqBody string } tests := []struct { name string args args want interface{} wantErr bool }{ { \u0026#34;Normal\u0026#34;, args{ \u0026#34;{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;kevin\\\u0026#34;}\u0026#34;, }, map[string]string{\u0026#34;name\u0026#34;: \u0026#34;kevin\u0026#34;}, false, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { c := \u0026amp;gin.Context{} b := \u0026amp;gini.MockJSONBinder{} b.Body(tt.args.reqBody) gini.SetBinder(b) if err := Echo(c); (err != nil) != tt.wantErr { t.Errorf(\u0026#34;Echo() error = %v, wantErr %v\u0026#34;, err, tt.wantErr) } if data, _ := c.Get(\u0026#34;data\u0026#34;); !assert.Equal(t, tt.want, data) { t.Errorf(\u0026#34;Echo() want = %v, got %v\u0026#34;, tt.want, data) } }) } } 这个似乎看起来是比HTTP请求方式并没有什么太多区别，但是实际开发过程中，我们会遇到各种认证等等前提条件式验证，这在开发过程中常常需要做额外的fixtures帮助，但是借助这种方式，则可以更加方便的观察书输入输出，同时在测试过程中，绕过可能会出现的额外的数据内容。\n同时你户注意到，我们通常在使用json格式作为返回数据，并且包含固定的返回数据格式，这也更方便的进行统一数据管理，方便修正数据输出格式（如：可以切换成yaml输出？切换字段内容和定义）。为了方便更好的兼容数据处理，提供了gini.RegisterError方法，该方法则更好的提供返回状态值与提示信息的定义。如果未进行定义，则默认status为500，msg默认为error.Error()。\ngini.RegisterError(ErrNotValid, 400, ErrNotValid.Error()) 结尾 gini库是我们在实践中总结出来的一套合作规约，难免存在各种由于思路与技术存在的短板，如果你有更好的建议与思路，非常欢迎到issues中与我们一起讨论。\n","date":"2019-01-18T23:20:00Z","permalink":"https://www.4async.com/2019/01/2019-01-18-gin-toolkit-gini/","title":"gini：一个让你更方便使用Gin框架的库"},{"content":"没想到今年突然有动力还能写一篇。可能来自于两个方面，有些东西需要整理，另外有一个事情我是感觉到很诧异的。今天技术圈最火的事情当属AntDesign彩蛋事件了（以下简称AntD），很不幸的是，本司也是本次受害者之一。\n考虑到AntD是一个使用十分广泛并且关注度非常高的国内项目，并且有阿里支付宝的背书效应，这一次我想影响的会在两个方面：一个是阿里开源本身的信用，一个是开源项目在国内的推广。当然，我们确实在引入AntD方面确实太过草率：我其实关注AntD了大概一年时间，考虑到issue情况和项目开发活跃度情况，要求团队引入此框架希望可以提升开发效率，没想到相关的项目1-2个月就出现了这种事情，真是令人实在痛心。\n这件事情也暴露出来，我们以前在使用开源项目时，太过信任\n另外，作为最后的一个建议，我们希望如果大家以后开源项目，如果做不到，请不要说自己是__企业级项目__，因为一般也就是放在自己企业里面用，这跟企业级项目是两个概念。至少我们具体到AntD这个项目上，这个项目无论代码开发和管控上，都是算不上的。\n","date":"2018-12-25T23:20:00Z","permalink":"https://www.4async.com/2018/12/2018-12-25-scm-security/","title":"漫谈代码安全：也来聊一聊AntD这档子事"},{"content":"最近我们在把线上系统升级至Go 1.10版本时发现，在我们实现的某些接口中，出现了客户反馈调用失败提示参数缺失的情况。这种情况我们在测试过程中未能复线，后来经过了解，发现了故障原因为Go 1.10版本升级过程中更改了部分程序代码与验证逻辑导致的。\n故障说明 为了方面说明，我们假设我们存在一个要求以application/x-www-urlencoded-form的Content-Type提交的表单，在具体的实现过程中，我们采用标准库net/http中的ParseForm方法处理表单，最后获取结果。\n但是在使用过程中，某些开发人员素质不足，从网上检索部分代码复制粘贴（如部分Java开发），导致提交multipart/formdata数据到服务器进行请求。这个请求方式在ParseForm处理中是可以被接受的，代码会根据Content-Type进行处理：如果是提供了multipart的数据，Go内部也会正常处理。\n在Go1.10在开发过程中，在CL70931中进行了修改，为了兼容RFC2388中文件名称可选的问题。在修改之后，在使用multipart/formdata方式提交的表单会被统一识别成文件，只能通过http.Request.FormFile方式读取。这样就导致我们使用FormValue读取表单数据后读取结果为空。\n故障处理 处理方式不碍乎两种，一种是推动用户修改自己的代码，这种不需要额外修改。\n另外一种主要是己方动作，兼容Go1.10+版本带来的问题（或者降级1.9版本）。这里主要介绍一下第二种方法的处理。\n在第二种方法中，我们需要将使用FormValue读取的数据更换为FormFile获取文件形式进行获取。这个可以参考如下伪代码进行修改（未处理异常情况，请自行修改）：\nf, _, err := req.FormFile(key) if err == nil { b, _ := ioutil.ReadAll(f) value = string(b) } 截止目前为止（Go1.10.3），该兼容性问题一直尚未处理，大家可以根据自己的情况自行判断解决方案。\n2018年11月更新 此问题在Go 1.11.1版本中回滚，可选择Go 1.11.1以后版本编译原程序即可。\n","date":"2018-06-08T12:20:00Z","permalink":"https://www.4async.com/2018/06/2018-06-08-compatibility-in-golang-1-10/","title":"Go 1.10中的一处不兼容问题"},{"content":"今天正好看到一篇关于敏感信息过滤的文章，这算做一个interface实际应用的一些举例和应用。\n例子中介绍了一种比较常见的使用场景：使用JSON保存数据时的对诸如用户密码等信息进行保护时候应该做的事情。作者以使用JSON格式保存用户账户和密码为例，讲解了使用json.Unmarshaler接口类型过滤敏感信息。\n比如，对于保存敏感数据的结构体：\ntype Credentials struct { Email string `json:”email”` Password string `json:”password”` } func (co Credentials) MarshalJSON() ([]byte, error) { type credentials Credentials cn := credentials(co) cn.Password = \u0026#34;[REDACTED]\u0026#34; return json.Marshal((*credentials)(\u0026amp;cn)) } 通过定义MarshalJSON方法满足json.Unmarshaler接口类型的要求，这样，当使用json.Unmarshal等方法时，就可以规避掉在日志或者JSON接口之类的方法中输出敏感信息Password。\n文中提及了json.Unmarshaler接口一个方法，但这种方法并不是完全能够解决所有的类型的敏感信息过滤问题。比如在使用调试过程中，开发人员常常使用的fmt/log包，则不能用这种方法解决。\n要解决这个问题，则需要使用另外一个值得注意的接口类型，那么就是fmt.Stringer接口类型。该接口类型通常用于如log/fmt之类的包的输出中。\n实际上，我个人认为非常合适的方法是，我们可以特定某个特殊类型Sensitivity，对于敏感信息统一采用这个类型予以保护。这样也方便我们后续添加新的保护方式。\n看一下这个敏感信息如何过滤：\ntype Sensitivity string func (s Sensitivity) String() string { return \u0026#34;[SENSITIVE DATA]\u0026#34; } ... request := CreateUserRequest{ Credentials: Credentials{ Email: \u0026#34;bilbro@theshire.net\u0026#34;, Password: \u0026#34;theonering\u0026#34;, }, } fmt.Println(\u0026#34;request:\u0026#34;, request) 输出结果为：\nrequest: {{bilbro@theshire.net [SENSITIVE DATA]}} 同样的，我们结合第一个方法中的json.Unmarshaler一起使用时，那么就是一个比较完整的敏感信息过滤方案了。\ntype Sensitivity string func (s Sensitivity) String() string { return \u0026#34;[SENSITIVE DATA]\u0026#34; } func (s Sensitivity) MarshalJSON() ([]byte, error) { return []byte(`\u0026#34;[SENSITIVE DATA]\u0026#34;`), nil } 这种方式也能很好的兼容各种数据库ORM库，保证我们的功能可以正常应用在数据模型等场景上：\nsess, _ := mgo.Dial(\u0026#34;\u0026#34;) sess.DB(\u0026#34;test\u0026#34;).C(\u0026#34;data\u0026#34;).Insert(\u0026amp;request.Credentials) var c Credentials sess.DB(\u0026#34;test\u0026#34;).C(\u0026#34;data\u0026#34;).Find(bson.M{}).One(\u0026amp;c) fmt.Println(c.Password == \u0026#34;theonering\u0026#34;) ... db, _ := gorm.Open(\u0026#34;mysql\u0026#34;, \u0026#34;root:pwd@/ytest?charset=utf8\u0026amp;parseTime=True\u0026amp;loc=Local\u0026#34;) ... db.Create(\u0026amp;request.Credentials) var c Credentials db.First(\u0026amp;c) fmt.Println(c.Password == \u0026#34;theonering\u0026#34;) 类似的还有一个涉及编码方法encoding.TextMarshaler，基本与fmt.Stringer类似，因此也不需要额外的赘述了。\n注意：如果你使用了如fmt.Sprintf之类的格式化请求，也会受到fmt.Stringer接口类型的影响，请根据使用情况酌情使用。\n","date":"2018-06-06T16:07:00Z","permalink":"https://www.4async.com/2018/06/2018-06-06-sensitive-info-filter-in-golang/","title":"关于Golang过滤敏感信息的正确姿势"},{"content":"我们从 Consul 0.6.x 版本开始使用，中间也遇到的一些各种各样的问题，比较常见的操作问题就是 consul 的升级问题（比如解决 BUG，早期 Consul 的 BUG 也遇到了好几个）。\n平滑升级时，我们常见的方式一般为替换 consul 可执行文件，然后执行 Graceful 重启\n常见 Consul 的 Graceful leave 的方法有以下两种：\n发送 SIGINT 信号至 Consul； 连接要升级的 consul，使用命令 consul leave 发送离开命令； 这两种方式都会让该节点主动退出集群并结束进程，如下：\n[INFO] agent: Caught signal: terminated [INFO] agent: Graceful shutdown disabled. Exiting [INFO] agent: Requesting shutdown [INFO] consul: shutting down server [WARN] serf: Shutdown without a Leave [ERR] agent: Coordinate update error: No cluster leader [ERR] agent: failed to sync remote state: No cluster leader [WARN] serf: Shutdown without a Leave [INFO] manager: shutting down [INFO] agent: consul server down [INFO] agent: shutdown complete [INFO] agent: Stopping DNS server 10.135.218.10:53 (tcp) [INFO] agent: Stopping DNS server 10.135.218.10:53 (udp) [INFO] agent: Stopping HTTP server 10.135.218.10:8500 (tcp) [INFO] agent: Waiting for endpoints to shut down [INFO] agent: Endpoints down [INFO] agent: Exit code: 1 之后重新启动 consul 进程即可。\n","date":"2018-05-10T15:20:00Z","permalink":"https://www.4async.com/2018/05/2018-05-10-consul-graceful-stop/","title":"Consul平滑升级的一点建议"},{"content":"感谢Github Page终于支持了Let\u0026rsquo;s Encrypt的HTTPS，很久之前购买的域名终于可以再次发光发热了。\n新的域名： https://www.4async.com\n值得注意的是，在新的域名生效之后，HTTPS需要等待1-2个小时才能启用。\n","date":"2018-05-03T11:20:00Z","permalink":"https://www.4async.com/2018/05/2018-05-03-new-domain/","title":"配置新域名"},{"content":"树莓派支持命令行下配置WiFi连接，可以通过编辑 /etc/wpa_supplicant/wpa_supplicant.conf 文件配置对应的WiFi配置。具体的内容可以使用 wpa_passphrase 工具生成对应的配置信息，附加到该文件结尾即可。\n假设我存在 SSID 为 testing 密码为 testingPassword 的WiFi网络，我可以使用 wpa_passphrase 生成对应的配置字符串：\n$ wpa_passphrase \u0026#34;testing\u0026#34; \u0026#34;testingPassword\u0026#34; network={ ssid=\u0026#34;testing\u0026#34; #psk=\u0026#34;testingPassword\u0026#34; psk=131e1e221f6e06e3911a2d11ff2fac9182665c004de85300f9cac208a6a80531 } 当然你可以在 `psk` 填写原始的密码。 不过在某些极限环境下，我们有可能便装出门，未配置网线等工具，这个时候只有未加入过的WiFi网络，这样如何处理呢？ 这种方式与开启SSH方式类似，在制作的SD卡根目录下，新增/修改对应的 `wpa_supplicant.conf` 文件，添加对应的信息即可。 *注意，该文件包含默认头部信息不能缺少。如Raspbian 20180418版本完整配置为：* ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1\nnetwork={ ssid=\u0026ldquo;testing\u0026rdquo; #psk=\u0026ldquo;testingPassword\u0026rdquo; psk=131e1e221f6e06e3911a2d11ff2fac9182665c004de85300f9cac208a6a80531 }\n","date":"2018-04-20T17:30:00Z","permalink":"https://www.4async.com/2018/04/2018-04-20-setup-wifi-raspberrypi-headless/","title":"树莓派无界面配置WiFi"},{"content":"背景介绍 我们在项目中逐渐切换使用了gRPC作为服务间调用的主要手段，逐步替换RESTful API在目前我们项目中的使用。在使用过程中，gRPC的效率是我们想对比较关心并且从目前来看相对难以优化的组件，所以本文就是探讨如何能够在不修改gRPC源码的前提下尽量提升gRPC的性能。\ngrpc官方提供了性能benchmark可以供大家查看，具体的链接可以在 gRPC Performance Dashboard 中查看。\n性能测试 性能测试采用我们的线上标配，目前由于业务量原因，其实性能并不高，这里只是对比参考，建议你在后续处理过程中自行测试。其实最简单的处理方式就是创建多个gRPC client。\n单Client处理：\n➜ client ./client -n 100000 -c 1 2017/08/04 16:49:39 concurrency: 1 requests per client: 100000 2017/08/04 15:59:39 message size: 581 bytes 2017/08/04 16:00:04 took 24467 ms for 100000 requests 2017/08/04 16:00:04 sent requests : 100000 2017/08/04 16:00:04 received requests : 100000 2017/08/04 16:00:04 received requests_OK : 100000 2017/08/04 16:00:04 throughput (TPS) : 4087 2017/08/04 16:00:04 mean: 244283 ns, median: 241106 ns, max: 6107692 ns, min: 138633 ns, p99: 678374 ns 2017/08/04 16:00:04 mean: 0 ms, median: 0 ms, max: 6 ms, min: 0 ms, p99: 0 ms 修改Client数目为10：\n➜ client ./client -n 100000 -c 10 2017/08/04 16:02:55 concurrency: 10 requests per client: 10000 2017/08/04 16:02:55 message size: 581 bytes 2017/08/04 16:03:01 took 6264 ms for 100000 requests 2017/08/04 16:03:01 sent requests : 100000 2017/08/04 16:03:01 received requests : 100000 2017/08/04 16:03:01 received requests_OK : 100000 2017/08/04 16:03:01 throughput (TPS) : 15964 2017/08/04 16:03:01 mean: 623814 ns, median: 550812 ns, max: 15572759 ns, min: 98979 ns, p99: 5868493 ns 2017/08/04 16:03:01 mean: 0 ms, median: 0 ms, max: 15 ms, min: 0 ms, p99: 5 ms 修改Client数目为20：\n➜ client ./client -n 100000 -c 20 2017/08/04 16:03:48 concurrency: 20 requests per client: 5000 2017/08/04 16:03:48 message size: 581 bytes 2017/08/04 16:03:54 took 6438 ms for 100000 requests 2017/08/04 16:03:54 sent requests : 100000 2017/08/04 16:03:54 received requests : 100000 2017/08/04 16:03:54 received requests_OK : 100000 2017/08/04 16:03:54 throughput (TPS) : 15532 2017/08/04 16:03:54 mean: 1277405 ns, median: 1132744 ns, max: 16286260 ns, min: 99258 ns, p99: 8126886 ns 2017/08/04 16:03:54 mean: 1 ms, median: 1 ms, max: 16 ms, min: 0 ms, p99: 8 ms 修改Client数目为15：\n➜ client ./client -n 100000 -c 15 2017/08/04 16:06:57 concurrency: 15 requests per client: 6666 2017/08/04 16:06:57 message size: 581 bytes 2017/08/04 16:07:02 took 5528 ms for 99990 requests 2017/08/04 16:07:02 sent requests : 99990 2017/08/04 16:07:02 received requests : 99990 2017/08/04 16:07:02 received requests_OK : 99990 2017/08/04 16:07:02 throughput (TPS) : 18087 2017/08/04 16:07:02 mean: 823191 ns, median: 741738 ns, max: 17466607 ns, min: 93208 ns, p99: 5787005 ns 2017/08/04 16:07:02 mean: 0 ms, median: 0 ms, max: 17 ms, min: 0 ms, p99: 5 ms ","date":"2018-04-20T12:00:00Z","permalink":"https://www.4async.com/2018/04/2018-04-20-grpc-performance-optimizing/","title":"gRPC性能优化"},{"content":"face_recognition 是一个热门的人脸识别库，常年占据 Github Trending Python 子类的 Top10。\n在官方文档中，介绍安装 face_recognition 步骤非常简单，只需要执行 pip install face_recognition 就可以了，实际在安装过程中 face_recognition 有一些非 Python 依赖，需要单独安装，本文就对内容进行一个简单介绍，权作记录，方便大家手动安装时提前规避。文章基于 OSX 10.12.6+pyenv+python3.6.2 介绍。\n问题在哪里？ face_recognition 的依赖大部分都比较好安装，只有一个库例外，那就是 dlib 这个依赖。这个依赖需要安装 boost 和 boost-python 两个非 Python 依赖。\n安装 Boost 和 boost-python 这两个库可以通过 Homebrew 快速安装，使用如下命令进行安装：\nbrew install boost brew install boost-python --with-python3 需要注意的是，如果是 Python3 使用，需要额外参数，使用源码编译安装。如果使用 Python2 可以使用 Homebrew 的预编译包。\n确认 Python dlib 要求 Python 编译时使用 --enable-shared 参数编译，默认情况下，pyenv 未启用该参数，因此编译需要使用下面参数进行 Python 重新编译\nPYTHON_CONFIGURE_OPTS=\u0026#34;--with-dtrace --enable-shared\u0026#34; pyenv install 3.6.2 前面一个参数是我一直编译 Python 3.6 使用的参数，方便使用 DTrace 监控 Python 的执行。具体的作用可以参考我之前的文章。\n这样还没有完，因为 dlib 的代码问题，对默认 Python3.6 的 library 识别有问题，因此需要将 Python 安装目录下的 lib/libpython3.6m.dylib 复制为 lib/libpython3.6.dylib。一般 pyenv 目录为 ~/.pyenv/versions/3.6.2/lib/\n安装 face_recognition 接下来就可以使用 pip 正常安装 face_recognition 了，其中 dlib 编译还是需要消耗一些时间的，需要耐心等待。\n","date":"2017-08-02T18:30:00Z","permalink":"https://www.4async.com/2017/08/2017-08-02-install-face-recognition-on-osx/","title":"OSX 下安装 face_recognition"},{"content":"由于过去的历史原因，我们使用的默认 DB 是 MongoDB 数据库。MongoDB 数据库本身在支持非格式化的数据存储方面有比较大的优势，也不需要额外做很多的 Schema Migration，在我们项目初期，数据存储结构变动频繁时帮助非常大。\n但是，随着我们的业务不断增长，我们也遇到了一些问题，这篇文章总结了一些我们在过去的过程中出现的一些问题或者失误，帮助大家在实践过程中进行规避。\n集合使用不当问题 在使用 MongoDB 过程中，我们创建了大量的集合。这种情况在 WiredTriger 引擎下创建大量零散文件。这个在使用过程中暂时未对我们造成实际的业务较大影响，但是从实际使用而言，对整体数据库性能、后续主从复制集群同步，都是有一定影响的。在实际应用过程中应该避免：拥有过多 DB、一个 DB 下有过多 Collection，这样都会导致部分指令的性能大幅度下滑。同时，过多的 DB 也会导致主从同步失败 (listDatabse 超时导致失败，从库退出)。\n数据库索引建立 MongoDB Collection 索引建立功能支持前台创建模式与后台创建模式两种。默认模式为前台创建模式。这种模式下会发生：DB 被锁，主库所有读写被禁止、从库无法访问。从另一方面来说，会导致业务系统的数据库访问受到影响。虽然前台创建模式效率更高，但是如果是线上操作，并且有一定数量级，创建索引时需要添加 {background:true} 让创建索引操作转换为后台操作，尽管时间较长，但是对业务影响较小。同时，该操作也应该在业务量小时进行。\n数据库主从切换 虽然 MongoDB 的 Replica Set 功能可以方便的进行自动主从切换。但是实际使用过程中，我们也会遇到一些需要手工进行主从库切换的情况。比如，进行进行版本升级修复 BUG 或者安全漏洞。但是如果这个时候强行进行 Primary 的重启，则可能会出现未同步至 Secondary 的数据丢失的情况（血泪教训）。这种情况下，重启集群的方式是，优先进行 Secondary 的更新操作，在 Secondary 更新完成后，对 Primary 进行 stepDown 操作，等待主节点降级成为 Secondary 节点，之后进行操作。这样的好处是不会丢失数据。但是如果主从数据差异较大时，有可能会造成降级失败，此时可以重复执行 stepDown 直至成功。还有一个值得注意的是，即便 Secondary 可以停机维护，但是仍旧有可能丢数据，这个与 oplog 大小有关，我们放在后面说。\n慢查询问题 慢查询的记录可以通过 Profiling 进行记录，这部分资料比较多，可以通过 db.setProfilingLevel() 进行管理。同时，主动分析也可以通过 explain 进行预分析。这一部分资料比较多，我这里就不再啰嗦了。但是还有一个问题是，部分慢查询操作不会随着客户端断开中断执行，需要通过 db.currentOp() 和 db.killOp() 功能干掉长时间执行、浪费资源的操作。\n主从同步延迟问题 主从同步问题是比较常见的延迟问题了，主要问题都是出在 oplog 上。毕竟主从同步机制都是通过此实现的。oplog 是固定大小的集合，如果满了，就会自动删除老的数据。\n这里可以看到，此处查询 oplog 操作记录到从库同步，已经执行了 194 毫秒。\n上面我们提到，当 Secondary 停机维护时也可能出现问题，就是因为，当 oplog 不足时，那么就有可能会导致数据同步失败。因此一般数据库停机维护操作，也应该放在业务量小的时候进行。不过 oplog 是可以调整的，可以参考 官方文档 进行调整。注意，这需要重启数据库来执行。\n主从同步延迟一般出现的情况都是数据出现大量插入和修改的情况。在 Secondary 进行 oplog 重放时，开销会大于 Primary 进行操作的开销。这种情况会产生大量的 oplog（每条修改记录一条，非操作），记录越多，同步速度越慢。同时，某些操作也会大幅度提高单条记录的同步成本，比如 $inc、$push 等等会让同步更慢。如果你对数据库中较大量数据进行 $set 操作，同样也会造成数据延迟。我们的一位工程师就因为这样曾经造成了分钟级别的延迟，教训也是非常惨痛的。这种操作比较频繁时，我们可以通过调整 replWriterThreadCount 参数进行 Secondary 重放线程数量调整。在 MongoDB 中默认为 16，这个可以根据情况进行具体调整。不过该参数为 MongoDB 启动参数，调整该参数会要求重启 MongoDB。另外一个值得注意的是这种操作会加大内存开销，如果内存紧张时也需要注意。\n","date":"2017-07-31T18:45:00Z","permalink":"https://www.4async.com/2017/07/2017-07-31-mongodb-optimizing/","title":"一些 MongoDB 的坑"},{"content":"我们在进行服务间调用时广泛采用 gRPC 作为主要的调用协议，借助 gRPC 的模块化与语言无关的特性，可以在我们拓展多语言模块之间提供更好的支持。但是我们在使用 gRPC 之中也出现了一些问题，这些问题会做一些记录，希望可以与大家一起沟通与交流。\n某日，我们的客服反馈，我们的基础设施操作工具出现了长时间无响应的问题。该问题出现在我们对某些设备进行 OTA 升级时，操作长时间无返回，与之前预期的 10 秒内返回存在较大出入。经过我们的工程师分析，我们发现在 gRPC 处理过程中，我们的操作工具通过 gRPC 调用远程服务端接口时，接口长时间没有返回结果。\n我们首先怀疑是 gRPC 调用过程中出现了连接问题。gRPC 过程中可能由于多种原因导致连接断开或者服务器无法连接。在调用 gRPC 方法过程中，我们可以通过 FailFast(true) 方式进行快速失败。实际上，这个值默认情况下为 true。\n那么接下来我们就需要从调用从使用角度上寻找问题。我们使用过程中默许服务端在处理某些操作时进行较长时间操作（如长时间操作），但是从客户端角度而言，部分操作正常情况下我们是希望可以在有预定特定环境下达到某些时间仍旧未返回结果可以标记为结果失败。这样就需要通过 gRPC 的机制进行调控。由于目前我们的接口很多情况下调用接口实际为硬件接口，因此，我们采用通过控制 gRPC 客户端接口超时的方法控制。\n在 gRPC 中，提供了 MethodConfig 用于控制每个方法的超时时间，这样可以对不同的 RPC 方法设置超时。\n下面，我们用 官方的 gRPC 示例 演示如何进行调用超时控制。\n首先，我们在 examples/helloworld/greeter_server/main.go 中的 SayHello 中添加一个长时间操作模拟：time.Sleep(10*time.Second)。\n这时，如果我们需要客户端在 5 秒以内返回结果，应该如何操作呢？\n那么我们修改 examples/helloworld/greeter_client/main.go 中的 main，添加超时处理内容：\nfunc main() { // Set up method timeout configure. var wg sync.WaitGroup wg.Add(1) ch := make(chan grpc.ServiceConfig) go func() {defer wg.Done() mc := grpc.MethodConfig{ WaitForReady: true, Timeout: 5 * time.Second, } m := make(map[string]grpc.MethodConfig) // 格式：/PackageName/ServiceName/MethodName m[\u0026#34;/helloworld.Greeter/SayHello\u0026#34;] = mc sc := grpc.ServiceConfig{Methods: m,} ch \u0026lt;- sc}() // Set up a connection to the server. conn, err := grpc.Dial(address, grpc.WithInsecure()) if err != nil {log.Fatalf(\u0026#34;did not connect: %v\u0026#34;, err) } defer conn.Close() c := pb.NewGreeterClient(conn) // Contact the server and print out its response. name := defaultName if len(os.Args) \u0026gt; 1 {name = os.Args[1] } log.Printf(\u0026#34;Starting...\\n\u0026#34;) // 明确执行时间 r, err := c.SayHello(context.Background(), \u0026amp;pb.HelloRequest{Name: name}) if err != nil {log.Fatalf(\u0026#34;could not greet: %v\u0026#34;, err) } log.Printf(\u0026#34;Greeting: %s\u0026#34;, r.Message) } 其中根据 MethodConfig 对应 map 结构，可以找到对应方法，对指定方法设置超时时间。最后，执行结果如下：\n2017/05/19 15:59:11 Starting call... 2017/05/19 15:59:17 could not greet: rpc error: code = DeadlineExceeded desc = context deadline exceeded ","date":"2017-04-06T18:00:00Z","permalink":"https://www.4async.com/2017/04/2017-05-19-grpc-call-timeout/","title":"gRPC 调用超时控制"},{"content":"这是一篇非常不错的 pandas 分析入门文章，在此简单翻译摘录如下。\n本周，西雅图的自行车共享系统 Pronto CycleShare 一周岁了。 为了庆祝这一点，Pronto 提供了从第一年的数据缓存，并宣布了 Pronto Cycle Share 的数据分析挑战。\n你可以用很多工具分析这些数据，但我的选择工具是 Python。 在这篇文章中，我想展示如何开始分析这些数据，并使用 PyData 技术栈，即 NumPy，Pandas，Matplotlib 和 Seaborn 与其他可用的数据源。\n这篇文章以 Jupyter Notebook 形式组织，它是一种开放的文档格式。结合了文本、代码、数据和图形，并且通过 Web 浏览器查看。本文中的内容可以下载 对应的 Notebook 文件，并通过 Jupyter 打开。\n下载 Pronto 的数据 我们可以从 Pronto 官网 下载对应的 数据文件。总下载大约 70MB，解压缩的文件大约 900MB。\n接下来我们需要导入一些 Python 包：\nIn [2]:\n%matplotlib inline import matplotlib.pyplot as plt import pandas as pd import numpy as np import seaborn as sns; sns.set() 现在我们使用 Pandas 加载所有的行程记录：\nIn [3]:\ntrips = pd.read_csv(\u0026#39;2015_trip_data.csv\u0026#39;, parse_dates=[\u0026#39;starttime\u0026#39;, \u0026#39;stoptime\u0026#39;], infer_datetime_format=True) trips.head() Out[3]:\n这个行程数据集的每一行是由一个人单独骑行，共包含超过 140,000 条数据。\n探索时间与行程的关系 让我们先看看一年中每日行程次数的趋势。\nIn [4]:\n# Find the start date ind = pd.DatetimeIndex(trips.starttime) trips[\u0026#39;date\u0026#39;] = ind.date.astype(\u0026#39;datetime64\u0026#39;) trips[\u0026#39;hour\u0026#39;] = ind.hour In [5]:\n# Count trips by date by_date = trips.pivot_table(\u0026#39;trip_id\u0026#39;, aggfunc=\u0026#39;count\u0026#39;, index=\u0026#39;date\u0026#39;, columns=\u0026#39;usertype\u0026#39;, ) In [6]:\nfig, ax = plt.subplots(2, figsize=(16, 8)) fig.subplots_adjust(hspace=0.4) by_date.iloc[:, 0].plot(ax=ax[0], title=\u0026#39;Annual Members\u0026#39;); by_date.iloc[:, 1].plot(ax=ax[1], title=\u0026#39;Day-Pass Users\u0026#39;); 此图显示每日趋势，以年费用户（上图）和临时用户（下图）分隔。 根据图标，我们可以获得几个结论：\n4 月份短期使用的临时用户大幅增加可能是由于 美国规划协会全国会议 在西雅图市中心举行。 其他一个比较接近的时间是 7 月 4 日周末。 临时用户呈现了一个与季节相关联的稳定的衰退趋势; 年费用户的使用没有随着秋天的来临而显着减少。 年费用户和临时用户似乎都显示出明显的每周趋势。 现在放大每周趋势，看一下所有的骑乘都是按照星期几分部的。由于 2015 年 1 月份左右模式的变化，我们按照年份和星期几进行拆分：\nIn [7]:\nby_weekday = by_date.groupby([by_date.index.year, by_date.index.dayofweek]).mean() by_weekday.columns.name = None # remove label for plot fig, ax = plt.subplots(1, 2, figsize=(16, 6), sharey=True) by_weekday.loc[2014].plot(title=\u0026#39;Average Use by Day of Week (2014)\u0026#39;, ax=ax[0]); by_weekday.loc[2015].plot(title=\u0026#39;Average Use by Day of Week (2015)\u0026#39;, ax=ax[1]); for axi in ax: axi.set_xticklabels([\u0026#39;Mon\u0026#39;, \u0026#39;Tues\u0026#39;, \u0026#39;Wed\u0026#39;, \u0026#39;Thurs\u0026#39;, \u0026#39;Fri\u0026#39;, \u0026#39;Sat\u0026#39;, \u0026#39;Sun\u0026#39;]) 我们看到了一个互补的模式：年费用户倾向于工作日使用他们的自行车（即作为通勤的一部分），而临时用户倾向于在周末使用他们的自行车。这种模式甚至在 2015 年年初都没有特别的体现出来，尤其是年费用户：似乎在头几个月，用户还没有使用 Pronto 的通勤习惯。\n查看平日和周末的平均每小时骑行也很有趣。这需要一些操作：\nIn [8]:\n# count trips by date and by hour by_hour = trips.pivot_table(\u0026#39;trip_id\u0026#39;, aggfunc=\u0026#39;count\u0026#39;, index=[\u0026#39;date\u0026#39;, \u0026#39;hour\u0026#39;], columns=\u0026#39;usertype\u0026#39;).fillna(0).reset_index(\u0026#39;hour\u0026#39;) # average these counts by weekend by_hour[\u0026#39;weekend\u0026#39;] = (by_hour.index.dayofweek\u0026gt;= 5) by_hour = by_hour.groupby([\u0026#39;weekend\u0026#39;, \u0026#39;hour\u0026#39;]).mean() by_hour.index.set_levels([[\u0026#39;weekday\u0026#39;, \u0026#39;weekend\u0026#39;], [\u0026#34;{0}:00\u0026#34;.format(i) for i in range(24)]], inplace=True); by_hour.columns.name = None 现在我们可以绘制结果来查看每小时的趋势：\nIn [9]:\nfig, ax = plt.subplots(1, 2, figsize=(16, 6), sharey=True) by_hour.loc[\u0026#39;weekday\u0026#39;].plot(title=\u0026#39;Average Hourly Use (Mon-Fri)\u0026#39;, ax=ax[0]) by_hour.loc[\u0026#39;weekend\u0026#39;].plot(title=\u0026#39;Average Hourly Use (Sat-Sun)\u0026#39;, ax=ax[1]) ax[0].set_ylabel(\u0026#39;Average Trips per Hour\u0026#39;); 我们看到一个 “通勤” 模式和一个 “娱乐” 模式之间的明显区别:“通勤” 模式在早上和晚上急剧上升，而 “娱乐” 模式在下午的时候有一个宽峰。 有趣的是，年费会员在周末的行为似乎与临时用户在周末的行为几乎相同。\n旅行时间 接下来，我们来看看旅行的持续时间。 Pronto 免费骑行最长可达 30 分钟; 任何长于此的单次使用，在前半个小时都会产生几美元的使用费，此后每小时大约需要十美元。\n让我们看看年费会员和临时使用者的旅行持续时间的分布：\nIn [10]:\ntrips[\u0026#39;minutes\u0026#39;] = trips.tripduration / 60 trips.groupby(\u0026#39;usertype\u0026#39;)[\u0026#39;minutes\u0026#39;].hist(bins=np.arange(61), alpha=0.5, normed=True); plt.xlabel(\u0026#39;Duration (minutes)\u0026#39;) plt.ylabel(\u0026#39;relative frequency\u0026#39;) plt.title(\u0026#39;Trip Durations\u0026#39;) plt.text(34, 0.09,\u0026#34;Free Trips\\n\\nAdditional Fee\u0026#34;, ha=\u0026#39;right\u0026#39;, size=18, rotation=90, alpha=0.5, color=\u0026#39;red\u0026#39;) plt.legend([\u0026#39;Annual Members\u0026#39;, \u0026#39;Short-term Pass\u0026#39;]) plt.axvline(30, linestyle=\u0026#39;--\u0026#39;, color=\u0026#39;red\u0026#39;, alpha=0.3); 在这里，我添加了一个红色的虚线，分开免费骑乘（左）和付费骑乘（右）。看来，年费用户对系统规则更加了解：只有行程分布的一小部分超过 30 分钟。另一方面，大约四分之一的临时用户时间超过半小时限制，并收取额外费用。 我的预期是，这些临时用户不能很好理解这种定价结构，并且可能会因为不开心的体验不再使用。\n估计行程距离 看看旅行的距离也十分有趣。Pronto 发布的数据中不包括行车的距离，因此我们需要通过其他来源来确定。让我们从加载行车数据开始 - 因为一些行程在 Pronto 的服务点之间开始和结束，我们将其添加为一个 “车站”：\nIn [11]:\nstations = pd.read_csv(\u0026#39;2015_station_data.csv\u0026#39;) pronto_shop = dict(id=54, name=\u0026#34;Pronto shop\u0026#34;, terminal=\u0026#34;Pronto shop\u0026#34;, lat=47.6173156, long=-122.3414776, dockcount=100, online=\u0026#39;10/13/2014\u0026#39;) stations = stations.append(pronto_shop, ignore_index=True) 现在我们需要找到两对纬度 / 经度坐标之间的骑车距离。幸运的是，Google 地图有一个距离 API，我们可以免费使用。\n从文档中知道，我们每天免费使用的限制为每天最多 2500 个距离，每 10 秒最多 100 个距离。现在有 55 个站，我们有（55 * 54/2） = 1485 个非零距离，所以我们可以在几天内免费查询所有车站之间的距离。\n为此，我们一次查询一行，在查询之间等待 10 + 秒（注意：我们可能还会使用 googlemaps Python 包 ，但使用它需要获取 API 密钥）。\nIn [12]:\nfrom time import sleep def query_distances(stations=stations): \u0026#34;\u0026#34;\u0026#34;Query the Google API for bicycling distances\u0026#34;\u0026#34;\u0026#34; latlon_list = [\u0026#39;{0},{1}\u0026#39;.format(lat, long) for (lat, long) in zip(stations.lat, stations.long)] def create_url(i): URL = (\u0026#39;https://maps.googleapis.com/maps/api/distancematrix/json?\u0026#39; \u0026#39;origins={origins}\u0026amp;destinations={destinations}\u0026amp;mode=bicycling\u0026#39;) return URL.format(origins=latlon_list[i], destinations=\u0026#39;|\u0026#39;.join(latlon_list[i + 1:])) for i in range(len(latlon_list) - 1): url = create_url(i) filename = \u0026#34;distances_{0}.json\u0026#34;.format(stations.terminal.iloc[i]) print(i, filename) !curl \u0026#34;{url}\u0026#34; -o {filename} sleep(11) # only one query per 10 seconds! def build_distance_matrix(stations=stations): \u0026#34;\u0026#34;\u0026#34;Build a matrix from the Google API results\u0026#34;\u0026#34;\u0026#34; dist = np.zeros((len(stations), len(stations)), dtype=float) for i, term in enumerate(stations.terminal[:-1]): filename = \u0026#39;queried_distances/distances_{0}.json\u0026#39;.format(term) row = json.load(open(filename)) dist[i, i + 1:] = [el[\u0026#39;distance\u0026#39;][\u0026#39;value\u0026#39;] for el in row[\u0026#39;rows\u0026#39;][0][\u0026#39;elements\u0026#39;]] dist += dist.T distances = pd.DataFrame(dist, index=stations.terminal, columns=stations.terminal) distances.to_csv(\u0026#39;station_distances.csv\u0026#39;) return distances # only call this the first time import os if not os.path.exists(\u0026#39;station_distances.csv\u0026#39;): # Note: you can call this function at most ~twice per day! query_distances() # Move all the queried files into a directory # so we don\u0026#39;t accidentally overwrite them if not os.path.exists(\u0026#39;queried_distances\u0026#39;): os.makedirs(\u0026#39;queried_distances\u0026#39;) !mv distances_*.json queried_distances # Build distance matrix and save to CSV distances = build_distance_matrix() 这里是第一个 5x5 距离矩阵：\nIn [13]:\ndistances = pd.read_csv(\u0026#39;station_distances.csv\u0026#39;, index_col=\u0026#39;terminal\u0026#39;) distances.iloc[:5, :5] Out[13]:\n让我们将这些距离转换为英里，并将它们加入我们的行程数据：\nIn [14]:\nstacked = distances.stack() / 1609.34 # convert meters to miles stacked.name = \u0026#39;distance\u0026#39; trips = trips.join(stacked, on=[\u0026#39;from_station_id\u0026#39;,\u0026#39;to_station_id\u0026#39;]) 现在我们可以绘制行程距离的分布：\nIn [15]:\nfig, ax = plt.subplots(figsize=(12, 4)) trips.groupby(\u0026#39;usertype\u0026#39;)[\u0026#39;distance\u0026#39;].hist(bins=np.linspace(0, 6.99, 50), alpha=0.5, ax=ax); plt.xlabel(\u0026#39;Distance between start \u0026amp; end (miles)\u0026#39;) plt.ylabel(\u0026#39;relative frequency\u0026#39;) plt.title(\u0026#39;Minimum Distance of Trip\u0026#39;) plt.legend([\u0026#39;Annual Members\u0026#39;, \u0026#39;Short-term Pass\u0026#39;]); 请记住，这显示站点之间的最短可能距离，是每次行程上实际距离的下限。许多旅行（特别是临时用户）在几个街区内开始和结束。除此之外，旅行高峰一般在大约 1 英里左右，也有一些用户将他们的旅行距离扩展到四英里或更长的距离。\n骑手速度 给定这些距离，我们还可以计算估计骑行速度的下限。 让我们这样做，然后看看年费用户和临时用户的速度分布：\nIn [16]:\ntrips[\u0026#39;speed\u0026#39;] = trips.distance * 60 / trips.minutes trips.groupby(\u0026#39;usertype\u0026#39;)[\u0026#39;speed\u0026#39;].hist(bins=np.linspace(0, 15, 50), alpha=0.5, normed=True); plt.xlabel(\u0026#39;lower bound riding speed (MPH)\u0026#39;) plt.ylabel(\u0026#39;relative frequency\u0026#39;) plt.title(\u0026#39;Rider Speed Lower Bound (MPH)\u0026#39;) plt.legend([\u0026#39;Annual Members\u0026#39;, \u0026#39;Short-term Pass\u0026#39;]); 有趣的是，分布是完全不同的，年费用户的速度平均值更高一些。你可能会想到这里的结论，年费用户的速度比临时用户更高，但数据本身不足以支持这一结论。如果年费用户倾向于通过最直接的路线从点 A 去往点 B，那么这些数据也可以被解释，而临时用户倾向于绕行并间接到达他们的目的地。我怀疑现实是这两种效应的混合。\n还要看看距离和速度之间的关系：\nIn [17]:\ng = sns.FacetGrid(trips, col=\u0026#34;usertype\u0026#34;, hue=\u0026#39;usertype\u0026#39;, size=6) g.map(plt.scatter,\u0026#34;distance\u0026#34;,\u0026#34;speed\u0026#34;, s=4, alpha=0.2) # Add lines and labels x = np.linspace(0, 10) g.axes[0, 0].set_ylabel(\u0026#39;Lower Bound Speed\u0026#39;) for ax in g.axes.flat: ax.set_xlabel(\u0026#39;Lower Bound Distance\u0026#39;) ax.plot(x, 2 * x,\u0026#39;--r\u0026#39;, alpha=0.3) ax.text(9.8, 16.5,\u0026#34;Free Trips\\n\\nAdditional Fee\u0026#34;, ha=\u0026#39;right\u0026#39;, size=18, rotation=39, alpha=0.5, color=\u0026#39;red\u0026#39;) ax.axis([0, 10, 0, 25]) 总的来说，我们看到较长的路途速度更快 - 虽然这受到与上述相同的下限影响。如上所述，作为参考，我绘制了需要的红线用于区分额外费用（低于红线）和免费费用（红线以上）。我们再次看到，年度会员对于不超过半小时的限制比每天通过用户更加精明 - 点的分布的指向了用户注意了他们使用的时间，以避免额外的费用。\n海拔高度 在西雅图自行车分享服务的可行性的一个焦点是，西雅图是一个丘陵城市。在服务发布之前，一些分析师预测，西雅图会有源源不断的自行车上坡下坡，所以并不适合分享单车系统的落地。\n数据版本中不包含海拔高度数据，但我们可以转到 Google Maps API 获取我们需要的数据; 请参阅 此网站 了解海拔 API 的描述。\n在这种情况下，自由使用限制为每天 2500 个请求，每次请求最多包含 512 个海拔高度。 由于我们只需要 55 个海拔高度，我们可以在单个查询中执行：\nIn [18]:\ndef get_station_elevations(stations): \u0026#34;\u0026#34;\u0026#34;Get station elevations via Google Maps API\u0026#34;\u0026#34;\u0026#34; URL = \u0026#34;https://maps.googleapis.com/maps/api/elevation/json?locations=\u0026#34; locs = \u0026#39;|\u0026#39;.join([\u0026#39;{0},{1}\u0026#39;.format(lat, long) for (lat, long) in zip(stations.lat, stations.long)]) URL += locs !curl \u0026#34;{URL}\u0026#34; -o elevations.json def process_station_elevations():\u0026#34;\u0026#34;\u0026#34;Convert Elevations JSON output to CSV\u0026#34;\u0026#34;\u0026#34; import json D = json.load(open(\u0026#39;elevations.json\u0026#39;)) def unnest(D): loc = D.pop(\u0026#39;location\u0026#39;) loc.update(D) return loc elevs = pd.DataFrame([unnest(item) for item in D[\u0026#39;results\u0026#39;]]) elevs.to_csv(\u0026#39;station_elevations.csv\u0026#39;) return elevs # only run this the first time: import os if not os.path.exists(\u0026#39;station_elevations.csv\u0026#39;): get_station_elevations(stations) process_station_elevations() 现在让我们读入海拔高度数据：\nIn [19]:\nelevs = pd.read_csv(\u0026#39;station_elevations.csv\u0026#39;, index_col=0) elevs.head() Out[19]:\n为了验证结果，我们需要仔细检查纬度和经度是否匹配：\nIn [20]:\n# double check that locations match print(np.allclose(stations.long, elevs.lng)) print(np.allclose(stations.lat, elevs.lat)) True True 现在我们可以将海拔数据与行程数据整合：\nIn [21]:\nstations[\u0026#39;elevation\u0026#39;] = elevs[\u0026#39;elevation\u0026#39;] elevs.index = stations[\u0026#39;terminal\u0026#39;] trips[\u0026#39;elevation_start\u0026#39;] = trips.join(elevs, on=\u0026#39;from_station_id\u0026#39;)[\u0026#39;elevation\u0026#39;] trips[\u0026#39;elevation_end\u0026#39;] = trips.join(elevs, on=\u0026#39;to_station_id\u0026#39;)[\u0026#39;elevation\u0026#39;] trips[\u0026#39;elevation_gain\u0026#39;] = trips[\u0026#39;elevation_end\u0026#39;] - trips[\u0026#39;elevation_start\u0026#39;] 让我们来看看海拔数据和会员类型的分布关系：\nIn [22]:\ng = sns.FacetGrid(trips, col=\u0026#34;usertype\u0026#34;, hue=\u0026#39;usertype\u0026#39;) g.map(plt.hist,\u0026#34;elevation_gain\u0026#34;, bins=np.arange(-145, 150, 10)) g.fig.set_figheight(6) g.fig.set_figwidth(16); # plot some lines to guide the eye for lim in range(60, 150, 20): x = np.linspace(-lim, lim, 3) for ax in g.axes.flat: ax.fill(x, 100 * (lim - abs(x)), color=\u0026#39;gray\u0026#39;, alpha=0.1, zorder=0) 我们在背景中绘制了一些阴影以帮助引导分析。 年度会员和临时用户之间有很大的区别：年费用户非常明显的表示出偏好下坡行程（左侧的分布），而临时用户表现并不明显，而是表示喜欢骑乘开始并在相同高度结束。\n为了使海拔数据变化的影响更加明显，我们做一些计算：\nIn [23]:\nprint(\u0026#34;total downhill trips:\u0026#34;, (trips.elevation_gain \u0026lt; 0).sum()) print(\u0026#34;total uphill trips:\u0026#34;, (trips.elevation_gain\u0026gt; 0).sum()) total downhill trips: 80532 total uphill trips: 50493 我们看到，第一年下坡比上坡多出了 3 万次 - 这是大约 60％ 以上。 根据目前的使用水平，这意味着 Pronto 工作人员必须每天从海拔较低的服务点运送大约 100 辆自行车到高海拔服务点。\n天气 另一个常见的反对循环共享的可行性的论点是天气。让我们来看看出行数量随着温度和降水量的变化。\n幸运的是，数据包括了大范围的天气数据：\nIn [24]:\nweather = pd.read_csv(\u0026#39;2015_weather_data.csv\u0026#39;, index_col=\u0026#39;Date\u0026#39;, parse_dates=True) weather.columns Out[24]:\nIndex([\u0026#39;Max_Temperature_F\u0026#39;, \u0026#39;Mean_Temperature_F\u0026#39;, \u0026#39;Min_TemperatureF\u0026#39;, \u0026#39;Max_Dew_Point_F\u0026#39;, \u0026#39;MeanDew_Point_F\u0026#39;, \u0026#39;Min_Dewpoint_F\u0026#39;, \u0026#39;Max_Humidity\u0026#39;, \u0026#39;Mean_Humidity\u0026#39;, \u0026#39;Min_Humidity\u0026#39;, \u0026#39;Max_Sea_Level_Pressure_In\u0026#39;, \u0026#39;Mean_Sea_Level_Pressure_In\u0026#39;, \u0026#39;Min_Sea_Level_Pressure_In\u0026#39;, \u0026#39;Max_Visibility_Miles\u0026#39;, \u0026#39;Mean_Visibility_Miles\u0026#39;, \u0026#39;Min_Visibility_Miles\u0026#39;, \u0026#39;Max_Wind_Speed_MPH\u0026#39;, \u0026#39;Mean_Wind_Speed_MPH\u0026#39;, \u0026#39;Max_Gust_Speed_MPH\u0026#39;, \u0026#39;Precipitation_In\u0026#39;, \u0026#39;Events\u0026#39;], dtype=\u0026#39;object\u0026#39;) dtype =\u0026#39;object\u0026#39;） 让我们将天气数据与行程数据结合起来：\nIn [25]:\nby_date = trips.groupby([\u0026#39;date\u0026#39;, \u0026#39;usertype\u0026#39;])[\u0026#39;trip_id\u0026#39;].count() by_date.name = \u0026#39;count\u0026#39; by_date = by_date.reset_index(\u0026#39;usertype\u0026#39;).join(weather) 现在我们可以看看按工作日和周末为纬度，查看出行数量随温度和降水量的变化：\nIn [26]:\n# add a flag indicating weekend by_date[\u0026#39;weekend\u0026#39;] = (by_date.index.dayofweek\u0026gt;= 5) #---------------------------------------------------------------- # Plot Temperature Trend g = sns.FacetGrid(by_date, col=\u0026#34;weekend\u0026#34;, hue=\u0026#39;usertype\u0026#39;, size=6) g.map(sns.regplot,\u0026#34;Mean_Temperature_F\u0026#34;,\u0026#34;count\u0026#34;) g.add_legend(); # do some formatting g.axes[0, 0].set_title(\u0026#39;\u0026#39;) g.axes[0, 1].set_title(\u0026#39;\u0026#39;) g.axes[0, 0].text(0.05, 0.95,\u0026#39;Monday - Friday\u0026#39;, va=\u0026#39;top\u0026#39;, size=14, transform=g.axes[0, 0].transAxes) g.axes[0, 1].text(0.05, 0.95,\u0026#39;Saturday - Sunday\u0026#39;, va=\u0026#39;top\u0026#39;, size=14, transform=g.axes[0, 1].transAxes) g.fig.text(0.45, 1,\u0026#34;Trend With Temperature\u0026#34;, ha=\u0026#39;center\u0026#39;, va=\u0026#39;top\u0026#39;, size=16); #---------------------------------------------------------------- # Plot Precipitation g = sns.FacetGrid(by_date, col=\u0026#34;weekend\u0026#34;, hue=\u0026#39;usertype\u0026#39;, size=6) g.map(sns.regplot,\u0026#34;Precipitation_In \u0026#34;,\u0026#34;count\u0026#34;) g.add_legend(); # do some formatting g.axes[0, 0].set_ylim(-50, 600); g.axes[0, 0].set_title(\u0026#39;\u0026#39;) g.axes[0, 1].set_title(\u0026#39;\u0026#39;) g.axes[0, 0].text(0.95, 0.95,\u0026#39;Monday - Friday\u0026#39;, ha=\u0026#39;right\u0026#39;, va=\u0026#39;top\u0026#39;, size=14, transform=g.axes[0, 0].transAxes) g.axes[0, 1].text(0.95, 0.95,\u0026#39;Saturday - Sunday\u0026#39;, ha=\u0026#39;right\u0026#39;, va=\u0026#39;top\u0026#39;, size=14, transform=g.axes[0, 1].transAxes) g.fig.text(0.45, 1,\u0026#34;Trend With Precipitation\u0026#34;, ha=\u0026#39;center\u0026#39;, va=\u0026#39;top\u0026#39;, size=16); 对于天气的影响，我们可以看出明显的趋势：人们更喜欢温暖、阳光明媚的天气。但是也有一些有趣的细节：工作日期间，所有的用户都受到天气的影响。然而周末年费用户受影响更少。我没有什么好的理论说明为什么有这种趋势，如果你有好的想法，欢迎提供。\n总结 根据上面的一些系列分析，但是我想我们可以从这些数据中获得一些结论：\n年费用户与临时用户整体上会有不同的行为：年费用户通常是利用 Pronto 进行工作日的通勤。临时用户则是周末使用 Pronto 探索城市的特定区域。 尽管年费用户对定价策略有所了解，但是四分之一的行程还是超过了半小时的限制，并产生了额外的费用。为了客户的权益，Pronto 应该更好告知用户这种定价策略。 海拔和天气会影响使用，正如你预计的一样：下坡比上坡多 60%，寒冷和下雨也会明显减少当天的骑行数量。 ","date":"2017-02-19T18:00:00Z","permalink":"https://www.4async.com/2017/02/2017-02-19-analyzing-pronto-cycleshare-data-with-python-and-pandas/","title":"使用 Python 和 Pandas 分析 Pronto CycleShare 数据"},{"content":"Golang 中的错误处理是一个被大家经常拿出来讨论的 话题(另外一个是 泛型)。其中泛型这个问题，rsc 在最近的计划中也提出 了纳入他今年的考虑计划中，同时，泛型的提案 在 2016 年也进行了一些更新，相信未来会有一些更好的方案提出。这个文章我们讨论一下如何在现行的 Golang 框架下提供更友好和优雅的错误处理。\n从现状谈起 Golang 中的错误处理原则，开发者曾经之前专门发布了几篇文章 (Error handling and Go 和 Defer, Panic, and Recover、Errors are values ) 介绍。分别介绍了 Golang 中处理一般预知到的错误与遇到崩溃时的错误处理机制。\n一般情况下，我们还是以官方博客中的错误处理例子为例：\nfunc main() {f, err := os.Open(\u0026#34;filename.ext\u0026#34;) if err != nil {log.Fatal(err) // 或者更简单的： // return err } ... } 当然对于简化代码行数，还有另外一种写法：\nfunc main() { ... if f, err = os.Open(\u0026#34;filename.ext\u0026#34;); err != nil{log.Fatal(err) } ... } 正常情况下，Golang 现有的哲学中，要求你尽量手工处理所有的错误返回，这稍微增加了开发人员的心智负担。关于这部分设计的讨论，请参考本文最开始提供的参考链接，此处不做太多探讨。\n本质上，Golang 中的错误类型 error 是一个接口类型：\ntype error interface {Error() string } 只要满足这一接口定义的所有数值都可以传入 error 类型的位置。在 Go Proverbs 中也提到了关于错误的描述： Errors are values。这一句如何理解呢？\nErrors are values 事实上，在实际使用过程中，你可能也发现了对 Golang 而言，所有的信息是非常不足的。比如下面这个例子：\nbuf := make([]byte, 100) n, err := r.Read(buf) buf = buf[:n] if err == io.EOF {log.Fatal(\u0026#34;read failed:\u0026#34;, err) } 事实上这只会打印信息 2017/02/08 13:53:54 read failed:EOF，这对我们真实环境下的错误调试与分析其实是并没有任何意义的，我们在查看日志获取错误信息的时候能够获取到的信息十分有限。\n于是乎，一些提供了上下文方式的一些错误处理形式便在很多类库中非常常见：\nerr := os.Remove(\u0026#34;/tmp/nonexist\u0026#34;) log.Println(err) 输出了：\n2017/02/08 14:09:22 remove /tmp/nonexist: no such file or directory 这种方式提供了一种更加直观的上下文信息，比如具体出错的内容，也可以是出现错误的文件等等。通过查看 Remove 的实现，我们可以看到：\n// PathError records an error and the operation and file path that caused it. type PathError struct { Op string Path string Err error } func (e *PathError) Error() string { return e.Op +\u0026#34; \u0026#34;+ e.Path +\u0026#34;: \u0026#34;+ e.Err.Error() } // file_unix.go 针对 *nix 系统的实现 // Remove removes the named file or directory. // If there is an error, it will be of type *PathError. func Remove(name string) error { // System call interface forces us to know // whether name is a file or directory. // Try both: it is cheaper on average than // doing a Stat plus the right one. e := syscall.Unlink(name) if e == nil {return nil} e1 := syscall.Rmdir(name) if e1 == nil {return nil} // Both failed: figure out which error to return. // OS X and Linux differ on whether unlink(dir) // returns EISDIR, so can\u0026#39;t use that. However, // both agree that rmdir(file) returns ENOTDIR, // so we can use that to decide which error is real. // Rmdir might also return ENOTDIR if given a bad // file path, like /etc/passwd/foo, but in that case, // both errors will be ENOTDIR, so it\u0026#39;s okay to // use the error from unlink. if e1 != syscall.ENOTDIR {e = e1} return \u0026amp;PathError{\u0026#34;remove\u0026#34;, name, e} } 实际上这里 Golang 标准库中返回了一个名为 PathError 的结构体，这个结构体定义了操作类型、路径和原始的错误信息，然后通过 Error 方法对所有信息进行了整合。\n但是这样也会存在问题，比如需要进行单独类型复杂的分类处理，比如上面例子中，需要单独处理 PathError 这种问题，你可能需要一个单独的类型推导：\nerr := xxxx() if err != nil {swtich err := err.(type) { case *os.PathError: ... default: ... } } 这样反倒会增加错误处理的复杂度。同时，这些错误必须变为导出类型，也会增加整个系统的复杂度。\n另外一个问题是，我们在出现错误时，我们通常也希望获取更多的堆栈信息，方便我们进行后续的故障追踪。在现有的错误体系中，这相对比较复杂：你很难通过一个接口类型获取完整的调用堆栈。这时，我们可能就需要一个第三方库区去解决遇到的这些错误处理问题。\n还有一种情况是，我们希望在错误处理过程中同样可以附加一些信息，这些也会相对比较麻烦。\n更优雅的错误处理 之前提到了多种实际应用场景中出现的错误处理方法和遇到的一些问题，这里推荐使用第三方库去解决部分问题：github.com/pkg/errors。\n比如当我们出现问题时，我们可以简单的使用 errors.New 或者 errors.Errorf 生成一个错误变量：\nerr := errors.New(\u0026#34;whoops\u0026#34;) // or err := errors.Errorf(\u0026#34;whoops: %s\u0026#34;, \u0026#34;foo\u0026#34;) 当我们需要附加信息时，则可以使用：\ncause := errors.New(\u0026#34;whoops\u0026#34;) err := errors.Wrap(cause,\u0026#34;oh noes\u0026#34;) 当需要获取 @用堆栈时，则可以使用：\nerr := errors.New(\u0026#34;whoops\u0026#34;) fmt.Printf(\u0026#34;%+v\u0026#34;, err) 其他建议 在上面做类型推导时，我们发现在处理一类错误时可能需要多个错误类型，这可能在某些情况下相对来说比较复杂，很多时候我们可以使用接口形式去方便处理：\ntype temporary interface {Temporary() bool } // IsTemporary returns true if err is temporary. func IsTemporary(err error) bool {te, ok := errors.Cause(err).(temporary) return ok \u0026amp;\u0026amp; te.Temporary()} 这样就可以提供更加方便的错误解析和处理。\n广告时间 我们正在招收新人 Gopher，应届毕业生 or 实习生欢迎投递简历。我们正在努力实现开发流程标准化，如果你想获得提高，相信也是一个非常不错的机会。简历投递 kevin [at] yeeuu [dot] com。\n","date":"2017-02-08T18:00:00Z","permalink":"https://www.4async.com/2017/02/2017-02-08-more-effective-golang-error/","title":"更优雅的 Golang 错误处理"},{"content":"今年在 Pycon China 上，来自饿了么的郭浩川分享了 利用 systemtap 进行 Python 执行情况分析 的内容。分享利用 systemtap 在线上环境中实时监控 gevent patch 的 green thread 程序的执行状况。\ndtrace 和 systemtap 均支持在 Linux 上进行分析，在 macOS 系统上则只有 dtrace 使用。在 Python3.5 和之前版本中，需要使用手工 Patch 的方式进行埋点监控。在 Python 3.6 以上中 dtrace 和 systemtap 埋点支持功能可以通过编译参数 \u0026ndash;with-dtrace 开启。\n从 dtrace 开始 dtrace 是一个低开销的成本动态跟踪工具，可以通过埋点 probs 方式监控各项程序运行状态。dtrace 最初内置在 Solaris 系统中，因此我们可以借助 Solaris 系统的相关文档了解 dtrace 的基本操作。DTrace 用户指南 是 Oracle 提供的基于 Solaris 系统的 dtrace 操作手册，操作基本与其它系统相同，推荐在最初开始阶段阅读该使用手册。\n在 macOS 上，已经很多系统底层功能和 framework 中已经集成了 dtrace 的功能。\n比如说我们需要监控 read 这个 syscall 的入口，可以通过下面这个命令实现：\n➜ ~ sudo dtrace -n syscall::read:entry dtrace: description \u0026#39;syscall::read:entry\u0026#39; matched 1 probe CPU ID FUNCTION:NAME 0 156 read:entry 0 156 read:entry 0 156 read:entry ... 其中 -n 参数表示打印特定的 probs 内容的调用。现在这样仅仅显示了调用，但是调用的信息还是不详细的。这个时候就需要使用 dtrace 的脚本获取更多的信息。\ndtrace 的脚本 dtrace 的名字暗示了自己的脚本，dtrace 使用了 D 语言作为脚本语言（WTF）。这个时候就需要学习一下基础的 D 语言内容。dtrace 中 D 语言的使用，可以在上面提到的 dtrace 用户指南中的对应章节。\n我们继续上一节中的例子，检测调用 read syscall 的参数内容。在进行操作之前，我们需要了解一下 read 的参数：\nsize_t read(int fildes, void *buf, size_t nbyte); 我们想知道调用来自于哪些进程，读取了多少字节。首先，我们需要创建一个叫 syscall.d 文件。\nsyscall::read:entry {printf (\u0026#34;%s called read, asking for %d bytes\\n\u0026#34;, execname, arg2); } 然后通过命令行执行：\n➜ ~ sudo dtrace -s syscall.d dtrace: script \u0026#39;syscall.d\u0026#39; matched 1 probe CPU ID FUNCTION:NAME 0 156 read:entry steam_osx called read, asking for 128 bytes 0 156 read:entry steam_osx called read, asking for 128 bytes 0 156 read:entry steam_osx called read, asking for 128 bytes 0 156 read:entry steam_osx called read, asking for 128 bytes 0 156 read:entry iTerm2 called read, asking for 32 bytes 0 156 read:entry iTerm2 called read, asking for 1024 bytes ... 从 dtrace 到 Python 跟踪 在 Pycon 上的分享上，提供了一种 CPython 虚拟机代码 Patch 方式进行跟踪的方案，这种方案需要系统支持，比如 REHL 系列系统默认支持，假如你是用了 Ubuntu 系统，除了重新编译 CPython 以外没有其他办法。不过除此之外，还有一种更简单的方式，就是使用 Python USDT。但是值得注意的是 Python USDT 是一种用户态修饰器监控方法，这种方式与 patch CPython 的方式相比差距较大，毕竟 USDT 无法监控一些系统底层的尤其是涉及到比如 gevent 导致的上下文切换时的混乱。同样的，systemtap 也有一个专门的库：python-systemtap。\n不过还是从 USDT 开始。USDT 可以通过 pip 直接安装：\npip install usdt 接下来用 ipython 演示一下 usdt 的基础使用：\n➜ ~ ipython Python 2.7.12 (default, Jul 4 2016, 11:33:35) Type \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. IPython 5.0.0 -- An enhanced Interactive Python. ? -\u0026gt; Introduction and overview of IPython\u0026#39;s features. %quickref -\u0026gt; Quick reference. help -\u0026gt; Python\u0026#39;s own help system. object? -\u0026gt; Details about \u0026#39;object\u0026#39;, use \u0026#39;object??\u0026#39; for extra details. In [1]: import os In [2]: os.getpid() Out[2]: 7777 In [3]: from usdt.tracer import fbt In [4]: @fbt ...: def example(v): ...: pass ...: In [5]: example(\u0026#34;hello\u0026#34;) 我们使用监听进程的方式监控来自于 usdt 的探针：\n➜ ~ sudo dtrace -l -p 7777 -m fbt ID PROVIDER MODULE FUNCTION NAME 1206 python-fbt7777 fbt example entry 1207 python-fbt7777 fbt example return 其中 -p 用于指定进程，-m 指定对应的模块。如果不进行模块过滤的话，你很有可能被很多相关的调试信息淹没。\n在新版的 Python 3.6 以上版本中通过开启 --with-dtrace 开关的方式可以获取详细的 Python 运行状态信息，包括：\n函数调用与返回 GC 开始与结束 执行代码行数 首先运行一个 Python 3.6\n➜ ~ ipython Python 3.6.0b1 (default, Sep 15 2016, 10:16:39) Type \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. IPython 5.1.0 -- An enhanced Interactive Python. ? -\u0026gt; Introduction and overview of IPython\u0026#39;s features. %quickref -\u0026gt; Quick reference. help -\u0026gt; Python\u0026#39;s own help system. object? -\u0026gt; Details about \u0026#39;object\u0026#39;, use \u0026#39;object??\u0026#39; for extra details. In [1]: import os In [2]: os.getpid() Out[2]: 33164 In [3]: 接下来通过 dtrace 来获取所有的 Python 探针输出：\n➜ ~ sudo dtrace -l -m python3.6 ID PROVIDER MODULE FUNCTION NAME 35775 python33164 python3.6 _PyEval_EvalFrameDefault function-entry 35776 python33164 python3.6 _PyEval_EvalFrameDefault function-return 35777 python33164 python3.6 collect gc-done 35778 python33164 python3.6 collect gc-start 35779 python33164 python3.6 _PyEval_EvalFrameDefault line 如果想要实现一个类似 pycon 中监控程序执行具体内容的 dtrace 脚本，请参考官方文档 Instrumenting CPython with DTrace and SystemTap 中的现成脚本即可。\n总结 dtrace 提供了一种方便的、低干扰的 Python 内部执行探查方式。通过这种方式可以方便的了解目前 Python 的具体执行状况。同时，通过脚本，可以快速获取和定位相关想要了解的具体内容。当然，你也可以像郭浩川分享的那样，做一个比较炫酷的烈焰图。XD\n","date":"2016-09-15T18:00:00Z","permalink":"https://www.4async.com/2016/09/2016-09-15-tracing-python-program-with-dtrace/","title":"使用 dtrace 跟踪 Python 应用"},{"content":"traefik(https://traefik.io/) 是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置。目前支持 Docker, Swarm, Mesos/Marathon, Mesos, Kubernetes, Consul, Etcd, Zookeeper, BoltDB, Rest API 等等后端模型。\ntraefik 的具体模型如下： 为什么选择 traefik？ 事实上在之前我对 LB 的选择一直更倾向于使用 HAProxy。但是选择 traefik 主要是有以下特点让我们决定使用：\nGolang 编写，单文件部署，与系统无关，同时也提供小尺寸 Docker 镜像。 支持 Docker/Etcd 后端，天然连接我们的微服务集群。 内置 Web UI，管理相对方便。 自动配置 ACME(Let\u0026rsquo;s Encrypt) 证书功能。 性能尚可，我们也没有到压榨 LB 性能的阶段，易用性更重要。 除了这些以外，traefik 还有以下特点：\nRestful API 支持。 支持后端健康状态检查，根据状态自动配置。 支持动态加载配置文件和 graceful 重启。 支持 WebSocket 和 HTTP/2。 除了上面提到的微服务化集群支持，一些 AB 测试阶段也可以通过 frontend 的路由特性进行动态分配，当然这些对 HAProxy 等软件都是标准支持的。\ntraefik 的配置 traefik 支持的配置方式支持文件方式进行配置，这个也是比较常见的配置方式，我们这里简单介绍一下。\ntraefik 支持的 toml 方式进行配置，官方提供了一个 示例的 traefik.toml 文件 用于演示配置。除此之外，后端服务一般是采用单独文件进行存储，比如演示配置中指定的 rules.toml。\n具体一个例子，如果我们有两个后端，127.0.0.1:7727，127.0.0.1:7728，我们希望所有的 Chrome 用户都可以访问 127.0.0.1:7727，其它人都访问 127.0.0.1:7728，这样这个 rules.toml 应该如何配置呢？\n# rules.toml [backends] [backends.backend1] [backends.backend1.servers.server1] url = \u0026#34;http://127.0.0.1:7727\u0026#34; [backends.backend2] [backends.backend2.servers.server1] url = \u0026#34;http://127.0.0.1:7728\u0026#34; [frontends] [frontends.frontend1] entrypoints = [\u0026#34;http\u0026#34;] backend = \u0026#34;backend1\u0026#34; [frontends.frontend1.routes.test_1] rule = \u0026#34;HeadersRegexp: User-Agent, Chrome\u0026#34; [frontends.frontend2] entrypoints = [\u0026#34;http\u0026#34;] backend = \u0026#34;backend2\u0026#34; 首先定义两个后端服务，每个后端服务可以支持多个服务单元，这里我们只有一个。前端 frontends 用于匹配请求落到哪个后端服务中。我们这里定义一个规则 test_1，设置规则为根据 HTTP 请求头部正则进行分配：如果 UserAgent 中包含 Chrome 字样，则访问到 127.0.0.1:7727。匹配的规则方式包含了以下几种方式:\nHeaders/HeaderRegexp: 头部匹配方式，分别对应按值和正则表达式两种方式。 Host/HostRegexp: 按照请求主机名进行匹配，与头部信息相似。 Method: 按照请求方式区分。 Path/PathStrip/PathPrefix/PathPrefixStrip: 按照路径区分后端。 traefik 与微服务集群 这个有人已经写过相关的文章了，我在这里简单推荐一下： Microservices Bliss with Docker and Traefik（中文译文）。我就不做额外的描述了。\n","date":"2016-08-01T11:00:00Z","permalink":"https://www.4async.com/2016/08/2016-08-01-introduce-traefik-load-balance/","title":"traefik 简介"},{"content":"Type Hint（或者叫做 PEP-484）提供了一种针对 Python 程序的类型标注标准。\n为什么使用 Type Hint？对于动态语言而言，常常出现的情况是当你写了一段代码后，隔段时间你可能忘记这个方法的原型是什么样子的了，你也不清楚具体应该传入什么类型的参数，这样往往需要你去阅读代码才能定义每个类型具体是什么。或者当你使用一个文档并不是特别完全的第三方库，你不知道这个库应该如何使用，这都会很痛苦。\n现在，借助 Type Hint，你可以实现：\n实现类型检查，防止运行时出现的类型不符合情况。 作为文档附加属性，方便开发者调用时传入传出的参数类型。 提升 IDE 的检查机制，在智能提示时更快给出提示和类型检查结果。 实现这个过程中，你需要使用 Python 3.5+ 中提供的新模块 typing。值得注意的是，这个改动并不会影响程序运行，仅仅是为了方便类型检查器实现的。\nType Hint 类型检查器 目前，比如 JetBrains 家的 PyCharm 已经支持 Type Hint 语法检查功能，如果你使用了这个 IDE，可以通过 IDE 功能进行实现。如果你像我一样，使用了 SublimeText 编辑器，那么第三方工具 mypy 可以帮助到你。AnacondaST3 最近要发布的 2.0 版本也内置了 mypy 功能的支持，具体的进度可以看一下 这个 issue。一些其它的 Python 工具 (比如 代码提示工具 jedi 0.10+) 也支持了 Type Hint 功能。\n从简单的例子开始 从简单的例子开始，我们先从一个简单的程序开始，运行环境为 Python 3.5.2，使用 mypy 工具进行检查。\n首先通过 pip install mypy-lang 命令安装 mypy 工具。注意是 mypy-lang，之所以是这样，是因为在 pypi 里 mypy 这个名字已经被占用掉了。\n接下来，通过 mypy 检查下面这个文件\n# fib.py from typing import Iterator def fib(n: int) -\u0026gt; Iterator[int]: a, b = 0, 1 while a \u0026lt; n: yield a a, b = b, a + b i = fib(3.2) print(next(i)) print(next(i)) 在命令行中执行命令 mypy fib.py，获取返回结果：\n➜ mypy fib.py fib.py:11: error: Argument 1 to \u0026#34;fib\u0026#34; has incompatible type \u0026#34;float\u0026#34;; expected \u0026#34;int\u0026#34; 但是在实际的应用过程中，这个功能在 Python 里是可以正常运行的：\n➜ mypy python fib.py 0 1 可以看到，mypy 工具提示了我们的代码中存在一处类型不匹配的问题，但是如果不进行检查，代码有可能执行出不可预知的结果。\n在这个例子里面，我们使用了两种类型，一种是 Python 基础数据类型，比如 str、int 等等，这些类型数据是可以直接使用的；另外一种是来自于 typing 中引入的 Iterator，用来表示迭代器类型。另外一个值得注意的是，typing 中部分类型也会随时添加，一般我们以演示版本为准。\n从简单到复杂，类型组合怎么办？ 实际上，在我们使用过程中还有可能传递一些更加复杂的参数类型，比如 list 类型，tuple 类型等等，这类型的数据如何声明呢？我们可以先看一个例子：\ndef foo(strings, string_list, count, total): 这个函数的参数我们从字面可以看出来分别是 str，元素为 str 的 list 类型和两个整数参数。我们假定一个返回值为 ((int, int), str)，那么这个类型检查可以这样定义：\nfrom typing import List, Tuple Result = Tuple[Tuple[int, int], str] def foo(strings: str, lines: List[str], line_number: int, total_lines: int) -\u0026gt; Result: 其它的一些类型提示、协程等等的支持都可以在官方的 typing 模块文档 中进行查看。\n关于生产的一些闲扯 我们现在也在进行一些 mypy 工具在生产环境中的具体使用测试，但是我们也发现了一些存在的问题，比如 Python 本身的动态语言特性给类型标注就带来了一些麻烦。另外，变量复用导致的类型变换有可能会提示采用新的变量实现。这对于一个已经存在的线上项目来说相对成本较高，我们后续也会在一些新项目中采用这种方式。另外 mypy 还是一个比较新的项目，本身是拥有一些 bug。另外一个是在某些 mypy 的非类型错误提示其实非常的模糊，导致很多错误有时需要进行人工排查。\n不管怎样，即便在 mypy 存在一些缺陷，但是仍旧是未来非常有潜力的工具，提前了解和应用也能有效的提升程序的强壮性。\n","date":"2016-07-13T20:00:00Z","permalink":"https://www.4async.com/2016/07/2016-07-13-type-hint-improve-python-programming/","title":"利用 Type Hint 提升 Python 程序开发效率"},{"content":"服务发现是微服务化架构中重要的一环，服务的配置信息需要有一种可靠高效的发现机制，保证服务上线时可以及时被使用，服务失效中断时可以及时切走。服务发现工具 Etcd 就是为了这种需求开发的。\n什么是 Etcd？ Etcd 是一个分布式 KV 数据库，通过将数据分散存储在多台独立的设备上，从而提高数据的可靠性或读写性能。Etcd 是几个比较常见的服务发现应用之一，它支持 TTL 的支持和 HTTP Restful API，同时通过 Raft 一致性算法处理日志复制以保证强一致性。关于 Raft 算法，请参考 这篇文章，这里不多介绍。Etcd 本来就是 CoreOS 团队开发支持的，因此也是原生存在在 CoreOS 系统中。\nEtcd 中提供了订阅通知机制，同时提供了一个线上服务 https://discovery.etcd.io/，这个服务可以用于发现集群中的机器。比如 Fleet 等等工具也是基于 Etcd 去发现网络中的节点服务器。在 CoreOS 机器部署之后，系统中一个叫做 cloud-init 的服务会根据之前的 user-data 文件去启动 Etcd。Etcd 会更新对应的自己的节点信息，并且获取其它的节点信息。\n另外比较常见的服务发现还有 ZooKeeper（应用最广泛）、Consul 等等，如果有兴趣，可以自己在进行研究。\nEtcdctl 使用 工具 etcdctl 是 etcd 的控制程序，我们可以通过执行命令查看所有键值：\ncore@core-01 ~ $ etcdctl ls / --recursive /coreos.com /coreos.com/network /coreos.com/network/config /coreos.com/network/subnets /coreos.com/network/subnets/10.1.64.0-24 /coreos.com/network/subnets/10.1.48.0-24 /coreos.com/network/subnets/10.1.10.0-24 /coreos.com/updateengine /coreos.com/updateengine/rebootlock /coreos.com/updateengine/rebootlock/semaphore 还可以通过类似 Redis 的 get 等命令获取具体存储内容：\ncore@core-01 ~ $ etcdctl get /coreos.com/network/subnets/10.1.64.0-24 {\u0026#34;PublicIP\u0026#34;:\u0026#34;172.17.8.101\u0026#34;} 区别是 Etcd 是支持目录的：\ncore@core-01 ~ $ etcdctl mkdir hello core@core-01 ~ $ etcdctl get hello /hello: is a directory core@core-01 ~ $ etcdctl rmdir hello 刚刚我们介绍时也提到，Etcd 支持 HTTP 方式调用，比如：\ncore@core-01 ~ $ curl -L -X PUT http://127.0.0.1:2379/v2/keys/message -d value=\u0026#34;Hello\u0026#34; {\u0026#34;action\u0026#34;:\u0026#34;set\u0026#34;,\u0026#34;node\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;/message\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;Hello\u0026#34;,\u0026#34;modifiedIndex\u0026#34;:10318,\u0026#34;createdIndex\u0026#34;:10318},\u0026#34;prevNode\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;/message\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;Hello\u0026#34;,\u0026#34;modifiedIndex\u0026#34;:10300,\u0026#34;createdIndex\u0026#34;:10300}} core@core-01 ~ $ etcdctl get /message Hello core@core-01 ~ $ curl -L -X DELETE http://127.0.0.1:2379/v2/keys/message {\u0026#34;action\u0026#34;:\u0026#34;delete\u0026#34;,\u0026#34;node\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;/message\u0026#34;,\u0026#34;modifiedIndex\u0026#34;:10462,\u0026#34;createdIndex\u0026#34;:10318},\u0026#34;prevNode\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;/message\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;Hello\u0026#34;,\u0026#34;modifiedIndex\u0026#34;:10318,\u0026#34;createdIndex\u0026#34;:10318}} core@core-01 ~ $ etcdctl get /message Error: 100: Key not found (/message) [10467] TTL 的特性可以在设置状态时进行设定：\netcdctl set /foo \u0026#34;Expiring Soon\u0026#34; --ttl 20 这个 KV 对就会在 20 秒内时效。\nEtcd 集群管理 除了本身的 KV 数据库特性外，作为集群服务发现工具时，也可以通过 restful api 方式发现当前集群信息：\ncore@core-01 ~ $ curl -L http://127.0.0.1:4001/v2/stats/leader {\u0026#34;leader\u0026#34;:\u0026#34;efb737dfdc9ee528\u0026#34;,\u0026#34;followers\u0026#34;:{\u0026#34;6219cfe16536320\u0026#34;:{\u0026#34;latency\u0026#34;:{\u0026#34;current\u0026#34;:0.003561,\u0026#34;average\u0026#34;:0.0031178274017212987,\u0026#34;standardDeviation\u0026#34;:0.018016615756979302,\u0026#34;minimum\u0026#34;:9e-06,\u0026#34;maximum\u0026#34;:1.338917},\u0026#34;counts\u0026#34;:{\u0026#34;fail\u0026#34;:0,\u0026#34;success\u0026#34;:51588}},\u0026#34;a8cc28a8e121c40d\u0026#34;:{\u0026#34;latency\u0026#34;:{\u0026#34;current\u0026#34;:0.002212,\u0026#34;average\u0026#34;:0.002837358092138387,\u0026#34;standardDeviation\u0026#34;:0.015615452604769925,\u0026#34;minimum\u0026#34;:1.7e-05,\u0026#34;maximum\u0026#34;:1.48721},\u0026#34;counts\u0026#34;:{\u0026#34;fail\u0026#34;:0,\u0026#34;success\u0026#34;:51618}},\u0026#34;e44ee28dd4e590ac\u0026#34;:{\u0026#34;latency\u0026#34;:{\u0026#34;current\u0026#34;:0.001838,\u0026#34;average\u0026#34;:0.006247004906804689,\u0026#34;standardDeviation\u0026#34;:0.4458222893241591,\u0026#34;minimum\u0026#34;:9e-06,\u0026#34;maximum\u0026#34;:105.637627},\u0026#34;counts\u0026#34;:{\u0026#34;fail\u0026#34;:32,\u0026#34;success\u0026#34;:58694}}}} core@core-01 ~ $ curl -L http://127.0.0.1:4001/v2/stats/self {\u0026#34;name\u0026#34;:\u0026#34;c99fef8aac9b4a3e9d3a44f58b0739a6\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;efb737dfdc9ee528\u0026#34;,\u0026#34;state\u0026#34;:\u0026#34;StateLeader\u0026#34;,\u0026#34;startTime\u0026#34;:\u0026#34;2016-05-28T09:44:01.416629553Z\u0026#34;,\u0026#34;leaderInfo\u0026#34;:{\u0026#34;leader\u0026#34;:\u0026#34;efb737dfdc9ee528\u0026#34;,\u0026#34;uptime\u0026#34;:\u0026#34;1h59m14.023315023s\u0026#34;,\u0026#34;startTime\u0026#34;:\u0026#34;2016-05-28T09:44:44.495996231Z\u0026#34;},\u0026#34;recvAppendRequestCnt\u0026#34;:0,\u0026#34;sendAppendRequestCnt\u0026#34;:162816,\u0026#34;sendPkgRate\u0026#34;:20.004570208101516,\u0026#34;sendBandwidthRate\u0026#34;:1953.7463493742348} 其它的系列集群接口，也可以在 官方文档 中查看。\nEtcd 配置 如果你有印象在第一篇中，如果你打开 user-data 文件，你就会发现 Etcd 的踪影：\n➜ coreos-vagrant git:(master) cat user-data #cloud-config --- coreos: etcd2: advertise-client-urls: http://$public_ipv4:2379 initial-advertise-peer-urls: http://$private_ipv4:2380 listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001 listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001 discovery: https://discovery.etcd.io/xxxxxxxxxxxx fleet: public-ip: \u0026#34;$public_ipv4\u0026#34; flannel: interface: \u0026#34;$public_ipv4\u0026#34; .... 我们这里使用了线上服务 https://discovery.etcd.io/ 发现集群中的机器，这个服务同样也可以使用本地内系统。\n利用 Etcd 制作服务发现 之前提到，Etcd 的特性非常方便用做服务发现，具体如何操作呢？在谈具体实现之前，我们来介绍一下两种服务注册方法：一种叫做 自注册方法；另外一种叫 第三方注册方法。区别是是否由自身来进行健康检查和提醒。另外，服务发现重要的是，在服务失效时可以及时去除无效服务，这个在 Etcd 的 TTL 功能上就会显得比较重要。我们可以设置一个键值的有效期为 3 秒，并且每秒钟都来刷新授权，如果程序异常退出或者刷新不及时，那么这个服务简直就会失效。通过这种方式就可以有效验证服务是否有效。\n具体的代码就不再额外给出了，大家可以自己动手实现一下。另外，其实服务发现还有一个重要的是提供自身工作的地址端口信息，这些可以通过环境变量传递到容器的注册过程中去，这个也是能够让负载均衡或者 WebService 服务器可以识别服务的重要手段。比如 Nginx 可以在根据服务注册信息定期更新自己的配置文件，利用重载保证不间断的服务运行。\n","date":"2016-05-31T09:13:00Z","permalink":"https://www.4async.com/2016/05/2016-05-31-learning-coreos-part3/","title":"CoreOS 折腾笔记（三）了解 Etcd"},{"content":"如果要说什么样子的分布式集群对用户是最友好的，那无疑是对客户来说，像本地执行命令一样方便的执行集群命令肯定是最舒服的了。这个我们在上一节 集群部署 里面就提到了一个叫做 \u0026ldquo;fleetctl\u0026rdquo; 的命令，这个命令是做什么用的呢？\nfleet 是什么 工具 fleet 是一个在集群层面上的 systemd 管理工具。它的配置文件语法基于 systemd 的语法，另外添加了一些自有的属性。如果你希望在集群中运行你的服务，那么使用 fleet 管理 systemd 单元是再有必要不过的了。\n在比较新的系统 (CentOS 7+、Ubuntu 16+、Debian 8+) 中均采用了 systemd 作为启动项管理工具。如果你对 systemd 有疑问的话，请到其 官方网站 查看具体的介绍，这里不做赘述。\n之前使用的 fleetctl 就是 fleet 的管理工具，默认是在集群中的某台机器上进行管理。当然，fleetctl 同样也可以通过远程进行管理，可以通过如下命令连接远程集群。\nFLEETCTL_ENDPOINT=http://\u0026lt;IP:[PORT]\u0026gt; fleetctl list-units fleetctl 常见命令 比较常见的 fleetctl 命令有：\ncore@core-01 ~ $ fleetctl -h ... COMMANDS: cat\t查看已经提交的单元文件内容 destroy\t销毁集群中的一个或多个单元 fd-forward\t将标准输入输出转向到一个 unix socket 中 journal\t将集群中的某个 unit 的日志输出到当前 list-machines\t查看集群中的已知机器 list-unit-files\t查看集群中存在的单元 list-units\t查看集群中的单元状态 load\t将一个或多个单元加载到集群中，必要时会先执行 submit 功能 ssh\t连接到集群中的某台机器 start\t启动集群中一个或多个单元，必要时会先执行 submit 和 load 功能 status\t输出集群中一个或多个单元的状态 stop\t停止集群中一个或多个单元 submit\t上传一个或多个单元到集群中，并不会加载执行 unload\t卸载集群中的一个或多个单元 fleet 单元文件 以一个 Hello World 程序作为演示来讲解：\n[Unit] Description=Echo Hello World After=docker.service Requires=docker.service [Service] TimeoutStartSec=0 ExecStartPre=-/usr/bin/docker kill busybox ExecStartPre=-/usr/bin/docker rm busybox ExecStartPre=/usr/bin/docker pull busybox ExecStart=/usr/bin/docker run --name busybox busybox /bin/sh -c \u0026#34;trap\u0026#39;exit 0\u0026#39;INT TERM; while true; do echo Hello World; sleep 1; done\u0026#34; ExecStop=/usr/bin/docker stop busybox 看起来基本与 systemd 语法一致。保存成 helloworld.service，然后执行命令：\ncore@core-01 ~ $ fleetctl start helloworld.service Unit helloworld.service inactive Unit helloworld.service launched on 6e1b9fae.../172.17.8.104 可以通过 fleetctl 查看状态：\ncore@core-01 ~ $ fleetctl list-units UNIT\tMACHINE\tACTIVE\tSUB helloworld.service\t6e1b9fae.../172.17.8.104\tactive\trunning core@core-01 ~ $ fleetctl list-unit-files UNIT\tHASH\tDSTATE\tSTATE\tTARGET helloworld.service\tce68bd4\tlaunched\tlaunched\t6e1b9fae.../172.17.8.104 同时，可以通过 fleetctl journal 查看程序是否正常运行了。\ncore@core-01 ~ $ fleetctl journal helloworld The authenticity of host \u0026#39;172.17.8.104\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is 11:63:ee:93:e4:b9:5e:06:e9:c6:cd:63:e2:df:ef:9e. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added \u0026#39;172.17.8.104\u0026#39; (ECDSA) to the list of known hosts. -- Logs begin at Sat 2016-05-28 08:45:39 UTC, end at Sat 2016-05-28 10:09:13 UTC. -- May 28 10:09:04 core-04 docker[1163]: Hello World May 28 10:09:05 core-04 docker[1163]: Hello World May 28 10:09:06 core-04 docker[1163]: Hello World May 28 10:09:07 core-04 docker[1163]: Hello World May 28 10:09:08 core-04 docker[1163]: Hello World May 28 10:09:09 core-04 docker[1163]: Hello World May 28 10:09:10 core-04 docker[1163]: Hello World May 28 10:09:11 core-04 docker[1163]: Hello World May 28 10:09:12 core-04 docker[1163]: Hello World May 28 10:09:13 core-04 docker[1163]: Hello World 通过 fleet 单元文件实现高可用服务 实现高可用服务需要多个服务实例：当其中一个服务实例出现问题时，不会干扰其它服务实例的运行。这里我们还是使用上一节中提到的单元文件，但是需要做额外的修改让其支持多实例运行：\n[Unit] Description=Echo Hello World After=docker.service Requires=docker.service [Service] TimeoutStartSec=0 ExecStartPre=-/usr/bin/docker kill busybox ExecStartPre=-/usr/bin/docker rm busybox ExecStartPre=/usr/bin/docker pull busybox ExecStart=/usr/bin/docker run --name busybox busybox /bin/sh -c \u0026#34;trap\u0026#39;exit 0\u0026#39;INT TERM; while true; do echo Hello World; sleep 1; done\u0026#34; ExecStop=/usr/bin/docker stop busybox [X-Fleet] Conflicts=helloworld@*.service 最后新增的内容是 fleet 的专属语法，使用 Conflicts 可以限定每台机器上仅允许一个 helloworld 服务运行。\ncore@core-01 ~ $ fleetctl stop helloworld Unit helloworld.service loaded on 6e1b9fae.../172.17.8.104 core@core-01 ~ $ fleetctl unload helloworld Unit helloworld.service inactive core@core-01 ~ $ mv helloworld.service helloworld@.service core@core-01 ~ $ fleetctl start helloworld@1 Unit helloworld@1.service inactive Unit helloworld@1.service launched on 6e1b9fae.../172.17.8.104 core@core-01 ~ $ fleetctl start helloworld@2 Unit helloworld@2.service inactive Unit helloworld@2.service launched on 91060182.../172.17.8.103 core@core-01 ~ $ fleetctl start helloworld@3 Unit helloworld@3.service inactive Unit helloworld@3.service launched on af1494a6.../172.17.8.102 core@core-01 ~ $ fleetctl start helloworld@4 Unit helloworld@4.service inactive Unit helloworld@4.service launched on c99fef8a.../172.17.8.101 core@core-01 ~ $ fleetctl list-unit-files UNIT\tHASH\tDSTATE\tSTATE\tTARGET helloworld.service\tce68bd4\tinactive\tinactive\t- helloworld@1.service\tce68bd4\tlaunched\tlaunched\t6e1b9fae.../172.17.8.104 helloworld@2.service\tce68bd4\tlaunched\tlaunched\t91060182.../172.17.8.103 helloworld@3.service\tce68bd4\tlaunched\tlaunched\taf1494a6.../172.17.8.102 helloworld@4.service\tce68bd4\tlaunched\tlaunched\tc99fef8a.../172.17.8.101 卸载之前的 helloworld，执行了新的 4 个实例，通过观察可以看到目前实例都是运行在了 4 台不同机器上。\nfleet 的一些疑问 事实上，通过 fleet，可以将服务进行多副本部署，同时通过 fleet 守护程序的正常运行。但是在实际使用中你会发现有一个实际问题，在微服务化场景中，每个服务部署的机器是无法控制的，那么我们怎么知道服务究竟在哪呢？\n如何控制流量流向，或者说服务发现问题，这个我会在下一个 Etcd 的介绍中说一下，敬请期待。\n","date":"2016-05-28T18:00:00Z","permalink":"https://www.4async.com/2016/05/2016-05-28-learning-coreos-part2/","title":"CoreOS 折腾笔记（二）Fleet 进阶"},{"content":"最近在技改完成之后打算进行大量的微服务化改造，而方便进行微服务化的步骤之一，就是将现有的系统移植进入 Docker 环境之中。在标准容器系统的选择上，我把目光放在了 CoreOS 上。实际上，我在 CoreOS 版本还是 2 开头的时候就有简单研究过，但是当时主要作为研究 Docker 的途径，现在则是作为集群化部署的基准系统。从本文开始的一系列折腾则是我在研究 CoreOS 集群化使用的一些纪录，而本文就是介绍一个本地实现容器化机群的步骤。\n安装 Vagrant 略，因为太简单了。另外还需要 VirtualBox，不要忘记装。\n配置 CoreOS-vagrant 执行以下命令：\ngit clone https://github.com/coreos/coreos-vagrant.git cd coreos-vagrant 下载 CoreOS 的 vagrant 配置。值得在进入正式配置之前一提的是，CoreOS 本身是没有默认密码或者安装密码机制的，也就是说，现有的所有认证登录需要通过 SSH 进行。如果是通过 vagrant 安装，会自动生成登录需要的 SSH 密钥，这个是需要额外注意的内容。\n首先，将仓库中提供的两个模版配置文件复制成正式，接下来需要修改对应的文件：\ncp config.rb.sample config.rb cp user-data.sample user-data 首先修改 config.rb 文件。这里重点是两个参数，一个是启动的实例数量，另外一个是升级的版本，我修改成了\n$num_instances=4 $update_channel=\u0026#39;stable\u0026#39; 启动 4 个实例，升级选择的版本则是月度升级版。另外一个 user-data 暂时不作修改。\n接下来执行：\nvagrant up 在一堆输出之后，生成的 4 个实例就已经在运行了。也可以通过命令 vagrant status 查看运行状态。\n➜ coreos-vagrant git:(master) vagrant status Current machine states: core-01 running (virtualbox) core-02 running (virtualbox) core-03 running (virtualbox) core-04 running (virtualbox) This environment represents multiple VMs. The VMs are all listed above with their current state. For more information about a specific VM, run `vagrant status NAME`. 接下来需要把 SSH 密钥添加到 SSH 可以识别的路径中，当然，也可以使用 ssh -i 每次指定 key。\n➜ coreos-vagrant git:(master) ssh-add ~/.vagrant.d/insecure_private_key Identity added: /Users/user/.vagrant.d/insecure_private_key (/Users/user/.vagrant.d/insecure_private_key) ➜ coreos-vagrant git:(master) vagrant ssh core-01 -- -A CoreOS stable (1010.5.0) core@core-01 ~ $ 这里的 -- -A 前两的个横杆表示 ssh 参数的起始。之后的 -A 是标准的 SSH 命令参数，表示将主机的 SSH 秘钥传递到虚拟机里面，这样做是为了之后的涉及 SSH 操作更加方便，否则会在某些命令时报错。\n进入 CoreOS 之后，就可以查看现在集群中的机器，可以通过如下命令：\ncore@core-01 ~ $ fleetctl list-machines MACHINE\tIP\tMETADATA 6e1b9fae...\t172.17.8.104\t- 91060182...\t172.17.8.103\t- af1494a6...\t172.17.8.102\t- c99fef8a...\t172.17.8.101\t- 查看设备是否正常加入。当然，也可以很方便的登录其它系统：\ncore@core-01 ~ $ fleetctl ssh -machine af1494a6 The authenticity of host \u0026#39;172.17.8.102\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is 24:34:be:4e:b7:43:be:94:34:33:b2:81:e0:0c:08:9d. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added \u0026#39;172.17.8.102\u0026#39; (ECDSA) to the list of known hosts. CoreOS stable (1010.5.0) core@core-02 ~ $ exit logout 这样我们的第一步，使用 CoreOS 部署本地集群就已经实现了。\n最后，如果需要关闭这些 CoreOS 实例，可以通过 vagrant halt 停止所有，下次可以通过 vagrant up 就可以重新启动了。\n","date":"2016-05-28T17:00:00Z","permalink":"https://www.4async.com/2016/05/2016-05-28-learning-coreos-part1/","title":"CoreOS 折腾笔记（一）集群部署"},{"content":"之前作为 Docker beta 的第一批用户获得了 beta 的授权，但是因为邮件进了垃圾邮件，所以一直没有发现。今天给 Docker 发邮件申请 beta 测试才知道已经通过了，赶紧尝鲜起来。如果你没有权限，可以尝试到 Docker Beta 申请测试资格。\n测试版本可能存在风险，请自行判断。\n安装之前 Docker for Mac 需要一些前置要求，官方文档提供的数据如下：\n2010 年之后的 Intel Mac 机型，支持 MMU(Memory Management Unit) 虚拟化、EPT(Extended Page Table) 等特性 OSX 10.10.3 以上系统 至少 4GB 内存 VirtualBox 4.x 与 Docker for Mac 冲突，因此如果你安装这个系列的 Virtualbox 需要卸载。 如果之前装过 Docker Toolbox 的话，需要一些额外的操作去与 Docker Toolbox 兼容。（我选择了直接卸载现有的 Docker Toolbox。XD ）\n安装 Docker for Mac Docker for Mac 是一个 98.3M(Mac 显示为 103.1MB) 的 DMG 文件，下载下来之后双击文件，将鲸鱼拖拽到 Applications 文件夹中即可。\n第一次打开 Docker for Mac 需要提供邀请码，同时需要特殊权限写入 Docker bin 文件。\nDocker for Mac 包含了 Docker 引擎，Docker 命令行客户端，Docker Compose 和 Docker Machine。之后双击应用程序中的 Docker 就可以启用引擎，这时，右上角会有一个鲸鱼的标志。\n点击标记可以获得菜单，进行设置，查看日志等等信息。\n点击检查更新可以验证当前是否为最新版本的 Docker for Mac。\nDocker for mac 尝鲜 先看一下 Docker 的设置，可以设置使用的内存启用方式，是否启用 VPN 兼容模式 (现在我看论坛有人提到会不兼容 Cisco AnyConnect)，禁用 VM 的时光机备份功能。另外可以在这里提供了直接卸载按钮，不想用的时候也比较方便。\n附带的软件版本也是新的 Docker 软件：\n➜ ~ docker -v Docker version 1.11.0, build 4dc5990 ➜ ~ docker-compose --version docker-compose version 1.7.0, build 0d7bf73 ➜ ~ docker-machine --version docker-machine version 0.7.0, build a650a40 跑一个 Ubuntu 测试一下：\n➜ ~ docker run -i -t --name ubuntu ubuntu:16.04 /bin/bash root@d5ceb77516bf:/# uname -a Linux d5ceb77516bf 4.4.6 #1 SMP Mon Apr 18 19:18:15 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux root@d5ceb77516bf:/# 我发现一直不怎么会写结尾／结论，那么最后扯个蛋：我们在进行 Docker 在 CI/CD、线上环境中的应用摸索，如果你有兴趣交流，欢迎联系 kevin \u0026lt;at\u0026gt; yeeuu \u0026lt;dot\u0026gt; com。\n","date":"2016-04-26T18:00:00Z","permalink":"https://www.4async.com/2016/04/2016-04-26-docker-for-mac-beta/","title":"Docker for Mac 尝鲜"},{"content":"目前我们有大量的应用采用了 Golang 程序进行构建，但是在执行研发流程里我们会发现一些来自于静态编译程序的不便：相对于我们之前使用的 Python 语言程序而言，我们无法在程序功能的单元测试里大量的使用 Mock 方式来进行高效测试。\n而这些东西往往可以在开发人员编写单元测试用例时有效的节省时间和一些额外的环境准备成本。因此，这也给我们的程序的单元覆盖率带来了很多麻烦的地方：一些依赖于额外验证和表现的情况或者小几率出现的情况需要复杂的模拟步骤，对开发进度和效率带来了一些额外的影响。如何编写一个测试友好的 Golang 程序成为一个无法绕开的问题。\n从动态语言到静态语言 动态语言有良好的运行时修改属性，在运行时的动态修改函数，可以进行有效的 Mock。比如在 Python（以 3 为例，内置了 unittest.mock 标准库）程序中:\nwith patch.object(ProductionClass,\u0026#39;method\u0026#39;, return_value=None) as mock_method: thing = ProductionClass() thing.method(1, 2, 3) 自然而然的，我们想到了这样的用法：\nvar imp = func() bool {return true} func TestFunc(t *testing.T) {defer func(org func() bool) {imp = org}(imp) img = func() bool {return false} // testing or something else... } 这样实现 Mock 是完全可以的，但是实际上会带来一些额外的问题，比如说在 MVC 框架中，我们正常采用的方式一般是这样的：\nimport (\u0026#34;models\u0026#34; ... ) func A(ctx Context) error { ... data := models.Data() ... } 这种方式则是无法在运行中进行动态 Mock 的，除非将其转换为参数方式进行调用。\nfunc TestFunc(t *testing.T) {Convey(\u0026#34;test\u0026#34;, t, func() {defer func(org func() string) {models.Data = org // Error: cannot assign to models.Data}(models.Data) models.Data = func() string {return\u0026#34;mocked!\u0026#34;} .... }) } 转成\n// var data = models.Data // in A: data := data() func TestFunc(t *testing.T) {Convey(\u0026#34;test\u0026#34;, t, func() {defer func(org func() string) {data = org}(data) data = func() string {return\u0026#34;mocked!\u0026#34;}}) } 这样写法略微会多处大量的临时函数指针变量，如果是使用这种方式则需要额外的变量值的对应关系，测试完成后变量值需要恢复成原有指针（如果需要测试正常功能）。\n从变量到接口 除了上面介绍的方法以外，是不是还有看起来稍微优雅一点的测试方法呢？我们尝试将上面的函数形式换成下面的接口形式，将 interface 对应的变量作为全局变量。\n// main.go var fetcher DataFetcherInterface type DataFetcherInterface interface {Data() string } type DataFetcher struct { } func (d DataFetcher) Data() string {return\u0026#34;hello world!\u0026#34;} func Func(w http.ResponseWriter, r *http.Request) {fmt.Fprintf(w,\u0026#34;%s\u0026#34;, fetcher.Data()) } func main() {fetcher = DataFetcher{} http.HandleFunc(\u0026#34;/\u0026#34;, Func) http.ListenAndServe(\u0026#34;127.0.0.1:12821\u0026#34;, nil) } 这样的话我们就可以在测试文件里面定义一个 FakeDataFetcher，实现相关的功能：\n// main_test.go type FakeDataFetcher struct { } func (f FakeDataFetcher) Data() string {return\u0026#34;mocked!\u0026#34;} func TestFunc(t *testing.T) {Convey(\u0026#34;test\u0026#34;, t, func() {defer func(org DataFetcherInterface) {fetcher = org}(fetcher) fetcher = FakeDataFetcher{} req, _ := http.NewRequest(\u0026#34;GET\u0026#34;, \u0026#34;http://example.com/\u0026#34;, nil) w := httptest.NewRecorder() Func(w, req) So(w.Body.String(), ShouldEqual, \u0026#34;mocked!\u0026#34;) }) } 这样可以减少变量的生成个数，同时，也可以通过 FakeDataFetcher{} 传入不同的参数，实现不同的 Faker 测试。值得注意的是，在这个 interface 方法中需要特别注意变量共享的线程安全问题。\n依赖注入 上面两种方法似乎思路类似，除了这些方案之外，还有没有其他的方案呢？最后介绍一下依赖注入的方式，这种方式也可以与上面提到的接口方式搭配使用。这种方式实现起来比较简单方便，也非常适合利用在一些面向过程场景中。\n// main.go type EchoInterface interface {Echo() string } type Echoer struct { } func (e Echoer) Echo() string {return\u0026#34;hello world!\u0026#34;} func Echo(e EchoInterface) string {return e.Echo() } func main() {provider := Echoer{} fmt.Println(Echo(provider)) } 测试文件:\n// main_test.go type FakeEchoer struct { } func (f FakeEchoer) Echo() string {return\u0026#34;mocked!\u0026#34;} func TestFunc(t *testing.T) {Convey(\u0026#34;test\u0026#34;, t, func() {provider := FakeEchoer{} So(Echo(provider), ShouldEqual, \u0026#34;mocked!\u0026#34;) }) } 总结 上面的几种测试方法基本上是通过固定的原型将代码转为测试友好的 Golang 代码。这样可以通过 Mock，减少来自于其他数据和前置条件的影响，尽可能的降低代码开发的附加成本。\n","date":"2016-04-18T18:40:00Z","permalink":"https://www.4async.com/2016/04/2016-04-18-writing-testable-golang-code/","title":"编写测试友好的 Golang 代码"},{"content":"从 asyncio 简单实现看异步是如何工作的\nby ipfans\n注：请使用 Python 3.5+ 版本运行以下代码。\n先从例子看起 首先我们来看一个 socket 通讯的例子，这个例子我们可以在官方 socket 模块的文档中找到部分原型代码：\n# echo.py from socket import * # 是的，这是一个不好的写法 def echo_server(address): sock = socket(AF_INET, SOCK_STREAM) sock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1) sock.bind(address) sock.listen(5) while True: client, addr = sock.accept() print(\u0026#34;connect from\u0026#34;, addr) echo_handler(client) def echo_handler(client): while True: data = client.recv(10000) if not data: break client.send(str.encode(\u0026#34;Got: \u0026#34;) + data) print(\u0026#34;connection closed.\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: echo_server((\u0026#39;\u0026#39;, 25000)) 但是同步模式会有一个问题，当进行通讯是阻塞的，当一个连接占用时就会阻碍其他连接的继续，这个时候应该怎么更快的运行呢？\n回顾历史 在 asyncio 出现之前，我们都是怎么提高效率的呢？首先想到的方法就是多线程处理：\n# echo_thread.py from socket import * import _thread def echo_server(address): sock = socket(AF_INET, SOCK_STREAM) sock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1) sock.bind(address) sock.listen(5) while True: client, addr = sock.accept() print(\u0026#34;connect from\u0026#34;, addr) _thread.start_new_thread(echo_handler, (client,)) def echo_handler(client): while True: data = client.recv(10000) if not data: break client.send(str.encode(\u0026#34;Got: \u0026#34;) + data) print(\u0026#34;connection closed.\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: echo_server((\u0026#39;\u0026#39;, 25000)) 当然了，我们都知道多线程之下总是会有一些问题的。那么还有更好的方案吗？如果你了解过C10k 问题，你一定听过 epoll```、```kqueue 之类的大名。那么，能在 Python 中使用这些功能吗？答案是肯定的。那就是select。\n# echo_select.py from socket import * import select def echo_server(address): sock = socket(AF_INET, SOCK_STREAM) sock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1) sock.bind(address) sock.listen(5) input = [sock,] while True: r, _, _ = select.select(input, [], []) for s in r: if s == sock: client, addr = sock.accept() print(\u0026#34;connect from\u0026#34;, addr) echo_handler(client) def echo_handler(client): while True: data = client.recv(10000) if not data: break client.send(str.encode(\u0026#34;Got: \u0026#34;) + data) print(\u0026#34;connection closed.\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: echo_server((\u0026#39;\u0026#39;, 25000)) 相比 _thread 来说，select 更加底层，提供了最基础的等待 IO 完成功能。但是缺点是这个功能太单一了，这也就是为什么后面语言提供了 asyncio```。最早应该是 python-dev中 [提出了](https://mail.python.org/pipermail/python-ideas/2012-May/015223.html) 要在标准库中添加基于select的异步 IO 功能。之后 Python 在 3.4 版本之中就加入了 [```selectors](https://docs.python.org/3.5/library/selectors.html) 与 asyncio 库用于异步 IO。\n其他的方法还有 gevent、Twisted、Tornado 等等的方案，这里就不多赘述了。(在 3.4 的时候我一直觉得 yield form 太丑陋了，相对我宁愿继续用 Tornado 的 yield 方式。当然这个更加主观的原因吧，不过现在 async/await 方式明显让我又让我爱上了。）\n从同步到 asyncio 那么如何在 asyncio 框架下如何实现异步 socket 通讯的例子呢？事实上官方文档中提供了两个比较高层封装过的 asyncio 库例子 TCP echo server protocol 和TCP echo server using streams。这两个例子采用的是 asyncio 的 socket 通讯高级别封装，似乎与我们同步代码相差有点远。这里我们实际例子中使用了更加底层的Low-level socket operations。这个更接近于我们在同步状态下使用 socket 的代码。\n# aecho.py from socket import * import asyncio loop = asyncio.get_event_loop() async def echo_server(address): sock = socket(AF_INET, SOCK_STREAM) sock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1) sock.bind(address) sock.listen(5) sock.setblocking(False) # 设置非阻塞 while True: client, addr = await loop.sock_accept(sock) print(\u0026#34;connect from\u0026#34;, addr) loop.create_task(echo_handler(client)) async def echo_handler(client): with client: while True: data = await loop.sock_recv(client, 10000) if not data: break await loop.sock_sendall(client, str.encode(\u0026#34;Got: \u0026#34;) + data) print(\u0026#34;connection closed\u0026#34;) loop.create_task(echo_server((\u0026#39;\u0026#39;, 25000))) loop.run_forever() 其中遇到的 create_task 会相对同步状态下无法对应，这个方法用于安排一个异步任务的执行，将一个异步方法封装为 future 对象。其他的 Event Loop 中的功能基本与传统的程序相同。\n从 asyncio 到自己的实现 那么在 asyncio.event_loop 中到底发生了什么呢？我们可以尝试用自己的程序实现一下。\n如果你阅读过PEP-0492，你就知道，实际上 Python 的协程是通过生成器实现的。\n# async_yield.py from types import coroutine @coroutine def read_wait(sock): yield \u0026#34;read_wait\u0026#34;, sock # 为什么有个 read_wait？等下介绍 下面来模拟实际调用：\npython -i async_yield.py \u0026gt;\u0026gt;\u0026gt; f = read_wait(\u0026#34;somesocket\u0026#34;) \u0026gt;\u0026gt;\u0026gt; f \u0026lt;generator object read_wait at 0x10200d5c8\u0026gt; \u0026gt;\u0026gt;\u0026gt; f.send(None) (\u0026#39;read_wait\u0026#39;, \u0026#39;somesocket\u0026#39;) \u0026gt;\u0026gt;\u0026gt; f.send(None) Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; StopIteration 如果不了解 send() 与 StopIteration 作用的话，请参考 PEP-0492 中相关的描述。\n接下来继续完善 write 方法，并且实现我们自己的 Loop。\n# async_yield.py from types import coroutine from collections import deque from selectors import DefaultSelector, EVENT_READ, EVENT_WRITE @coroutine def read_wait(sock): yield \u0026#34;read_wait\u0026#34;, sock @coroutine def write_wait(sock): yield \u0026#34;write_wait\u0026#34;, sock class Loop(object): def __init__(self): self.ready = deque() self.selector = DefaultSelector() async def sock_recv(self, sock, maxbytes): await read_wait(sock) return sock.recv(maxbytes) async def sock_accept(self, sock): await read_wait(sock) return sock.accept() async def sock_sendall(self, sock, data): while data: await write_wait(sock) n = sock.send(data) data = data[n:] def create_task(self, coro): self.ready.append(coro) def run_forever(self): while True: while not self.ready: events = self.selector.select() for key, _ in events: self.ready.append(key.data) self.selector.unregister(key.fileobj) while self.ready: self.current_task = self.ready.popleft() try: op, *args = self.current_task.send(None) getattr(self, op)(*args) except StopIteration: pass def read_wait(self, sock): self.selector.register(sock, EVENT_READ, self.current_task) def write_wait(self, sock): self.selector.register(sock, EVENT_WRITE, self.current_task) 对于之前一节中的 aecho.py 文件，我们只需要修改一下导入模块与 loop 的获取方法即可：\n# pecho.py from socket import * import async_yield loop = async_yield.Loop() async def echo_server(address): sock = socket(AF_INET, SOCK_STREAM) sock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1) sock.bind(address) sock.listen(5) sock.setblocking(False) # 设置非阻塞模式 while True: client, addr = await loop.sock_accept(sock) print(\u0026#34;connect from\u0026#34;, addr) loop.create_task(echo_handler(client)) async def echo_handler(client): with client: while True: data = await loop.sock_recv(client, 10000) if not data: break await loop.sock_sendall(client, str.encode(\u0026#34;Got: \u0026#34;) + data) print(\u0026#34;connection closed\u0026#34;) loop.create_task(echo_server((\u0026#39;\u0026#39;, 25000))) loop.run_forever() async_yield 发生了什么？ 首先，我们定义了两个协程函数 read_wait 和 write_wait，分别用于相应处理读取操作与写入操作。其中返回了一个 tuple 类型数据，用于在 op, *args = self.current_task.send(None) 中填充方法名和参数，之后在 getattr(self, op)(*args) 中进行分别调用。\n下面 Loop 类实现了在 pecho 中用到的所有异步函数。初始化时的 self.ready 用于存储协程的调用序列。该序列通过 create_task 添加协程到队列中。\n在 run_forever 中，如果目前队列为空，则通过 self.selector.select() 提取一个事件放入队列处理，若队列存在通过 self.current_task.send(None) 通知事件发送，从而调用对应的事件功能。你也可以在 op, *args = self.current_task.send(None) 后添加 print(op) 获取实时的调用情况。\n结语 事实上这篇文章的思路是基于 @dabeaz 在 Python Brasil 上的 keynote 整理而来。dabeaz 还有另外一个非常不错的基于 select 的异步库，名字叫做curio，是一个了解实现异步库的很好教程。\n最后讲个段子，之前有人开玩笑，蟒爹开发一个功能，之后大家都不会正确使用，直到 dabeaz 站出来告诉大家如何正确使用新功能。在写这篇文章的时候虽然很想找出来出处，但是似乎找不到了\u0026hellip;\n","date":"2016-02-03T15:40:00Z","permalink":"https://www.4async.com/2016/02/2016-02-03-simple-implement-asyncio-to-understand-how-async-works/","title":"从 asyncio 简单实现看异步是如何工作的"},{"content":"在创业公司初创伊始，如何选择合适的语言决定了产品后续的技术栈和如何进行合理的业务支撑方向。如果你在读这篇文章之前，更倾向于选择 Java/C#/PHP 常见语言技术栈，我觉得对于你而言，这篇文章帮助不大。因为对你而言，这些技术栈意味着更加方便招人，更方便的故障处理资料等等。但是如果一个初创公司想要选择合理的小众语言技术栈，我想这篇文章对你也许有一些帮助。\n在文章开始之前，我觉得有必要描述一下所谓的小众语言，这里我在最初进行技术选型时，考察了包括：\nPython: 你可能在接触爬虫、大数据分析等等方面听过 Python 的大名，大家都知道 Pythonista 都习惯说的一句话就是：人生苦短，我用 Python。 Ruby: 你如果做过 Web 框架，你遇到最多的是很多人都会提到 RoR 框架。用过之后只有一个惊叹了。 JavaScript（Nodejs）: 有没有听过 JavaScript 全栈工程师？有没有听过 JavaScript 全栈工程师？有没有听过 JavaScript 全栈工程师？ Golang: 简单粗暴的语言，也许你见过很多人跟你吹嘘，Golang 是下一代的云计算开发语言。 其他语言太过于小众，考虑语言的应用很多需要得到更多的社区支持，目前不在考虑的范畴内。\n小众语言的劣势 正所谓知己知彼百战不殆，在了解一个技术选型之前，最好是研究这些语言的缺点。因为你最后感觉这个技术选型不适合你的时候，根本的原因是这些劣势影响了你。\n在上面提到的几个语言中，Python、Ruby、JavaScript 是属于动态语言。关于动态语言的争议最大的地方是：动态语言到底是否合适进行大型项目。事实上，在某些阶段，多人合作，并且大家水平语言不同时，这个时候通常会有这样的问题：团队需要花更多的时间在确保动态语言的准确性上。对于一个项目有高可用、低错误率的要求时，由于语言的动态特性，就需要对程序开发时的单元测试和后期集成测试的要求更高。因为变量在运行时才会赋予类型含义，所以很难在静态检测过程中发现足够多的问题。这样对测试人员的压力也会更大，当你没有合适的测试人员时，这个时候通常会变成，你只覆盖测试了理想情况下的成功失败情况，而对特别异常情况缺少评估。\nPython、Ruby 语法对程序员而言最大的成本在于需要重新学习一门新的语言。这个学习成本、时间成本通常对初创公司而言通常是支付不起的，哪怕像这些比较容易学习的动态语言而言。另外一个值得一提的是，无论是 Python 还是 Ruby，从长远看，如果你后续有较大的用户增长又需要保证用户体验时，Python、Ruby 的执行效率和吞吐量会有较大的影响。\nPython 和程序员入门的语言差距较大：用 4 个空格表述程序缩进。这意味着，程序员直接从网上寻找解决方案（拷贝代码）时成本更高，因为很有可能他需要手工进行代码格式化，这样有可能造成程序逻辑的改变。另外一个不得不提的是一些 Python 库看似好用，实际上或多或少有有一些坑，这对新手而言，往往是致命的。对应的，Ruby 语言本身时不存在这些问题的。然而 Ruby 作为开发主语言时最大的问题是，如果选择 RoR 框架作为初始的 Web 框架时，如果没有一个熟悉 RoR 框架的人，那么学习修改 RoR 框架的成本是特别高的：对于一个通用型框架而言，你可能需要更多的特殊场景定制，这可能需要做大量的猴子补丁，如果不对框架有一个清晰了解时，这样的成本会更高。\nJavaScript（后面统一用 Nodejs 代称）则借助 Nodejs 实现了高性能和较大吞吐量。而且从语言层面上，JavaScript 对很多程序员并不陌生。然而，在过去的很长时间，真正熟练掌握 JavaScript 的都是前端工程师，这是一个非常尴尬的问题。对后端工程师而言，Nodejs 需要与前端不同的技术栈，而且大概没有公司希望一个完全没有任何后端经验的前端工程师去接手后端项目的开发的。Nodejs 是一个年轻的语言，年轻必然会伴随一些问题，比如，库比较少（当然现在也是井喷期）。一些必要的库需要慢慢寻找。事实上，我也不得不吐槽，可能是开发者水平问题，导致很多 npm 提供的包，往往或多或少存在一些比较恼人的 BUG，这些 BUG 可能会在你开发过程中，正常运行中出现，而你却不得不干掉它。对于这种 BUG，很多时候更快的处理方式是你自己动手进行快速修复。然而当你的程序员不具备这种能力的时候，就需要提一个 issue 到开发者，由开发者进行修复，并且需要等待版本更新到 npm 源中。很多时候这个过程都是比较尴尬的，尤其是你选择了一个开发者并不是特别活跃的包。\nGolang 是一个编译型语言，语法简单，似乎一切看起来都是那么的美好。事实上 Golang 本身还是处于在 Google 开发维护的阶段，本质上虽然语言完全开源了，但是却不是一个完全社区维护的语言。换句话说，Google 会决定 Golang 未来的走向。不过好在第三方包都是由社区来驱动的，这样还是提供了更多的可定制性。另外一个比较烦恼的是 Golang 本身的包管理机制，事实上，这是完全是一个伪命题。别告诉我你觉得 go get 完全足够了，那只是因为你还没有遇到依赖导致 break 的问题而已（Godep 也可以解决一部分问题，然而第三方包 API 的变更你是没办法控制的）。不过从现在看起码 Google 意识到自己的问题，也正在努力改变吧。Golang 同时也是一个年轻的语言，库比较少的问题也会出现。虽然你可以利用 cgo 去桥接一些现有的 C/C++ 库到 Golang 程序中，但是这部分的代码维护，涉及到 GC 的优化处理等各个方面，对开发者的要求不低。另外一个大家都会抓住讲的就是 GC 问题，在高并发环境下 GC 的影响从 1.5 开始下降了很多，但是 GC 并不像 Java 一样更加可控，很多时候 GC 还是需要进行代码层面进行控制。\n上面所有的小众语言还面临一个共同的问题，招人。没错，找到一个适合的小众语言工程师是一个痛苦的事情。虽然你可以通过语言的高开发效率去节省人力，但是当你面临人手紧缺的困境时，去找一个合适的替补人就变成了一件非常昂贵的事情。通常你需要从现有的人员中培养则更加靠谱。（庆幸的是，经过培养后，哪怕我们的移动开发工程师也可以 hold 住我们现在的后端部分需求。当然，这只是感兴趣的前提下。）\n如何选择合适的技术栈 这是一个复杂的问题。选择合适的技术栈，你需要覆盖上面我提到的所有的劣势问题。比如说，你选择了 Nodejs，你就要考虑，你可以 hold 住所有的难点，你可以修正开源包的问题，甚至你可以解决现在没有包的难题，OK，那么你选择这个语言本身是没有任何问题的。\n而我在综合考虑之后，选择的技术栈比较简单：Python 和 Golang。其实选择的原因很简单，这两种语言我更熟悉。没错，这个是第一个理由。为什么会有 2 个技术栈，这个其实与我们现有的业务状态和未来发展的思考有关，这个会在后面进行一下介绍。厚脸皮的说一下，Python 和 Golang 语言中的绝大部分问题目前都可以自己解决，这也是另外一个理由。\n接下来，人员培养方面，上面提到的小众语言大多培训容易，以现在产品发展节奏和产品演进速度，我们的人员培养成本目前是可以承受的。\n使用 Python 的原因，是__开发更快，从而可以快速试错__。利用现有的 Web 框架，搭配合适的数据库，我们可以在 1-2 周内实现一个完整产品的上线，进行快速试错。我们针对 Python 制订了一系列的标准，用于规范代码的格式，保证代码的强壮度。这个可以参考下我之前关于 代码风格要求的文章。\n使用 Golang 的原因，在于需要制作大型长期稳定运行项目的考量。事实上，我在前面也提到了，在目前的开发过程中，Python 在多人协作过程中个人编码风格、工程性上要更弱、长期运行无法控制的内存泄漏等等问题，如果需要长期稳定运行，我更倾向于选择可以进行编译的编译型语言，通过静态检查＋动态测试方式，更好的保证程序的强壮型。\n结语 上面啰里八嗦说了那么多，只是想告诫大家，对小众语言而言，选择的机会成本是特别高的。如果你只是验证试错，或者你只是想卷一笔钱就跑，小众语言的高开发效率是绝对可以满足你的。但是从一个大型工程的角度，你需要通盘考虑小众语言的劣势，选择一个合适的语言作为你的技术栈是十分必要的。\n另，杭州云柚科技长期招聘有潜力的 Python/Golang 开发者，有兴趣的请发送简历至 kevin |at| yeeuu |dot| com\n","date":"2016-01-27T09:48:00Z","permalink":"https://www.4async.com/2016/01/2016-01-27-startup-architecture-language/","title":"聊聊初创公司的后端语言选型 (小众语言)"},{"content":"mgo 库 是一个很好用的 MongoDB 驱动。对我们来说，主力数据库是 MongoDB，因此这个驱动对我们来说也是非常重要的。但是，mgo 库有些问题算是一些坑，这里我做了一些简单的整理。\n一些关于 bson.ObjectId 的问题 ObjectId 为空的判断 如果你看 bson.ObjectId 定义的话，它是一个 string 类型的数据。但是如果你直接定义一个结构，并且生成对象时，这个对象并不是这样的。\n我们首先定义一个结构体：\ntype Home struct {ID bson.ObjectId `bson:\u0026#34;_id,omitempty\u0026#34;` Name string `bson:\u0026#34;name\u0026#34;` } 然后看看一个生成的内容\nh := Home{Name:\u0026#34;123\u0026#34;} fmt.Println(h.ID) 结果是 ObjectIdHex(\u0026quot;\u0026quot;)。换句话说，如果你是想判断一个结构体的 ObjectId 是否为空，使用 h.ID ==\u0026quot;\u0026quot; 是一定会结果为 false 的。如果你想判断是否为空，正确的方式应该为：\nh := Home{Name:\u0026#34;123\u0026#34;} fmt.Println(h.ID) fmt.Println(h.ID.Hex() == \u0026#34;\u0026#34;) 正确的生成 ObjectId 首先值得注意的是 NewObjectIdWithTime(t time.Time) 这个方法生成的 ObjectId 并不是唯一的，结果可能导致的是插入失败。最有效的方式是设定 ObjectId 对象支持 omitempty 属性，就像我上面生成的结构体一样，由数据库统一调配生成 ObjectId。如果真的确实需要，可以选择 NewObjctId()。\n时间问题 之前看到有人问，为什么保存的时间进入到数据库中慢了 8 个小时呢？原因是在保存进入 MongoDB 时，数据是按照 UTC 时间（不懂什么是 UTC？看这里）进行的保存，但是取出是按照当前时区来取出。那么问题来了，我的客户如果不都是国人，我怎么保存时间呢？目前我们采用了两种方式来确定数据库的保存时间。一种是 Unix 时间戳 ，这个是不受到时区的影响的，由前端格式化为对应的时区时间；另外一种则是需要在额外的对从 MongoDB 数据库中取出的数据进行额外的时区校准，简单来说可以这样：\ntype Home struct {ID bson.ObjectId `bson:\u0026#34;_id,omitempty\u0026#34;` Name string `bson:\u0026#34;name\u0026#34;` InsertTime time.Time `bson:\u0026#34;insert_time\u0026#34;` } func main() {sess, _ := mgo.Dial(\u0026#34;127.0.0.1\u0026#34;) c := sess.DB(\u0026#34;test\u0026#34;).C(\u0026#34;home\u0026#34;) h := Home{Name:\u0026#34;123\u0026#34;, InsertTime: time.Now()} c.Upsert(bson.M{\u0026#34;name\u0026#34;:\u0026#34;123\u0026#34;}, h) c.Find(bson.M{\u0026#34;name\u0026#34;:\u0026#34;123\u0026#34;}).One(\u0026amp;h) fmt.Println(h.InsertTime.Format(\u0026#34;2006-01-02 15:04:05\u0026#34;)) tz, _ := time.LoadLocation(\u0026#34;America/New_York\u0026#34;) fmt.Println(h.InsertTime.In(tz).Format(\u0026#34;2006-01-02 15:04:05\u0026#34;)) } 更高效的使用 Session 在 MongoDB 中合理使用 Session 可以更高效的操作数据库，做法是在之前进行一次 Copy 操作：\nsessionCopy := mongoSession.Copy() defer sessionCopy.Close() collection := sessionCopy.DB(TestDatabase).C(\u0026#34;buoy_stations\u0026#34;) log.Printf(\u0026#34;RunQuery : %d : Executing\\n\u0026#34;, query) // Retrieve the list of stations. var buoyStations []BuoyStation err := collection.Find(nil).All(\u0026amp;buoyStations) if err != nil {log.Printf(\u0026#34;RunQuery : ERROR : %s\\n\u0026#34;, err) return } log.Printf(\u0026#34;RunQuery : %d : Count[%d]\\n\u0026#34;, query, len(buoyStations)) 另外值得一提的是 MongoDB 本身，目前我们已经提升到了 MongoDB 3.0+ 版本，优势是相对之前版本的 WiredTiger 引擎比较令人印象深刻（如果有条件可以选择最新的 3.2 版本）。大家对之前 MongoDB 中索引建立的痛苦有印象，这个现象在 WiredTiger 引擎中也有所改进。更多的改进可以在 WiredTiger 的 官网 和 MongoDB 的 性能白皮书 中了解更多。最后，当然，我仍然不建议在非严重必要情况下创建多余索引，这对 MongoDB 的性能和资源消耗都有较大影响。\n","date":"2016-01-26T21:30:00Z","permalink":"https://www.4async.com/2016/01/2016-01-26-something-about-mgo-driver/","title":"Mgo 库的常见坑总结"},{"content":"我们是如何落实 Code Style Guide 的（Python 篇）\n最近年终，总是想谈谈过去一年的感悟和积累。接下来大概有几篇关于项目管理等等一些小方面的介绍，这篇文章主要介绍一下我们如何将 Python 编码规范真正落实到程序的实际开发过程中的。\n编码规范选择 Python 作为灵活的脚本语言，在格式方面并不存在太多的限制（相对编译语言）。这样会导致一个比较蛋疼的问题：在项目开发过程中，由于个人的习惯和编码风格，导致程序缺少一个统一的标准，每个人的代码表现形式也不同。因此，在实际项目由于新人加入、老人退出过程中会产生比较高的模块维护成本。因此，在实际的项目开发中，选择一个编码标准也是比较重要的。\n面对编码风格选择，比较常见的包括 PEP-8 和 Google Python Style Guide。在最后，我选择了 PEP-8 作为项目中的实际应用标准，要求程序需要在满足编码要求规范的前提下进行编码。\n除了对代码编码更个的要求以外，我们还对 import 等具体的细节进行了标准化。\n尽量规范注释 在降低模块维护成本过程中，另外一个比较好的方式是尽量提供良好的代码注释。尽管这个算是一个和语言无关的老生常谈的问题，我只是想在这里提一下另外一个 PEP：PEP-0257，这里介绍了一种约定的 docstring 编写方法，对于编辑器而言，可以通过插件快速实现注释。\n不过我考虑到对个人习惯的影响较大，这个 PEP 实际项目开发中并未作为实际开发规范，只是鼓励大家在项目中进行执行。\n从规范到执行 从代码开发最初的规范约定是一回事，当回到开发过程中，开发者难免会因为个人的习惯或者疏忽等各种原因导致程序开发过程中程序编码风格不统一问题。因此在实际开发过程中，我们又需要通过工具保证程序在实际过程中能够帮助规范化或者检查格式错误。\n借助社区的力量，我们最终选择了工具 flake8、yapf 和 isort。其中，flake8 用于检查我们的代码是否正确的执行了标准；yapf 工具用于快速进行 PEP-8 标准化，减少了人工修改的成本；isort 工具则是执行我们之前提到的 import 标准化工作。\nyapf 是 Google 员工开发的一个 Python 格式化工具，它支持 PEP8 与 Google 编码标准，一些具体的使用方式可以查看项目的主页。在实际的项目落地过程中，你应该会遇到的一个问题是关于 flake8 与 yapf 标准不一致导致一个通过另一个无法正常通过的问题。这一个方面，我们选择的方式是统一妥协成 flake8 的标准。对于 yapf 不支持的部分，我们建议活用 # yapf: disable 标记。\n还有另一个问题是关于一些 flake8 本身的标准（或者说 PEP－8 标准）问题，比如 flake8 常见问题：E501 程序代码长度超过 79 个字符问题，我们实际编码过程中对这一标准做了适当妥协，允许最大单行字符串长度为 200。但是我们仍然建议缩小至 79 个字符内表示完。\n从执行到检查 在最后一关，是我们的上线前检查。我们设置了代码质量检查关卡和系统集成测试。\n在代码质量检查过程中，我们会对程序的实际代码质量进行评估。我们对代码质量进行打分，对于分值较低的代码不允许提交进入 master 分支。代码质量的检查，我们同样的采用了 flake8 工具作为统一标准。最后个人的代码质量，通过 Webhook 也会直接反馈在具体的项目管理工具中。\n","date":"2016-01-15T17:30:00Z","permalink":"https://www.4async.com/2016/01/2016-01-15-how-we-follow-python-style-guide/","title":"我们是如何落实 Code Style Guide 的（Python 篇）"},{"content":"在 cargo.toml 文件中添加\n[dependencies] hyper = \u0026#34;0.7.2\u0026#34; src 中创建 main.rs\nextern crate hyper; use std::io::Read; use hyper::Client; fn main() {println!(\u0026#34;welcome to rust http\u0026#34;); let client = Client::new(); let mut res = client.get(\u0026#34;https://httpbin.org/get\u0026#34;).send().unwrap(); assert_eq!(res.status, hyper::Ok); println!(\u0026#34;headers:\\n {}\u0026#34;, res.headers); let mut body = String::new(); res.read_to_string(\u0026amp;mut body).unwrap(); println!(\u0026#34;body:\\n {}\u0026#34;, body); res = client.post(\u0026#34;https://httpbin.org/post\u0026#34;).body(\u0026#34;{\\\u0026#34;a\\\u0026#34;:1}\u0026#34;).send().unwrap(); assert_eq!(res.status, hyper::Ok); println!(\u0026#34;headers:\\n {}\u0026#34;, res.headers); let mut body = String::new(); res.read_to_string(\u0026amp;mut body).unwrap(); println!(\u0026#34;body:\\n {}\u0026#34;, body); } 执行 cargo build 后执行 ./target/debug / 项目名 即可看到效果。\n","date":"2016-01-06T17:13:00Z","permalink":"https://www.4async.com/2016/01/2016-01-06-rust-http-client/","title":"一个 Rust HttpClient 例子"},{"content":"我们在项目中除了 大量的使用 Python 外，也大量的使用了 Golang 构建高效基础运行服务。在使用 Golang 过程中，我们发现 Golang 程序缺少依赖库版本功能是一个非常令人头大的问题：某些依赖在某个 commit 之后发生了 API 变更之后，如果不修改代码很难兼容，然而开发者之间很有可能因为参与的时间不同，导致执行 go get 命令获取的版本不同，而导致在不同电脑上出现编译不通过问题。同时，在多个程序中，如果使用的 commit 版本不同，也可能会导致程序编译过程中出现不同的问题。\n在之前，我们解决这个问题有两个方案，一种是拆解 go get 命令的执行，首先创建对应依赖目录，利用 git 命令切换至指定的 commit，然后执行 go install 命令。另外一种比较省事的方法是使用 godep 工具，这里就不做过多介绍了，具体可以参考文档或者搜索中文教程。\n在 Golang1.5 之后，Go 提供了 GO15VENDOREXPERIMENT 环境变量，用于将 go build 时的应用路径搜索调整成为 当前项目目录 / vendor 目录方式。通过这种形式，我们可以实现类似于 godep 方式的项目依赖管理。不过起码在程序编译过程中，再也无需在其他端部署一个 godep 工具。\n在使用之前，需要安装一个辅助工具（如果 Golang 自改一个就好了）：go get -u -v github.com/kardianos/govendor。\n下面，我们用一个例子来说明。首先有一个名为 vendorproj 的项目。假如只有一个文件：\npackage main import (\u0026#34;github.com/yeeuu/echoic\u0026#34;) func main() {e := echoic.New() e.SetDebug(true) e.Run(\u0026#34;127.0.0.1:4321\u0026#34;) } 执行一下命令就可以生成 vendor 文件夹：\n$ govendor init $ ls main.go vendor $ cd vendor/ $ ls vendor.json 这个 vendor.json 会类似 godep 工具中的描述文件版本的功能。接下来，需要执行命令将当前应用必须的文件包含进来\n$ govendor add +external 如果需要更新或移除，可以参考一下工具的具体文档使用 update 或者 remove 命令即可。这样会在 vendor 目录下将必须的编译文件移入进来（注意：测试所需依赖并不包含，依赖项目的测试文件也不会包含）。\n$ ls github.com golang.org vendor.json $ cat vendor.json {\u0026#34;comment\u0026#34;: \u0026#34;\u0026#34;,\u0026#34;ignore\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;package\u0026#34;: [{\u0026#34;path\u0026#34;:\u0026#34;github.com/yeeuu/echoic\u0026#34;,\u0026#34;revision\u0026#34;:\u0026#34;a7d6994f92e2dc60cff071ae38b204fbd4bd2a3f\u0026#34;,\u0026#34;revisionTime\u0026#34;:\u0026#34;2015-12-18T11:14:29+08:00\u0026#34;}, {\u0026#34;path\u0026#34;: \u0026#34;golang.org/x/net/context\u0026#34;, \u0026#34;revision\u0026#34;: \u0026#34;1d9fd3b8333e891c0e7353e1adcfe8a612573033\u0026#34;, \u0026#34;revisionTime\u0026#34;: \u0026#34;2015-11-13T15:40:13-08:00\u0026#34; } ] } $ cd github.com/yeeuu/echoic $ ls LICENSE context.go group.go router.go README.md echoic.go response.go 通过设置环境变量 GO15VENDOREXPERIMENT=1 使用 vendor 文件夹构建文件。可以选择 export GO15VENDOREXPERIMENT=1 或者干脆 GO15VENDOREXPERIMENT=1 go build 执行编译。\n通过这种方式就可以保证程序能够实现类似 Python 中 Virtualenv 的模式，实现不同程序使用不同版本依赖的目的。\n","date":"2016-01-05T23:13:00Z","permalink":"https://www.4async.com/2016/01/2016-01-05-golang-vendor/","title":"使用 vendor 管理 Golang 项目依赖"},{"content":"在服务器端开发过程中，比较痛苦的是在多个人员进行开发时，容易因为环境不统一等等的情况，容易出现传说中的 “我这儿能跑，换个环境就出问题” 的情况。我们在项目开发过程中大量的使用了 Python 语言构建 Web 类型服务。Python 对于某些需要编译的扩展模块，可能在不同的场景下带来不同的影响（如编译不过等等、二进制版本不同等等问题）。\n在处理这种多人协作的环境统一问题时，我们选择 Vagrant＋VirtualBox＋CentOS 虚拟环境方案统一开发环境，避免出现切换开发者，切换开发环境带来的不统一的麻烦。\n初始化项目 vagrant 可以在 https://www.vagrantup.com/ 下载安装，对应的系统镜像可以在 http://www.vagrantbox.es/ 找到下载安装。根据国内实际的下载情况，我们建议先行下载镜像方式初始化项目。在选择镜像时，建议选择与线上服务器版本相同的镜像。下面我们仅演示本地加载镜像方式。\n下载 vagrant 和镜像后，安装 vagrant 软件。这样就可以在命令行中执行相关指令。还有，记得安装 VirtualBox。\nvagrant box add centos /path/to/centos-6.6-x86_64.box 比如我们线上服务器使用了 centos 环境，我们在开发时，也是选择相同版本的 CentOS box。\n接下来进入对应的项目目录，使用如下命令创建 Vagrantfile（centos 是刚刚添加的 box）。当然，如果项目已经存在 Vagrantfile，则可以进行省略。\nvagrant init centos 项目文件夹下会生成 Vagrantfile 文件。\nVagrantfile 编辑 Vagrant 在执行环境初始化时会根据 Vagrantfile 文件的描述进行。比较常见的编辑内容一般包含以下几种。\n如果想要使用 IP 访问该虚拟服务器，可以通过删除 config.vm.network :private_network, ip: \u0026quot;192.168.33.10\u0026quot; 前的 “#”，将该设置生效。注意需要确认 ip 不要被占用。\n如果需要同步文件夹到虚拟机中（比如项目文件夹），可以注释掉 config.vm.synced_folder \u0026quot;../data\u0026quot;, \u0026quot;/vagrant_data\u0026quot; 前的 “#”，将设置生效。该命令可以切换更复杂的设置：config.vm.synced_folder \u0026quot;./\u0026quot;, \u0026quot;/var/www\u0026quot;, create: true, group: \u0026quot;nginx\u0026quot;, owner: \u0026quot;nginx\u0026quot; 命令表示将本地的当前目录映射到 /var/www 目录，若不存在时创建，同意 user 和 group 设置为 nginx。\n服务器初始化 服务器初始化功能也算是在 Vagrantfile 编辑之中的，之所以单独介绍，是因为服务器的环境是初始化时一个更加独立的过程，应用也最多。在 vagrant 环境初始化时，可以通过 Vagrantfile 的 config.vm.provision \u0026quot;shell\u0026quot; 功能，初始化服务器配置和一些基础运行环境。比如说我们现在正在使用的配置如下（Vagrantfile 中对应内容）：\nconfig.vm.provision \u0026#34;shell\u0026#34;, inline: \u0026lt;\u0026lt;-SHELL mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo sudo yum install -y zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel sudo yum install -y mysql-server mysql-client mysql-devel sudo yum install -y make gcc gcc-c++ cd /tmp wget https://www.python.org/ftp/python/2.7.11/Python-2.7.11.tgz tar zxvf Python-2.7.11.tgz cd /tmp/Python-2.7.11 ./configure --prefix=/usr/local --with-ensurepip=upgrade make sudo make install sudo /usr/local/bin/pip install virtualenv virtualenv /mnt/pyenv sudo /usr/local/bin/virtualenv /mnt/pyenv source /mnt/pyenv/bin/activate sudo /mnt/pyenv/bin/pip install -r /vagrant/requirements.txt SHELL 该项配置使用了阿里云的 CentOS 源，编译 Python 2.7.11 并且安装应用必须的一些依赖。项目默认路径会安装在 / vagrant / 目录下，可以通过其他形式，安装至与线上环境相同的位置。\nVagrant 也支持使用 ansible-playbook 形式部署服务，如果你了解具体的使用方法，可以使用下面的形式指定 playbook。\nVagrant.configure(2) do |config| # # Run Ansible from the Vagrant Host # config.vm.provision \u0026#34;ansible\u0026#34; do |ansible| ansible.playbook = \u0026#34;playbook.yml\u0026#34; end end 当然，利用 Vagrant 也可以很方便的学习和测试 ansible-playbook。\n运行测试 在编辑 Vagrantfile 完成后，可以通过 vagrant up 命令，启动虚拟环境。此时如果设置了服务器初始化，会在最开始过程中执行大量的初始化操作。待操作完成后，则可以验证。\n打包分发 在编辑完成后，则可以根据当前版本生成 Box 文件。对其他开发者而言，可以通过 Box 文件直接快速生成开发环境，也避免了还需要进行服务器初始化阶段。\nvagrant package 输出会提示保存文件地址：\n==\u0026gt; default: Attempting graceful shutdown of VM... ==\u0026gt; default: Clearing any previously set forwarded ports... ==\u0026gt; default: Exporting VM... ==\u0026gt; default: Compressing package to: /path/to/package.box 如果进行了初始化环境的变更，我们建议同时更新 Vagrantfile 文件，通过重新打包的方式进行同步更新。\nVagrant 其他命令 在刚刚提到的几个命令中，vagrant up 可以用于开启环境，对应的还有一些其他命令：\nvagrant ssh\t// ssh vagrant halt\t// 关机 vagrant status\t// 查看状态 vagrant destory\t// 删除虚拟机，如果你需要重新开始 ... ","date":"2015-12-28T15:43:12Z","permalink":"https://www.4async.com/2015/12/2015-12-28-using-vagrant-development-env/","title":"使用 Vagrant 统一开发环境实践"},{"content":"今天在讨论 Python 的异步编程的时候提到的问题：到底为什么现在 Python 的异步数据库那么少呢？到底针对 Python 而言，什么是影响 Python 性能最大的门槛呢？\n为了搞清楚这个问题，我对线上环境应用使用了性能探针系统。性能出人意料 (CPython 2.7.10)：\n似乎真实来说，Python 本身的性能问题才是影响 Python 性能的真凶。\n在此以前，我们谈这个之前，还是先复习一下什么是 GIL 吧。无论是 CPython(普通的 Python 2.7.x 和 Python 3.5.x) 还是 pypy，都是有一个名曰『Global Interpreter Lock』的东西限制了它的性能，用一张图可以生动的表现带 GIL 的 Python 在多核下的 CPU 压力：\nGIL 限制了 Python 的 bytecode 只能在一个线程中运行，使用这种 GIL 避免多线程编程时隐含并发访问对象可能带来的潜在问题，但是也让 Python 在多核下性能表现感人。这种实现方式一直备受诟病，这也导致 Python 在某些高并发场景时会出现较严重的性能下滑问题。\n在 Python 性能提升上，比较常见的性能提升的方式，是将多线程转化为多进程方式，以充分利用 CPU 多核心。但是这样带来直接的问题是进程间通讯会严重影响整体程序的吞吐量。一个简单的 CPU 密集 Python socket 的通讯程序，非 Block 状态下，吞吐量能有大概 10 倍以上的差距。(参考 Python Concurrency From the Ground Up )\n不过相对来说，这样虽然影响了最大吞吐量，但是在利用多核效率上提升明显，即便是 CPU 密集的情况，也能保证程序的整体吞吐量不会出现线程模式的成百上千倍的性能差距。\n那么，这些与异步又有什么关系呢？\n对异步情况而言，对 Python 的代码执行并没有影响，我们所提到的异步，能够提升的性能，也仅仅限于针对针对于较长 IO 等待的问题，比如常见的就是请求远程 API 接口 (http query) 等等情况。对这种情况而言，当进入 Blocking 的 socket 等待时间时，将 CPU 时间释放，用于去做更有意义的其他操作是完全合理的。对于数据库操作而言，则也需要根据不同的情况进行对待。比如连接远程网络数据库，那么使用异步则根据实际的性能消耗时间而定，如果是本地的数据库，那么我想回到第一张图，相信根本无需使用异步库就可以解决这个问题。\n从一个 CPU 密集的例子，我们也能看出，其实，使用 JIT 带来的性能提升比较明显。然而，对异步库的异步队列仍是按顺序单个处理的，当队列较长时，同样会带来更严重的性能损耗：因为对那些快速的网络连接而言，Python GIL 则更加限制了 Python 的执行性能。\n","date":"2015-11-11T18:18:18Z","permalink":"https://www.4async.com/2015/11/2015-11-11-async-python/","title":"Python 异步与性能迷思"},{"content":"原文地址: PEP-0492\nPEP 492 标题 协程与 async/await 语法 作者 Yury Selivanov \u0026lt;yury at magic.io\u0026gt; 翻译 ipfans \u0026lt;ipfanscn at gmail.com\u0026gt; 状态 最终稿 Python 版本 3.5 翻译最后更新 2015-11-03 目录\n摘要 API 设计和实现的备注 基本原理和目标\n语法规范\n新协程声明语法 types.coroutine() Await 表达式 新的操作符优先级列表 await 表达式示例 异步上下文管理与 async with 新语法 例子 异步迭代器与 async for 新语法 例子 1 例子 2 为什么使用 StopAsyncIteration 协程对象 与生成器的不同之处 协程对象方法 调试特性 新的标准库函数 新的抽象基类 专用术语表\n函数与方法列表\n移植计划\n向后兼容性 asyncio asyncio 移植策略 CPython 代码中的 async/await 语法更新 失效计划 设计思路 (暂时不考虑翻译)\n性能\n总体影响 编译器修改 async/await 实现参考\n上层修改和新协议列表 可以工作的实例 参考\n致谢\n版权信息\n摘要 不断增长的网络连通性需求带动了对响应性、伸缩性代码的需求。这个 PEP 的目标在于回答如何更简单的、Pythinic 的实现显式的异步 / 并发的 Python 代码。\n我们把协程概念独立出来，并为其使用新的语法。最终目标是建立一个通用、易学的 Python 异步编程模型，并尽量与同步编程的风格保持一致。\n这个 PEP 假设异步任务被一个事件循环器（类似于标准库里的 asyncio.events.AbstractEventLoop）管理和调度。不过，我们并不会依赖某个事件循环器的具体实现方法，从本质上说只与此相关：使用 yield 作为给调度器的信号，表示协程将会挂起直到一个异步事件（如 IO）完成。\n我们相信这些改变将会使 Python 在这个异步编程快速增长的领域能够保持一定的竞争性，就像许多其它编程语言已经、将要进行的改变那样。\nAPI 设计和实现的备注 根据 Python 3.5 Beta 期间的反馈，我们进行了重新设计：明确的把协程从生成器里独立出来 \u0026mdash; 原生协程现在拥有了自己完整的独立类型，而不再是一种新的生成器类型。\n这个改变主要是为了解决在 Tornado Web 服务中里集成协程时出现的一些问题。\n基本原理和目标 现在版本的 Python 支持使用生成器实现协程功能 (PEP-342)，后面通过 PEP-380 引入了 yield from 语法进行了增强。但是这样仍有一些缺点：\n协程与常规的生成器在相同语法时用以混淆，尤其是对心开发者而言。 一个函数是否是协程需要通过是否主体代码中使用了 yield 或者 yield from 语句进行检测，这样在重构代码中添加、去除过程中容易出现不明显的错误 异步调用的支持被 yield 支持的语法先定了，导致我们无法使用更多的语法特性，比如 with 和 for 语句。 这个提议的目的是将协程作为原生 Python 语言特性，并且将他们与生成器明确的区分开。它避免了生成器 / 协程中间的混淆请困高，方便编写出不依赖于特定库的协程代码。这个也方便 linter 和 IDE 能够实现更好的进行静态代码分析和重构。\n原生协程和相关的新语法特性使得可以在异步框架下可以定义一个上下文管理器和迭代协议。在这个提议后续中，新的 async with 语法让 Python 程序在进入和离开运行上下文时实现异步调用，新的 async for 语法可以在迭代器中实现异步调用。\n语法规范 这个提议介绍了新的语法用于增强 Python 中的协程支持。\n这个语法规范假设你已经了解 Python 现有协程实现方法 (PEP-342 和 PEP-380)。这次语法改变的动机来自于 asyncio 框架 (PEP-3156) 和 Cofunctions 提议 (PEP-3152，现在此提议已被废弃)。\n从本文档中，我们使用 原生协程 代指新语法生命的函数，基于生成器的协程 用于表示那些基于生成器语法实现的协程。协程 则表示两个地方都可以使用的内容。\n新协程声明语法 下面的新语法用于声明原生协程：\nasync def read_data(db): pass 协程的主要属性包括：\nasync def 函数始终为协程，即使它不包含 await 表达式。 如果在 async 函数中使用 yield 或者 yield from 表达式会产生 SyntaxError 错误。 在内部，引入了两个新的代码对象标记： CO_COROUTINE 用于标记原生协程（和新语法一起定义） CO_ITERABLE_COROUTINE 用于标记基于生成器的协程，兼容原生协程。(通过 types.coroutine() 函数设置) 常规生成器在调用时会返回一个 genertor 对象，同理，协程在调用时会返回一个 coroutine 对象。 协程不再抛出 StopIteration 异常，而是替代为 RuntimeError。常规生成器实现类似的行为需要进行引入 __future__(PEP-3156) 当协程进行垃圾回收时，一个从未被 await 的协程会抛出 RuntimeWarning 异常。(参考 调试特性) 更多内容请参考 协程对象 一节。 types.coroutine() 在 types 模块中新添加了一个函数 coroutine(fn) 用于 asyncio 中基于生成器的协程与本 PEP 中引入的原生携协程互通。\n@types.coroutine def process_data(db): data = yield from read_data(db) ... 这个函数将生成器函数对象设置 CO_ITERABLE_COROUTINE 标记，将返回对象变为 coroutine 对象。\n如果 fn 不是一个生成器函数，那么它会对其进行封装。如果它返回一个生成器，那么它会封装一个 awaitable 代理对象 (参考下面 awaitable 对象的定义)。\n注意：CO_COROUTINE 标记不能通过 types.coroutine() 进行设置，这就可以将新语法定义的原生协程与基于生成器的协程进行区分。\ntypes 模块添加了一个新函数 coroutine(fn)，使用它，“生成器实现的协程” 和 “原生协程” 之间可以进行互操作。\nAwait 表达式 下面新的 await 表达式用于获取协程执行结果：\nasync def read_data(db): data = await db.fetch(\u0026#39;SELECT ...\u0026#39;) ... await 与 yield from 相似，挂起 read_data 协程的执行直到 db.fetch 这个 awaitable 对象完成并返回结果数据。\n它复用了 yield from 的实现，并且添加了额外的验证参数。await 只接受以下之一的 awaitable 对象：\n一个原生协程函数返回的原生协程对象。 一个使用 types.coroutine() 修饰器的函数返回的基于生成器的协程对象。 一个包含返回迭代器的 __await__ 方法的对象。\n任意一个 yield from 链都会以一个 yield 结束，这是 Future 实现的基本机制。因此，协程在内部中是一种特殊的生成器。每个 await 最终会被 await 调用链条上的某个 yield 语句挂起（参考 PEP-3156 中的进一步解释）。\n为了启用协程的这一特点，一个新的魔术方法 __await__ 被添加进来。在 asyncio 中，对于对象在 await 语句启用 Future 对象只需要添加 __await__ = __iter__ 这行到 asyncio.Future 类中。\n在本 PEP 中，带有 __await__ 方法的对象也叫做 Future-like 对象。 同样的，请注意到 __aiter__ 方法（下面会定义）不能用于这种目的。它是不同的协议，有点类似于用 __iter__ 替代普通调用方法的 __call___。\n如果 __await__ 返回非迭代器类型数据，会产生一个 TypeError. CPython C API 中使用 tp_as_async.am_await 定义的函数，并且返回一个迭代器（类似 __await__ 方法）。 新的操作符优先级列表 关键词 await 与 yield 和 yield form 操作符的区别是 await 表达式大部分情况下不需要括号包裹。\n同样的，yield from 允许允许任意表达式做其参数，包含表达式如 yield a()+b()，这样通常处理作为 yield from (a()+b())，这个通常会造成 Bug。通常情况下任意算数操作的结果都不会是 awaitable 对象。为了避免这种情况，我们将 await 的优先级调整为低于 [], () 和.，但是高于 ** 操作符。\n操作符 描述 yield x , yield from x Yield 表达式 lambda Lambda 表达式 if \u0026ndash; else 条件表达式 or 布尔或 and 布尔与 not x 布尔非 in , not in , is , is not , \u0026lt;, \u0026lt;= ,\u0026gt; , \u0026gt;= , != , == 比较，包含成员测试和类型测试 | 字节或 ^ 字节异或 \u0026amp; 字节与 \u0026laquo;,\u0026raquo; 移位 + , - 加和减 * , @ , / , // , % 乘，矩阵乘法，除，取余 +x , -x , ~x 正数, 复数, 取反 ** 平方 await x Await 表达式 x[index] , x[index:index] , x(arguments\u0026hellip;) , x.attribute 子集，切片，调用，属性 (expressions\u0026hellip;) , [expressions\u0026hellip;] , {key: value\u0026hellip;} , {expressions\u0026hellip;} 类型显示 await 表达式示例 有效的语法例子:\n表达式 会被处理为 if await fut: pass if (await fut): pass if await fut + 1: pass if (await fut) + 1: pass pair = await fut, \u0026lsquo;spam\u0026rsquo; pair = (await fut), \u0026lsquo;spam\u0026rsquo; with await fut, open(): pass with (await fut), open(): pass await foo()[\u0026lsquo;spam\u0026rsquo;].baz()() await (foo()[\u0026lsquo;spam\u0026rsquo;].baz()() ) return await coro() return ( await coro() ) res = await coro() ** 2 res = (await coro()) ** 2 func(a1=await coro(), a2=0) func(a1=(await coro()), a2=0) await foo() + await bar() (await foo()) + (await bar()) -await foo() -(await foo()) 错误的语法例子:\n表达式 应写作 await await coro() await (await coro()) await -coro() await (-coro()) 异步上下文管理与 async with 一个异步上下文管理器是用于在 enter 和 exit 方法中管理暂停执行的上下文管理器。\n为此，我们设置了新的异步上下文管理器。添加了两个魔术方法： __aenter__ 和 __aexit__。这两个方法都返回 awaitable 对象。\n异步上下文管理器例子如下：\nclass AsyncContextManager: async def __aenter__(self): await log(\u0026#39;entering context\u0026#39;) async def __aexit__(self, exc_type, exc, tb): await log(\u0026#39;exiting context\u0026#39;) 新语法 一个新的异步上下文管理语法被接受：\nasync with EXPR as VAR: BLOCK 语义上等同于：\nmgr = (EXPR) aexit = type(mgr).__aexit__ aenter = type(mgr).__aenter__(mgr) exc = True VAR = await aenter try: BLOCK except: if not await aexit(mgr, *sys.exc_info()): raise else: await aexit(mgr, None, None, None) 和普通的 with 语句一样，可以在单个 async with 语句里指定多个上下文管理器。\n在使用 async with 时，如果上下文管理器没有 __aenter__ 和 __aexit__ 方法，则会引发错误。在 async def 函数之外使用 async with 则会引发 SyntaxError 异常。\n例子 通过异步上下文管理器更容易实现协程对数据库事务的正确管理：\nasync def commit(session, data): ... async with session.transaction(): ... await session.update(data) ... 代码看起来也更加简单：\nasync with lock: ... 而不是\nwith (yield from lock): ... 异步迭代器与 async for 一个异步迭代器能够在它的迭代实现里调用异步代码，也可以在它的 __next__ 方法里调用异步代码。为了支持异步迭代，需要：\n一个对象必须实现 __aiter__ 方法（或者，使用 CPython C API 的 tp_as_async.am_aiter 定义），返回一个异步迭代器对象中的 ```awaitable```` 结果。 一个异步迭代器必须实现 __anext__ 方法（或者，使用 CPython C API 的 tp_as_async.am_anext 定义）返回一个 awaitable。 停止迭代器的 __anext__ 必须抛出一个 StopAsyncIteration 异常。 一个异步迭代的例子：\nclass AsyncIterable: async def __aiter__(self): return self async def __anext__(self): data = await self.fetch_data() if data: return data else: raise StopAsyncIteration async def fetch_data(self): ... 新语法 一种新的异步迭代方案被采纳：\nasync for TARGET in ITER: BLOCK else: BLOCK2 语义上等同于：\niter = (ITER) iter = await type(iter).__aiter__(iter) running = True while running: try: TARGET = await type(iter).__anext__(iter) except StopAsyncIteration: running = False else: BLOCK else: BLOCK2 如果对一个普通的不含有 __aiter__ 方法的迭代器使用 async for，会引发 TypeError 异常。如果在 async def 函数外使用 async for 会已发 SyntaxError 异常。\n和普通的 for 语法一样，async for 有可选的 else 分支。\n例子 1 通过异步迭代器，就可以实现通过迭代实现异步缓冲数据：\nasync for data in cursor: ... 当 cursor 是一个异步迭代器时，就可以在 N 次迭代后从数据库中预取 N 行数据。\n下面的代码演示了新的异步迭代协议：\nclass Cursor: def __init__(self): self.buffer = collections.deque() async def _prefetch(self): ... async def __aiter__(self): return self async def __anext__(self): if not self.buffer: self.buffer = await self._prefetch() if not self.buffer: raise StopAsyncIteration return self.buffer.popleft() 那么这个 Cursor 类可以按照下面的方式使用：\nasync for row in Cursor(): print(row) 这个等同于下面的代码：\ni = await Cursor().__aiter__() while True: try: row = await i.__anext__() except StopAsyncIteration: break else: print(row) 例子 2 下面的工具类用于将普通的迭代转换为异步。这个并没有什么实际的作用，这个代码只是用于演示普通迭代与异步迭代之间的关系。\nclass AsyncIteratorWrapper: def __init__(self, obj): self._it = iter(obj) async def __aiter__(self): return self async def __anext__(self): try: value = next(self._it) except StopIteration: raise StopAsyncIteration return value async for letter in AsyncIteratorWrapper(\u0026#34;abc\u0026#34;): print(letter) 为什么使用 StopAsyncIteration 协程在内部实现中依旧是依赖于迭代器的。因此，在 PEP-479 生效之前，下面两者并没有区别：\ndef g1(): yield from fut return \u0026#39;spam\u0026#39; and def g2(): yield from fut raise StopIteration(\u0026#39;spam\u0026#39;) 但是在 PEP 479 接受并且默认对协程开启时，下面的例子中的 StopIteration 会被封装成 RuntimeError。\nasync def a1(): await fut raise StopIteration(\u0026#39;spam\u0026#39;) 所以，想通知外部代码迭代已经结束，抛出一个 StopIteration 异常的是不行的。因此，一个新的内置异常类 StopAsyncIteration 被引入进来了。\n另外，根据 PEP 479，所有协程中抛出的 StopIteration 异常都会被封装成 RuntimeError。\n协程对象 与生成器的不同之处 这节进适用于 CO_COROUTINE 标记的原生协程，即，使用 async def 语法定义的对象。\n现有的 asyncio 库中的 * 基于生成器的协程 * 的行为未做变更。\n为了将协程与生成器区别开来，定义了下面的概念：\n原生协程对象不实现 __iter__ 和 __next__ 方法。因此，他们不能够通过 iter()，list()，tuple() 和其他一些内置函数进行迭代。他们也不能用于 for...in 循环。\n在原生协程中尝试使用 __iter__ 或者 __next 会触发 TypeError 异常。 未被装饰的生成器不能够 yield from 一个原生协程：这样会引发 TypeError。 基于生成器的协程 (asyncio 代码必须使用 @asyncio.coroutine) 可以 yield from 一个原生协程。 对原生协程对象和原生协程函数调用 inspect.isgenerator() 和 inspect.isgeneratorfunction() 会返回 False。 协程对象方法 协程内部基于生成器，因此他们同享实现过程。类似于生成器对象，协程包含 throw()，send() 和 close() 方法。StopIteration 和 GeneratorExit 在协程中扮演者同样的角色（尽管 PEP 479 默认对协程开启了）。参考 PEP-342, PEP-380 和 Python 文档了解更多细节。\n协程的 throw() 和 send() 方法可以用于将返回值和抛出异常推送到类似于 Future 的对象中。\n调试特性 一个初学者普遍会犯的错误是忘记在协程中使用 yield from。\n@asyncio.coroutine def useful(): asyncio.sleep(1) # this will do noting without \u0026#39;yield from\u0026#39; 为了调试这类错误，asycio 提供了一种特殊的调试模式：装饰器 @coroutine 封装所有的函数成一个特殊对象，这个对象的析构函数中记录警告。当封装的生成器垃圾回收时，会产生详细的记录信息，包括具体定义修饰函数、回收时的栈信息等等。封装对象同样提供一个 __repr__ 函数用于输出关于生成器的详细信息。\n唯一的问题是如何启用这些调试功能。这些调试工具在生产模式中什么都不做，@coroutine 修饰符在系统变量 PYTHONASYNCIODEBUG 设置后才会提供调试功能。这种方式可以让 asyncio 程序使用 asyncio 自己的函数分析。EventLoop.set_debug 是另外一个调试工具，他不会影响 @coroutine 修饰符行为。\n根据本提议，协程是原生的与生成器不同的概念。当抛出 RuntimeWarning 异常的协程是从来没有被 awaited 过的。因此添加了两条新的函数到 sys 模块：set_coroutine_wrapper 和 get_coroutine_wrapper。这个用于开启 asyncio 或者其他框架中的高级调试 (比如显示协程创建的位置和垃圾回收时的栈信息)。\n新的标准库函数 types.coroutine(gen)。参考 types.coroutine() 节中的内容。 inspect.iscoroutine(obj) 当 obj 是原生协程时返回 True。 inspect.iscoroutinefunction(obj) 当 obj 是原生协程函数时返回为 True。 inspect.isawaitable(obj) 当 obj 是 awaitable 时返回为 True。 inspect.getcoroutinestate(coro) 返回原生协程对象的当前状态（是 inspect.getfgeneratorstate(gen) 的镜像）。 inspect.getcoroutinelocals(coro) 返回原生协程对象的局部变量的映射（是 inspect.getgeneratorlocals(gen) 的镜像）。 sys.set_coroutine_wrapper(wrapper) 允许拦截原生协程对象的创建。wrapper 必须是一个接受一个参数 callable（一个协程对象），或者是 None。None 会重置 wrapper。当调用第二次时，新的 wrapper 会替代之前的封装。这个函数是线程专有的。参考 调度调试 了解更多细节。 sys.get_coroutine_wrapper() 返回当前的封装对象。如果封装未设置会返回 None。这个函数是线程专有的。参考 调度调试 了解更多细节。 新的抽象基类 为了允许更好的与现有的框架（比如 Tornado）和编译器（比如 Cython）整合，我们添加了两个新的抽象基类 (ABC)：\ncollections.abc.Awaitable 是 Future-like 类的抽象基类，它实现了 __await__ 方法。\ncollections.abc.Coroutine 是协程对象的抽象基类，它实现了 send(value)，throw(type, exc, tb)，close() 和 __await__() 方法。\n值得注意的是，带有 CO_ITERABLE_COROUTINE 标记的基于生成器的协程并没有实现 __await__ 方法，因此他不是 collections.abc.Coroutine 和 collections.abc.Awaitable 抽象类的实例：\n@types.coroutine def gencoro(): yield assert not isinstance(gencoro(), collections.abc.Coroutine) # 然而: assert inspect.isawaitable(gencoro()) 为了方便对异步迭代的调试，添加了另外两个抽象基类：\ncollections.abc.AsyncIterable \u0026ndash; 用于测试 __aiter__ 方法 collections.abc.AsyncIterator \u0026ndash; 用于测试 __aiter__ 和 __anext__ 方法。 专用术语表 函数与方法列表 移植计划 向后兼容性 asyncio asyncio 移植策略 CPython 代码中的 async/await 语法更新 失效计划 性能 总体影响 这个提议并不会造成性能影响。这是 Python 官方性能测试结果：\npython perf.py -r -b default ../cpython/python.exe ../cpython-aw/python.exe [skipped] Report on Darwin ysmac 14.3.0 Darwin Kernel Version 14.3.0: Mon Mar 23 11:59:05 PDT 2015; root:xnu-2782.20.48~5/RELEASE_X86_64 x86_64 i386 Total CPU cores: 8 ### etree_iterparse ### Min: 0.365359 -\u0026gt; 0.349168: 1.05x faster Avg: 0.396924 -\u0026gt; 0.379735: 1.05x faster Significant (t=9.71) Stddev: 0.01225 -\u0026gt; 0.01277: 1.0423x larger The following not significant results are hidden, use -v to show them: django_v2, 2to3, etree_generate, etree_parse, etree_process, fastpickle, fastunpickle, json_dump_v2, json_load, nbody, regex_v8, tornado_http. 编译器修改 修改后的编译器处理 Python 文件没有明显的性能下降：处理 12MB 大小的文件（Lib/test/test_binop.py 重复 1000 次）消耗时间相同。\nasync/await 下面的小测试用于检测『async』函数和生成器的性能差异：\nimport sys import time def binary(n): if n \u0026lt;= 0: return 1 l = yield from binary(n - 1) r = yield from binary(n - 1) return l + 1 + r async def abinary(n): if n \u0026lt;= 0: return 1 l = await abinary(n - 1) r = await abinary(n - 1) return l + 1 + r def timeit(func, depth, repeat): t0 = time.time() for _ in range(repeat): o = func(depth) try: while True: o.send(None) except StopIteration: pass t1 = time.time() print(\u0026#39;{}({}) * {}: total {:.3f}s\u0026#39;.format(func.__name__, depth, repeat, t1-t0)) 结果显示并没有明显的性能差异：\nbinary(19) * 30: total 53.321s abinary(19) * 30: total 55.073s binary(19) * 30: total 53.361s abinary(19) * 30: total 51.360s binary(19) * 30: total 49.438s abinary(19) * 30: total 51.047s 注意：19 层意味着 1,048,575 调用。\n实现参考 实现参考可以在 这里 找到。\n上层修改和新协议列表 新的协程定义语法：async def 和新的 await 关键字。 Future-like 对象提供新的 __await__ 方法和新的 PyTypeObject 的 tp_as_async.am_await。 新的异步上下文管理器语法： async with，协议提供了 __aenter__ 和 __aexit__ 方法。 新的异步迭代语法：async for，协议提供了 __aiter、__aexit 和新的内置异常 StopAsyncIteration。PyTypeObject 提供了新的 tp_as_async.am_aiter 和 tp_as_async.am_anext。 新的 AST 节点：AsyncFunctionDef，AsyncFor，AsyncWith 和 Await。 新函数 sys.set_coroutine_wrapper(callback)，sys.get_coroutine_wrapper()，types.coroutine(gen)，inspect.iscoroutinefunction(func)，inspect.iscoroutine(obj)，inspect.isawaitable(obj)，inspect.getcoroutinestate(coro) 和 inspect.getcoroutinelocals(coro)。 新的代码对象标记 CO_COROUTINE 和 CO_ITERABLE_COROUTINE。 新的抽象基类 collections.abc.Awaitable，collections.abc.Coroutine，collections.abc.AsyncIterable 和 collections.abc.AsyncIterator。 C API 变更：新的 PyCoro_Type（将 Python 作为 types.CoroutineType 输出）和 PyCoroObject。PyCoro_CheckExact(*o) 用于检测 o 是否为原生协程。 虽然变化和新内容列表并不短，但是重要的是理解：大部分用户不会直接使用这些特性。他的目的是在于框架和库能够使用这些为用户提供便捷的使用和明确的 API 用于 async def，await，async for 和 async with 语法。\n可以工作的实例 本 PEP 提出的所有概念都 已经实现，并且可以被测试。\nimport asyncio async def echo_server(): print(\u0026#39;Serving on localhost:8000\u0026#39;) await asyncio.start_server(handle_connection,\u0026#39;localhost\u0026#39;, 8000) async def handle_connection(reader, writer): print(\u0026#39;New connection...\u0026#39;) while True: data = await reader.read(8192) if not data: break print(\u0026#39;Sending {:.10}... back\u0026#39;.format(repr(data))) writer.write(data) loop = asyncio.get_event_loop() loop.run_until_complete(echo_server()) try: loop.run_forever() finally: loop.close() ","date":"2015-10-31T12:48:15Z","permalink":"https://www.4async.com/2015/10/2015-10-31-coroutines-with-async-and-await-syntax-chinese/","title":"PEP 0492 Coroutines with async and await syntax 中文翻译"},{"content":"从 Call 到命令端 在第一个文章中，我们介绍了实现一个 Call 的客户端基本模型，但只是 Call 怎么能满足需求呢？比如在 redis-py 中，一个完整的客户端应该是这样的：\nclient = redis.StrictRedis() client.setex(\u0026#34;key\u0026#34;, 10, \u0026#34;value\u0026#34;) 接下来作为一个程序的客户端，需要去做的就是封装出一个 Redis Client。比如 setex 方法：\ndef setex(self, key, seconds, value): \u0026#34;\u0026#34;\u0026#34;Set the value and expiration of a key. :raises TypeError: if seconds is neither int \u0026#34;\u0026#34;\u0026#34;if not isinstance(seconds, int): raise TypeError(\u0026#34;milliseconds argument must be int\u0026#34;) fut = self._conn.execute(b\u0026#39;SETEX\u0026#39;, key, seconds, value) return wait_ok(fut) 剩下的就是一个个方法逐个完善。\n什么是连接池 我们会看到，无论那个数据库客户端，总是会有连接池机制。那么连接池是什么呢？我们为什么需要连接池呢？\n首先，我们都知道，对连接而言，创建是必要重型的操作。比如说，TCP 连接，接下来之后是登录认证等等过程，最后才会执行命令。这也就是我们通常计算库性能时，很多时候会把建立连接的时候去掉。但是这就出现了一个问题，当一个连接被占用时，其他的操作仍旧是不能够完成操作了，只能等待前一个操作完成。但是假如我们一次性创建一堆连接呢？从一堆连接中找到空闲的连接，使用完成后释放成空闲的状态，这就是线程池的本质。因为减少了每次创建连接的过程，所以对性能提升也非常有帮助。\n从单连接到连接池 首先，还是创建一个 RedisPool 类，用于管理 Redis 的连接池。\nclass RedisPool: \u0026#34;\u0026#34;\u0026#34;Redis connections pool.\u0026#34;\u0026#34;\u0026#34; def __init__(self, address, db=0, password=None, encoding=None, *, minsize, maxsize, commands_factory, loop=None): if loop is None: loop = asyncio.get_event_loop() self._address = address self._db = db self._password = password self._encoding = encoding self._minsize = minsize self._factory = commands_factory self._loop = loop # 连接池数组 self._pool = collections.deque(maxlen=maxsize) self._used = set() self._acquiring = 0 self._cond = asyncio.Condition(loop=loop) def _create_new_connection(self): return create_redis(self._address, db=self._db, password=self._password, encoding=self._encoding, commands_factory=self._factory, loop=self._loop) 接下来，就需要创建大量的连接了：\nasync def create_pool(self, *, override_min): # todo: drop closed connections first # 判断是否达到了连接池数量限制 while not self._pool and self.size \u0026lt; self.maxsize: self._acquiring += 1 try: conn = await self._create_new_connection() self._pool.append(conn) finally: self._acquiring -= 1 # connection may be closed at yeild point self._drop_closed() 那么怎么从这些连接中抽取连接并且进行连接呢:\n@asyncio.coroutine def acquire(self): \u0026#34;\u0026#34;\u0026#34;Acquires a connection from free pool. Creates new connection if needed. \u0026#34;\u0026#34;\u0026#34; with await self._cond: while True: await self._fill_free(override_min=True) if self.freesize: conn = self._pool.popleft() assert not conn.closed, conn assert conn not in self._used, (conn, self._used) self._used.add(conn) return conn else: await self._cond.wait() 接下来就是使用完成后进行释放即可：\ndef release(self, conn): \u0026#34;\u0026#34;\u0026#34;Returns used connection back into pool. When returned connection has db index that differs from one in pool the connection will be closed and dropped. When queue of free connections is full the connection will be dropped. \u0026#34;\u0026#34;\u0026#34;assert conn in self._used,\u0026#34;Invalid connection, maybe from other pool\u0026#34;self._used.remove(conn) if not conn.closed: if conn.in_transaction: logger.warning(\u0026#34;Connection %r in transaction, closing it.\u0026#34;, conn) conn.close() elif conn.db == self.db: if self.maxsize and self.freesize \u0026lt; self.maxsize: self._pool.append(conn) else: # consider this connection as old and close it. conn.close() else: conn.close() # FIXME: check event loop is not closed asyncio.async(self._wakeup(), loop=self._loop) 至此，你已经可以实现一个基本的 Redis 客户端了，还在犹豫什么？快自己动手吧！\n注: 文中 Redis 库参考了 aio-lib/aioredis 库。\n","date":"2015-10-30T23:11:15Z","permalink":"https://www.4async.com/2015/10/2015-10-30-write-aio-python-redis-client-as-dummy-2/","title":"从零实现一个 Redis 客户端（二）"},{"content":"什么是 AIO AIO 是 Asynchronous Input/Output 的简写，也就是异步 IO。不过在谈什么是 AIO 之前，我们可能要先介绍一下 BIO。那么什么是 BIO 呢？简单的说，BIO 是 Blocking Input/Output，也就是阻塞 IO，他实现的通常是在线程池中找出一个线程处理 IO，在 IO 过程中，其他线程都需要等待 IO 完成后才可以从中选取一个线程占用 IO。这样最大的问题是，当线程数量较多，并且需要大量的 IO 操作时，就会造成一个大量的阻塞，因为实际上每次只有一个线程在处理 IO。\n那么如何解决这个时候的问题呢？这时候就提出了 AIO 的概念。通常在 IO 处理过程中也会伴有一些其他的处理操作，假如把所有的操作都浪费在了等待 IO 释放上，线程池中的线程利用率也太低了，因此我们需要一种方式，在申请 IO 处理之后，就去继续做其他的事情，等 IO 操作完成了，然后通知我们已经 OK，我们可以继续处理了。这也就是我们常说的 AIO 的原型。\nAIO 的情况也说明了它适用的场景：长连接场景，或者重度的 IO 操作等等的情况。\n如果找软件来做案例，我们可以找一个可能大家熟知的：NGINX。正如我们所知，NGINX 采用了 异步、事件驱动的方法来处理连接。这种处理方式无需（像使用传统架构的服务器一样）为每个请求创建额外的专用进程或者线程，而是在一个工作进程中处理多个连接和请求。为此，NGINX 工作在非阻塞的 socket 模式下，并使用了 epoll 和 kqueue 这样有效的方法。\n这部分的内容，在 NGINX 引入线程池 性能提升 9 倍 中进行了详细的介绍，包含了 NGINX 的异步应用经验，同时介绍了 NGINX 中引入了阻塞的线程池用于解决某些特定场景问题下的效率。\n如何实现 Python 的异步 IO 这篇文章会以最新的 Python 3.5 为基础来介绍实现一个异步的 Python Redis Client。不过在此之前，我们先来看一下，怎么实现 Python 的 aio。\nPython 的 aio 官方封装了一个比较合适的基础库 asyncio。\n从一个例子开始简单认识一下如何实现一个异步的 aio client。这里以官方文档中的例子为例：\nimport asyncio async def tcp_echo_client(message, loop): reader, writer = await asyncio.open_connection(\u0026#39;127.0.0.1\u0026#39;, 8888, loop=loop) print(\u0026#39;Send: %r\u0026#39; % message) writer.write(message.encode()) data = await reader.read(100) print(\u0026#39;Received: %r\u0026#39; % data.decode()) print(\u0026#39;Close the socket\u0026#39;) writer.close() message = \u0026#39;Hello World!\u0026#39; loop = asyncio.get_event_loop() loop.run_until_complete(tcp_echo_client(message, loop)) loop.close() 这里面用到的 Python 3.5 中引入的 async/await 关键字，还有 asyncio 库。这里面 asyncio.open_connection 会返回一个 coroutine，这个可以使用 await 进行一个 aio 的调用，即，在收到返回信号之前，程序可以继续去处理其他的任务。这里面真正核心的就是 EventLoop，它负责监视发送这些信号，并且返回数据，它可以通过 asyncio.get_event_loop 获取到。然后他会真正返回的数据是一个读取 StreamReader 和写入 StreamWriter 的对象。\n接下来，就可以通过这个 reader 和 writer 进行数据的读取和写入。writer 是可以直接写入的，如果是 reader 的话，就需要 aio 的方式等待受到数据后返回。这样看起来更接近于普通的 socket 编程。不过关闭连接时，仅仅需要关闭 writer 就足够了。\n从 socket 通讯到 redis 通讯 本质上来说，所有的网络请求都可以看成是 SocketIO 的请求，因此，我们可以把 Redis 的请求当做是一个 socket 的通讯来进行，这样就很方便了。\n不过先等一等，那么通讯的数据格式怎么办？没关系，这里我们使用 hiredis-py 来解决协议解析的问题。不过，从库设计的角度来说，我们需要封装一个 RedisConnection 的类出来解决 Redis 的通讯协议。它可能传入的参数包含，一个 StreamReader、一个 StreamWriter，一个 EventLoop，哦，别忘记还有编码 encoding。其他的我们就用一个 * 来表示好了。\nclass RedisConnection(object): \u0026#39;\u0026#39;\u0026#39;Redis Connection\u0026#39;\u0026#39;\u0026#39; def __init__(self, reader, writer, *, encoding=None, loop=None): if loop is None: loop = asyncio.get_event_loop() self._reader = reader self._writer = writer self._encoding = encoding self._loop = loop self._db = 0 def __repr__(self): return \u0026#39;\u0026lt;RedisConnection [db:{}]\u0026gt;\u0026#39;.format(self._db) 记得加上 __repr__ 用来描述这个对象，这个可是一个好习惯。接下来就需要完善这个类了，比如，我们需要添加一个关闭连接的方法，这需要至少一个参数用于标记连接是否关闭，一个用于执行关闭操作，比如我们需要这样子的：\ndef close(self): \u0026#34;\u0026#34;\u0026#34;Close connection.\u0026#34;\u0026#34;\u0026#34; self._do_close(None) def _do_close(self, exc): if self._closed: return self._closed = True self._closing = False # 关闭写入 self._writer.transport.close() # 取消读取任务 self._reader_task.cancel() self._reader_task = None self._writer = None self._reader = None @property def closed(self): \u0026#34;\u0026#34;\u0026#34;True if connection is closed.\u0026#34;\u0026#34;\u0026#34; closed = self._closing or self._closed if not closed and self._reader and self._reader.at_eof(): self._closing = closed = True self._loop.call_soon(self._do_close, None) return closed 连接这类的方法已经处理完了，接下来就应该是执行 Redis 命令了，我们可以叫它 execute。那他需要几个东西，一个是执行的指令 command，一个是指令参数 *args，还有一些其他的，比如编码 encoding。这里为了节省时间，只是考虑一些 Set 和 Get 的基本操作。哦，不过等等，那么 Redis 的数据结构是什么样子的呢？我们还需要先把它编译成 Redis-server 可以识别的形式，那么需要一个 encode_command 方法。\n_converters = { bytes: lambda val: val, bytearray: lambda val: val, str: lambda val: val.encode(\u0026#39;utf-8\u0026#39;), int: lambda val: str(val).encode(\u0026#39;utf-8\u0026#39;), float: lambda val: str(val).encode(\u0026#39;utf-8\u0026#39;), } def encode_command(*args): \u0026#34;\u0026#34;\u0026#34;Encodes arguments into redis bulk-strings array. Raises TypeError if any of args not of bytes, str, int or float type. \u0026#34;\u0026#34;\u0026#34;buf = bytearray() def add(data): return buf.extend(data + b\u0026#39;\\r\\n\u0026#39;) add(b\u0026#39;*\u0026#39;+ _bytes_len(args)) for arg in args: if type(arg) in _converters: barg = _converters[type(arg)](arg) add(b\u0026#39;$\u0026#39;+ _bytes_len(barg)) add(barg) else: raise TypeError(\u0026#34;Argument {!r} expected to be of bytes,\u0026#34;\u0026#34;str, int or float type\u0026#34;.format(arg)) return buf 这样可以转化为可以识别的形式了，接下来还有一个问题，那么怎么让程序可以等待信号的生效呢？这里介绍一下 asyncio.Future。这个 asyncio.Future 类是用于封装回调函数的类，包含了一些更加方便使用的方法。通过这个类，可以实现 aio 的通知机制，也就是回调。这个类实例可以通过 await 返回我们需要的结果。不过这样就还需要在项目中添加一些更多的变量，比如所有等待返回的 self._waiters。\ndef execute(self, command, *args, encoding=None): \u0026#34;\u0026#34;\u0026#34;Executes redis command and returns Future waiting for the answer. Raises: * TypeError if any of args can not be encoded as bytes. * ReplyError on redis \u0026#39;-ERR\u0026#39; resonses. * ProtocolError when response can not be decoded meaning connection is broken. \u0026#34;\u0026#34;\u0026#34;assert self._reader and not self._reader.at_eof(), (\u0026#34;Connection closed or corrupted\u0026#34;) if command is None: raise TypeError(\u0026#34;command must not be None\u0026#34;) if None in set(args): raise TypeError(\u0026#34;args must not contain None\u0026#34;) # 这样小写也没有问题了 command = command.upper().strip() if encoding is None: encoding = self._encoding fut = asyncio.Future(loop=self._loop) self._writer.write(encode_command(command, *args)) self._waiters.append((fut, encoding, cb)) return fut 现在所有的命令都已经发送到了 redis-server，接下来就需要读取对应的结果了。\nasync def _read_data(self): \u0026#34;\u0026#34;\u0026#34;Response reader task.\u0026#34;\u0026#34;\u0026#34; while not self._reader.at_eof(): try: data = await self._reader.read(65536) except asyncio.CancelledError: break except Exception as exc: # XXX: for QUIT command connection error can be received # before response logger.error(\u0026#34;Exception on data read %r\u0026#34;, exc, exc_info=True) break self._parser.feed(data) while True: try: obj = self._parser.gets() except ProtocolError as exc: # ProtocolError is fatal # so connection must be closed self._closing = True self._loop.call_soon(self._do_close, exc) if self._in_transaction: self._transaction_error = exc return else: if obj is False: break else: self._process_data(obj) self._closing = True self._loop.call_soon(self._do_close, None) def _process_data(self, obj): \u0026#34;\u0026#34;\u0026#34;Processes command results.\u0026#34;\u0026#34;\u0026#34; waiter, encoding, cb = self._waiters.popleft() if waiter.done(): logger.debug(\u0026#34;Waiter future is already done %r\u0026#34;, waiter) assert waiter.cancelled(), (\u0026#34;waiting future is in wrong state\u0026#34;, waiter, obj) return if isinstance(obj, RedisError): waiter.set_exception(obj) if self._in_transaction: self._transaction_error = obj else: if encoding is not None: try: obj = decode(obj, encoding) except Exception as exc: waiter.set_exception(exc) return waiter.set_result(obj) if cb is not None: cb(obj) 有了这些之后，我们就可以简单创建一个连接了：\nasync def create_connection(address, *, db=None, password=None, encoding=None, loop=None): \u0026#34;\u0026#34;\u0026#34;Creates redis connection. Opens connection to Redis server specified by address argument. Address argument is similar to socket address argument, ie: * when address is a tuple it represents (host, port) pair; * when address is a str it represents unix domain socket path. (no other address formats supported) Encoding argument can be used to decode byte-replies to strings. By default no decoding is done. Return value is RedisConnection instance. This function is a coroutine. \u0026#34;\u0026#34;\u0026#34;assert isinstance(address, (tuple, list, str)), \u0026#34;tuple or str expected\u0026#34; if isinstance(address, (list, tuple)): host, port = address reader, writer = await asyncio.open_connection(host, port, loop=loop) else: reader, writer = await asyncio.open_unix_connection(address, loop=loop) conn = RedisConnection(reader, writer, encoding=encoding, loop=loop) try: if password is not None: yield from conn.auth(password) if db is not None: yield from conn.select(db) except Exception: conn.close() return conn 这样，连接部分的代码基本上已经处理完成了，接下来要做的就是实现基于这个连接的命令执行了，下面的内容会下一个文章中继续介绍，敬请期待。\n","date":"2015-10-10T18:11:15Z","permalink":"https://www.4async.com/2015/10/2015-10-10-write-aio-python-redis-client-as-dummy-1/","title":"零基础编写 Python Redis Client（一）"},{"content":"在新版 Python3.5 中，引入了两个新关键字 async 和 await，用于解决在 Python 异步编程中无法有效 区分 yield 生成器与异步的关系的问题。\n异步是一个什么东西 异步的作用在于，对于 Python 这种拥有 GIL 的语言，某个线程在处理单个耗时较长的任务时（如 I/O 读取，RESTful API 调用）等操作时，不能有效的释放 CPU 资源，导致其他线程的等待时间随之增加。 异步的作用是，在等待这种花费大量时间的操作数，允许释放 CPU 资源执行其他的线程任务，从而提 高程序的执行效率。\n3.4 之前如何实现异步 在 3.5 版本以前的程序中，Python 程序通常是使用 yield 作为一个判断是否进入异步操作的关键词。 比如在 3.4.x 版本中，我们可以用这样的一个例子来看一下 (或者你也可以用一个 Tornado 的例子，这 样你的程序就也可以运行在 2.7.x 版本的 Python 中了)：\nimport time import asyncio @asyncio.coroutine def slow_operation(n): yield from asyncio.sleep(1) print(\u0026#34;Slow operation {} complete\u0026#34;.format(n)) @asyncio.coroutine def main(): start = time.time() yield from asyncio.wait([slow_operation(1), slow_operation(2), slow_operation(3), ]) end = time.time() print(\u0026#39;Complete in {} second(s)\u0026#39;.format(end-start)) loop = asyncio.get_event_loop() loop.run_until_complete(main()) 执行结果如下：\n➜ Desktop pyenv shell 3.4.3 ➜ Desktop python 3.4_asyncio.py ➜ Desktop python 3.4_asyncio.py Slow operation 2 complete Slow operation 1 complete Slow operation 3 complete Complete in 1.0008249282836914 second(s) 如果你了解过 yield，你会知道 yield 其实作用是用来生成一个生成器，而生成器的作用是用于输出一系列的结果。比如计算斐波那契数列：\ndef fab(max): n, a, b = 0, 0, 1 while n \u0026lt; max: yield b a, b = b, a + b n = n + 1 for n in fab(5): print n 其实这个在实际执行过程中，生成器并未实际执行，只有当调用. next() 时才开始执行，并返回当前的迭代值。最后执行完成之后，则会抛出 StopIteration 异常，表示迭代完成。当然，这个异常在大多数循环情况下 (比如 for) 并不需要手工处理。当然，你也可以选择使用下面手工的方法（注意：next 是 Python3 中的内置函数，如果是 Python2，请使用 f.next()）:\n➜ Desktop python Python 3.4.3 (default, Aug 14 2015, 11:21:11) [GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)] on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; def fab(max): ... n, a, b = 0, 0, 1 ... while n \u0026lt; max: ... yield b ... a, b = b, a + b ... n = n + 1 ... \u0026gt;\u0026gt;\u0026gt; f = fab(5) \u0026gt;\u0026gt;\u0026gt; next(f) 1 \u0026gt;\u0026gt;\u0026gt; next(f) 1 \u0026gt;\u0026gt;\u0026gt; next(f) 2 \u0026gt;\u0026gt;\u0026gt; next(f) 3 \u0026gt;\u0026gt;\u0026gt; next(f) 5 \u0026gt;\u0026gt;\u0026gt; next(f) Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; StopIteration 上面的方法如果不使用 yield，则换成一个支持 iterable 的可能更直观理解一点：\nclass Fab(object): def __init__(self, max): self.max = max self.n, self.a, self.b = 0, 0, 1 def __iter__(self): return self def next(self): if self.n \u0026lt; self.max: r = self.b self.a, self.b = self.b, self.a + self.b self.n = self.n + 1 return r raise StopIteration() 在异步中，程序的 ioloop 会自动处理这一类的生成器，这一个样例可以参考一下 Tornado 的处理方式: tornado.gen.coroutine。\n3.5 版本异步功能 在 3.5 之后的版本里，程序提供了 async 和 await 关键字，这两个关键字可以替代类似 tornado 中的 gen.coroutine/yield 或者是 asyncio.coroutine/yield 的作用。\n比如在上一节中的例子改造成为:\nimport asyncio import time async def slow_operation(n): await asyncio.sleep(1) print(\u0026#34;Slow operation {} complete\u0026#34;.format(n)) async def main(): start = time.time() await asyncio.wait([slow_operation(1), slow_operation(2), slow_operation(3), ]) end = time.time() print(\u0026#39;Complete in {} second(s)\u0026#39;.format(end-start)) loop = asyncio.get_event_loop() loop.run_until_complete(main()) 新的关键字会让我们的程序看上去更加清晰。不过从执行效率上看，现在还是相较 3.4.3 有一些退步：\n➜ Desktop pyenv shell 3.5.0rc1 ➜ Desktop python 3.5_asyncio.py Slow operation 1 complete Slow operation 3 complete Slow operation 2 complete Complete in 1.0012218952178955 second(s) ➜ Desktop python 3.4_asyncio.py Slow operation 1 complete Slow operation 3 complete Slow operation 2 complete Complete in 1.0060911178588867 second(s) 不过现在还只是 RC 版本，相信之后还是会有一些性能调优的可能性。\n更多的开发场景 现在 Tornado 正在开发的 4.3 版本已经支持了 Python3.5 的 async/await 关键字，在之后的开发中，可以替代 @gen.coroutine 和 yield 用于开发。根据作者的描述，如果是运行在 3.5 版本的 Python 上，建议使用关键字而不是 yield 方式，这样可以有更好的执行效率。当然，如果 3.5 只是一个可选版本，相信在相当长的一段时间之内，你还是需要使用原有的执行方式。\n潜在的坑 async 和 await 在 Python 3.5 与 3.6 版本中并不是关键字，Python 3.7 中会作为关键字。因此需要在现有的程序中替代掉已有的可能导致冲突的关键字。\n","date":"2015-08-14T18:11:15Z","permalink":"https://www.4async.com/2015/08/2015-08-14-introduction-to-async-and-await/","title":"Python async/await 入门"},{"content":"JWT 是什么 JWT 全称是 JSON Web Tokens，是 RFC 7519 标准，用于安全校验两方可信性的安全措施。\nJWT 解决了哪些问题？ JWT 本身设计是用于解决 Session 机制不能够很好的在 SPA/API 类型 (restful) 应用中处理身份认证问题。通常 API 的调用是无状态（stateless）的，使用 Session 等形式会有上下文要求。如当用户登录完成后，可以通过下发 JWT 的形式进行无状态的 API 调用。在此之前通常是使用的方式包括不限于如 Basic Auth、Oauth2 或 Token 形式进行。\nJWT 相比是额外添加了签名校验方式，本质上来说对抗如暴力碰撞等形式有一些作用。但是由于本身长度的限制，存储的信息量有限。\nJWT 处理方式 JWT 内容主要分为三段，分别对应头部信息，存储数据和签名信息三部分，中间使用『.』符号连接，三段信息均进行 Base64 编码。\n具体实现方式可参考如下伪代码实现：\nencodeBase64(header) + \u0026#39;.\u0026#39; + encodeBase64(payload) + \u0026#39;.\u0026#39; + Sign(key + encodeBase64(header) + \u0026#39;.\u0026#39; + encodeBase64(payload)) 其中 Sign 的算法是可以在 header 中进行定义，支持如 HMAC-SHA256 和 RSA-SHA256 等方式。key 则是用于验证的 key 信息。\n其中 header 格式如下:\n{\u0026#34;alg\u0026#34;: \u0026#34;HS256\u0026#34;, // 指定加密算法，可以选择 RS256 等。 \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34; } payload 为存储的数据区块，由于此部分未进行加密，不建议存储敏感信息。内容为字典结构。\nJWT 存在的安全问题 JWT 披露了一个比较 严重的安全问题：JWT 中允许设置加密方法『alg』为 none，这样签名信息可设置为 \u0026ldquo;\u0026quot;，这样就给恶意用户伪造 JWT 的可能。并且该验证是在验证签名之前（决定签名使用算法），所以在安全实现 JWT 时需要对验证算法进行可信安全校验。\n还有一个问题是 JWT 本身并不会加密 payload 中的信息，因此在传递敏感信息时需要单独对数据进行个别处理。\n参考连接 JWT.io http://jwt.io/ ","date":"2015-08-07T18:11:15Z","permalink":"https://www.4async.com/2015/08/2015-08-07-jwt-basic/","title":"JWT 介绍"},{"content":"最近在实现大型 B2B 系统之后，有很多细节需要考虑，其中一环就是权限控制。之前考虑的比较少，认为这一块框架可以 handle 问题不大，后来发现 Tornado 目前没有一个比较完善的权限管理模块，甚至连个包都没有\u0026hellip; 于是只能自己动手，丰衣足食了。参考的方式也是通过 Flask-Principal 的类似实现方式。\n原型 一个权限管理模块应该有的功能应该有哪些呢？从基本角度出发，应该包含用户身份 (Identity)、用户角色（RoleNeed）、用户权限（Permission）这三个最基本的分类组成。那么从最开始，就需要定义这几个类：\nclass RoleNeed(object): pass class Identity(object): pass class Permission(object): pass 再考虑一下，其实 RoleNeed 更像一个权限的枚举数组，除此之外，比如用户可能存在的权限与分组和单独的用户权限都有关系，这里需要更加抽象一下。具体的可以分成 UserNeed 和 RoleNeed，分别对应用户权限与用户组权限。这样如果通过类实现还是比较麻烦，需要继承，但是实际上这也只是一个元组而已。所以可以选择下面的方式：\nfrom functools import partial。 from collections import namedtuple Need = namedtuple(\u0026#39;Need\u0026#39;, [\u0026#39;method\u0026#39;, \u0026#39;value\u0026#39;]) UserNeed = partial(Need,\u0026#39;user\u0026#39;) RoleNeed = partial(Need,\u0026#39;role\u0026#39;) namedtuple 是 Python 高级数据结构中的一个内容，在 collections 包中，其实实现的效果是生成带名称的元组。比如 Need = namedtuple(\u0026lsquo;Need\u0026rsquo;, [\u0026lsquo;method\u0026rsquo;, \u0026lsquo;value\u0026rsquo;]) 实际上就是生成了一个名字叫做 Need 的元组，你可以通过 Need(\u0026lsquo;a\u0026rsquo;, \u0026lsquo;b\u0026rsquo;) 的方式设置对应的 method 和 value 的值。\n接下来的 partial 是和之前我另一篇文章中介绍的 wraps 在一个包中的功能，可以简单理解为方便填入一个初始化参数的方法，比如 partial(Need,\u0026lsquo;user\u0026rsquo;) 等同于 Need(\u0026lsquo;user\u0026rsquo;, x)，但是 x 的这个参数需要在调用 parial 的返回值的时候才会传入，可以用来预定义一部分的环境参数。\n现在相当于我们定义了一个以用户为维度身份权限要求和以用户组为维度的身份要求。接下来就可以实现用户身份的一些功能了。\n用户身份或者权限保存我们应该交由上层的业务逻辑来实现，以保证最大的模块复用性，那么用户身份就剩下了，声明用户身份和判断用户是否有权限执行某项权限操作的内容。\n所以我们可以这样实现：\nclass Identity(object): \u0026#34;\u0026#34;\u0026#34; A set of needs provided by this user example: identity = Identity(\u0026#39;ali\u0026#39;) identity.provides.add((\u0026#39;role\u0026#39;, \u0026#39;admin\u0026#39;)) \u0026#34;\u0026#34;\u0026#34;def __init__(self, name): self.name = name self.provides = set() def can(self, permission): return permission.allows(self) 这里的 provides 是作为用户权限的声明，是一个每个权限的集合。可以在最开始登录等等时间进行加载。而 can 方法则是判断某个权限是否可以被执行。\n","date":"2015-07-22T11:00:15Z","permalink":"https://www.4async.com/2015/07/2015-07-22-howto-make-your-own-permission-plugin/","title":"如何用 Python 实现一个权限管理系列 (一)"},{"content":"很久没有写技术存档了，太过于罪恶。最近在智能硬件创业公司担任架构师，推广一些更新更酷的技术应用在各个方面，包括 Golang/Python/Docker 等，如果你有兴趣，也欢迎加入我们: kevin[at yeeuu[dot]com。广告时间结束。\nPython 的修饰器是比较常见的开发应用帮助工具，他可以实现一些批量的修饰工作，比如统一来添加一些小功能等等。但是这些功能对原有的函数不产生侵入，也就是说可以实现快速的修改和替换、移除。\n如果你使用过 Python 的 Web 框架，相信你对修饰器应该并不陌生：Django、Flask、Tornado 等常见的框架中都包含了修饰器的使用。\n那么 decorators 是怎么实现的呢？还是先从一个简单的例子开始。先看下 Tornado 中的 tornado.web.authenticated 使用。\nclass IndexHandler(tornado.web.RequestHandler): @tornado.web.authenticated def get(self): self.render(\u0026#39;index.html\u0026#39;) tornado.web.authenticated 的作用就是判断 self.current_user 是否为 None 或者为空，否则跳转到之前设置的 login_url 地址去。至于获取 current_user 的内容，可以通过重载 get_current_user 函数实现。\n在查看修饰器的具体代码之前，我们先来了解一下 Python 修饰器的原理。Python 的修饰器其实是实现了下面的一个简单功能：\n@decorator def func(): pass 等价于\nfunc = decorator(func) 多层的修饰器则是实现了多层的回调调用。同时在底层层面，提供了 functools 包 用于实现相关功能，注意，该包是 2.5 之后版本中引入，如果你还在使用古老的 Python 版本，则可以手工实现同等功能。\n具体功能 实现代码 如下：\ndef authenticated(method): \u0026#34;\u0026#34;\u0026#34;Decorate methods with this to require that the user be logged in. If the user is not logged in, they will be redirected to the configured `login url \u0026lt;RequestHandler.get_login_url\u0026gt;`. If you configure a login url with a query parameter, Tornado will assume you know what you\u0026#39;re doing and use it as-is. If not, it will add a `next` parameter so the login page knows where to send you once you\u0026#39;re logged in.\u0026#34;\u0026#34;\u0026#34; # method 作为参数传入，实际上为类中的 get 等函数。 @functools.wraps(method) def wrapper(self, *args, **kwargs): # 判断当前用户 if not self.current_user: # 没有权限操作 if self.request.method in (\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;): # get、head 请求跳转到登录页面 url = self.get_login_url() if \u0026#34;?\u0026#34; not in url: if urlparse.urlsplit(url).scheme: # if login url is absolute, make next absolute too next_url = self.request.full_url() else: next_url = self.request.uri url += \u0026#34;?\u0026#34; + urlencode(dict(next=next_url)) self.redirect(url) return # 403 无权限操作 raise HTTPError(403) # 通过验证 return method(self, *args, **kwargs) # 函数式编程的典型范例 :) return wrapper 根据这个，我们也可以尝试写一个自己的修饰器:\n#!/usr/bin/python # -*- coding: utf-8 -*- import functools def print_hello(method): @functools.wraps(method) def wrapper(*args, **kwargs): print \u0026#34;hello!\u0026#34; return method(*args, **kwargs) return wrapper @print_hello def main(): print \u0026#39;in main\u0026#39; if __name__ == \u0026#39;__main__\u0026#39;: main() 输出结果如下：\n➜ Desktop python test_decorator.py hello! in main 现在看起来已经很不错了，但是不能修改参数看起来有些需求还是无法实现。那么能够通过修饰器传递修改一些参数么？答案是肯定的。\n修改一下上面的例子，我们试着用修饰器向函数中传递参数：\n#!/usr/bin/python # -*- coding: utf-8 -*- import functools def print_hello(method): @functools.wraps(method) def wrapper(*args, **kwargs): kwargs[\u0026#39;name\u0026#39;] = \u0026#39;Pythonic\u0026#39; return method(*args, **kwargs) return wrapper @print_hello def main(name): print \u0026#34;Hello, {}\u0026#34;.format(name) if __name__ == \u0026#39;__main__\u0026#39;: main() 输出结果为:\n➜ Desktop python test_decorator.py Hello, Pythonic 那么能不能向修饰器里面传递参数呢？当然也是可以的，不过相对来说更复杂一点：\n#!/usr/bin/python # -*- coding: utf-8 -*- import functools def print_hello(name): def real_wrapper(method): @functools.wraps(method) def wrapper(*args, **kwargs): kwargs[\u0026#39;name\u0026#39;] = name return method(*args, **kwargs) return wrapper return real_wrapper @print_hello(name=\u0026#39;Pythonic\u0026#39;) def main(name): print \u0026#34;Hello, {}\u0026#34;.format(name) if __name__ == \u0026#39;__main__\u0026#39;: main() 输出结果为:\n➜ Desktop python test_decorator.py Hello, Pythonic 上面的实现方法是函数式的实现，Python 同样支持类模式的修饰器支持，比如：\nclass function_wrapper(object): def __init__(self, wrapped): self.wrapped = wrapped def __call__(self, *args, **kwargs): return self.wrapped(*args, **kwargs) @function_wrapper def function(): pass 虽然修饰器很好，但是也是会存在一些问题的，使用这种方式之后，想要获取被包装函数的参数（argument）或源代码（source code）时并不能取到正确的结果。不过这个问题可以通过使用反射来解决：\ndef get_true_argspec(method): argspec = inspect.getargspec(method) args = argspec[0] if args and args[0] == \u0026#39;self\u0026#39;: return argspec if hasattr(method,\u0026#39;__func__\u0026#39;): method = method.__func__ if not hasattr(method,\u0026#39;func_closure\u0026#39;) or method.func_closure is None: raise Exception(\u0026#34;No closure for method.\u0026#34;) method = method.func_closure[0].cell_contents return get_true_argspec(method) 不过说实话，很少在项目中获取这些东西，只是提供一些解决方案，实际上 functools.wraps 可以解决绝大多数的问题。\n最后介绍一些比较常用的修饰器使用方法，比如，进行性能测试：\nimport cProfile, pstats, StringIO def run_profile(func): def wrapper(*args, **kwargs): datafn = func.__name__ + \u0026#34;.profile\u0026#34; # Name the data file prof = cProfile.Profile() retval = prof.runcall(func, *args, **kwargs) s = StringIO.StringIO() sortby = \u0026#39;cumulative\u0026#39; ps = pstats.Stats(prof, stream=s).sort_stats(sortby) ps.print_stats() print s.getvalue() return retval return wrapper 还有一些案例可以看之前提供的几个框架的 API，比如路由组织，异步处理等等都是通过修饰器实现的。\n","date":"2015-06-17T18:00:15Z","permalink":"https://www.4async.com/2015/06/2015-06-17-something-about-python-decorator/","title":"Python Decorator 修饰器简介"},{"content":"注：rust 目前更新仍旧很频繁，语法变化比较大，学习主要基于 Rust 1.0 Alpha 版本，之后可能会有部分变化。\n补充下笔记一中的内容：在 Rust 中变量默认是不能改变的，只有声明为 mut，才可以进行改变。\n比如：\nlet x = 5; x = 7; 会报错 re-assignment of immutable variable，不过下面的方法就可以正常执行了：\nlet mut x = 5; x = 7; 逻辑控制 一般逻辑控制分为判断分支、循环等等. if 在 rust 中区别不是特别大：\nlet x = 5; if x == 5 {println!(\u0026#34;x is five!\u0026#34;);} else {println!(\u0026#34;x is not five :(\u0026#34;);} 不过还可以这样写：\nlet x = 5; let y = if x == 5 {10} else {15}; // y: i32 值得注意的是，下面的写法是不正确的：\nlet x = 5; let y: i32 = if x == 5 {10;} else {15;}; 但是 for 循环区别就比较大了：\nfor x in range(0, 10) {println!(\u0026#34;{}\u0026#34;, x); // x: i32 } while 循环则可以更加简单：\nlet mut x = 5u; // mut x: uint let mut done = false; // mut done: bool while !done { x += x - 3; println!(\u0026#34;{}\u0026#34;, x); if x % 5 == 0 {done = true;} } 如果是要求死循环，使用 loop{} 更加方便。在循环中可以使用 break 和 continue 控制流程进度，这个没什么区别就不再说了。\n复杂类型 Rust 作为更先进的编程语言，一些复杂变量类型肯定是支持的，比如 Tuples、Structs、Enums 等等。\nlet x = (1,\u0026#34;hello\u0026#34;); // or let x: (i32, \u0026amp;str) = (1,\u0026#34;hello\u0026#34;); struct Point { x: i32, y: i32, } let mut point = Point {x: 0, y: 0}; enum Ordering { Less, Equal, Greater, } Ordering::Less let a = [1, 2, 3]; println!(\u0026#34;a has {} elements\u0026#34;, a.len()); for e in a.iter() {println!(\u0026#34;{}\u0026#34;, e); } println!(\u0026#34;{}\u0026#34;, a[0]); 值得一提的是 :: 表示的命名空间，这点和 c 有一些区别。\n除此之外，Rust 自定义了一些类型，一种是介于 tuple 和 struct 之间的类型，比如：\nstruct Color(i32, i32, i32); struct Point(i32, i32, i32); let black = Color(0, 0, 0); let origin = Point(0, 0, 0); 还有一种叫向量：\nlet mut nums = vec![1, 2, 3]; nums.push(4); 有点像队列，可变商都，可以 push 进行添加。\n还有一种切片：\nlet a = [0, 1, 2, 3, 4]; let middle = a.slice(1, 4); // A slice of a: just the elements [1,2,3] for e in middle.iter() {println!(\u0026#34;{}\u0026#34;, e); // Prints 1, 2, 3 } Match 这个也是 Rust 特有的，有点类似 switch 的感觉：\nlet x = 5; match x {1 =\u0026gt; println!(\u0026#34;one\u0026#34;), 2 =\u0026gt; println!(\u0026#34;two\u0026#34;), 3 =\u0026gt; println!(\u0026#34;three\u0026#34;), 4 =\u0026gt; println!(\u0026#34;four\u0026#34;), 5 =\u0026gt; println!(\u0026#34;five\u0026#34;), _ =\u0026gt; println!(\u0026#34;something else\u0026#34;), } 但是 Rust 一定要有最后一行，否则会编译报错。\n字符串类型 Rust 的字符串默认是 UTF-8 编码，但是有两种类型的字符串 \u0026amp; str 和 String。这两种类型下面的方式比较：\nlet a = \u0026#34;aaa\u0026#34;; //a: \u0026amp;str let b = a.to_string(); // b: String let c = b.to_slice(); //c: \u0026amp;str \u0026amp;str 和 String 的区别是，\u0026amp;str 更像是 slice，而 String 则是像一个分配过的内存空间字符串，不能够被修改长度（待确认）。String-\u0026gt;\u0026amp;str 会比较方便，但是 \u0026amp; str-\u0026gt;String 消耗则会比较大。 输入输出 输入是官方库 std::io::stdin()。:: 之前介绍过，是命名空间，相当于 std::io 库里的 stdin 函数。\n比如：\nuse std::io; fn main() {println!(\u0026#34;Type something!\u0026#34;); let input = io::stdin().read_line().ok().expect(\u0026#34;Failed to read line\u0026#34;); println!(\u0026#34;{}\u0026#34;, input); } ","date":"2015-01-12T20:46:15Z","permalink":"https://www.4async.com/2015/01/2015-01-12-rust-note-2/","title":"Rust 学习笔记二"},{"content":"注：rust 目前更新仍旧很频繁，语法变化比较大，学习主要基于 Rust 1.0 Alpha 版本，之后可能会有部分变化。\n先来说下 Rust 的优点：\nRAII(Resource Acquisition Is Initialization) ：垃圾回收 GC 是解决内存安全的最普通方式，但是 Rust 系统并不依靠 GC，而是在编译时通过 RAII 实现资源自动释放。 ** 安全便捷的并发开发 ** ：这个后面学习里面会讲到，这个和 Golang 应该算竞争了吧。 ** 高效的执行效率 **：根据测试，rust 的性能应该是超过了 Java 了。（一时半会找不到了，找到补上） 缺点我感觉如下： ** 学习复杂度不低 **：相对 go 或者其他语言，还是复杂很多，学习门槛比较高。 **GC 的问题 **：GC 好处是屏蔽了很多问题，会带来性能问题，比如 Go，Rust 需要了解更多的编译器细节。 Rust 基础 Rust 原文件扩展名是. rs，编译程序名字叫做 rustc。可以使用 rustc xxx.rs 编译成可执行文件。首先还是管用的 hello world:\nfn main(){println!(\u0026#34;hello world!\u0026#34;);} println 是一个宏定义，后面会再详细写一下。fn 有点类似表示函数的 function 的定义，说起来，rust 是 Javascript 之父搞的，品位嘛，呵呵。\nRust 安装程序带一个叫做 cargo 的工具，这个工具是用于项目编译的，可以按照下面的路径组织文件：\n|__Cargo.toml |__src |__main.rs Cargo.toml 文件内容如下：\n[package] name = \u0026#34;hello_world\u0026#34; version = \u0026#34;0.0.1\u0026#34; authors = [\u0026#34;Your name \u0026lt;you@example.com\u0026gt;\u0026#34; ] [[bin]] name = \u0026#34;main\u0026#34; 使用 cargo build 命令可以在 src 同级目录下生成 target 目录，下面就包含了生成的二进制文件。执行完成之后，在 Cargo.toml 同级目录下会生成一个 Cargo.lock 文件，这个文件是用于跟踪程序版本的。\n参数赋值 参数赋值主要是使用下面的方式：\nlet x = 5; let (y, z) = (2, 3); let i:i32 = 4; println!(\u0026#34;The value of x is: {}, y is: {}, z is: {}, i is: {}\u0026#34;, x, y, z, i); Rust 会自动定义类型，你也可以在变量后面加上 : 类型 定义类型，比如：let j:i32。值得注意的是 Rust 会严格检查一些这样的未初始化变量，如果不赋值的话，一定会编译报错的。\n","date":"2015-01-11T22:00:15Z","permalink":"https://www.4async.com/2015/01/2015-01-11-rust-note-1/","title":"Rust 学习笔记一"},{"content":"Phonegap 主要是方便用于移动开发的平台，对于没有太多功能需求的组建来说，可以实现快速的功能开发。友盟是一套针对程序使用者分析的工具，这两者集成可以方便对使用程序的用户进行分析。现阶段自己还没时间搭建一个自己用的平台出来，只好将就着用一些开源和商业方案实现。\n集成准备 首先需要有一个工程，如果你已经有了，则可以跳过这个步骤。同时，还需要你有一个 App Key（iOS 与 Android 不同），可以在管理中找到 key。\n假设你已经安装了 cordova，并且已经有 iOS 和 Android 开发环境。这样一般都是 Mac 系统了。\ncordova create hello com.example.hello HelloWorld cd hello cordova platform add android cordova platform add ios\niOS 集成 ios 集成方式比较简单，首先，找到项目目录的 platform/ios 中的项目，打开。\n请在你的工程目录结构中，右键选择 Add-\u0026gt;Existing Files…，选择 SDK 压缩包中的 libMobClickLibrary.a 和 MobClick.h。或者将这两个文件拖入 XCode 工程目录结构中，在弹出的界面中勾选 Copy items into destination group's folder(if needed), 并确保 Add To Targets 勾选相应的 target。\n添加依赖框架 (Framework) 和编译器选项 TARGETS\u0026ndash;\u0026gt;Build Phases\u0026ndash;\u0026gt;Link Binary With Libraries\u0026ndash;\u0026gt; + \u0026ndash;\u0026gt;libz.dylib 在 AppDelegate.m 中填写 Appkey，设置发送策略和填写渠道 id 三部分，代码示例如下：\n- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions { ...... [MobClick startWithAppkey:@\u0026#34; 你的 AppKey\u0026#34;];} Android 集成 待补充。\n","date":"2014-12-27T20:20:15Z","permalink":"https://www.4async.com/2014/12/2014-12-27-integrate-umeng-into-phonegap-app/","title":"Phonegap 中整合友盟分析"},{"content":"这是一篇如何编写高效 Golang 语言程序的文章。这是从一些开发者的经验总结中整理出来的。根据这篇文章，你可以编写出一篇更高效的 Golang 程序。\n尽量使用直接的类型 interface{} 提供了 golang 中的 interface 类似于 java 的 interface、PHP 的 interface 或 C++ 的纯虚基类。通过这种方式可以提供更快捷的编码。但是这种方式也带来了一些问题，最大的问题还是性能问题。\n// method 1 a.AA() // method 2 v.(InterfaceA).AA() // method 3 switch v.(type) { case InterfaceA: v.(InterfaceA).AA()} 这三组方法性能逐个下降，最好的方式是直接进行类型引用。\n指针传参效率更高 指针传参会减少对象复制过程，效率更高。\nfunc Call(a *Struct) uint64 {return a.Ba} 创建对象时避免创建引用 上面的方法效率要比下面的方法效率更高：\n// method 1 new(Struct) Struct{} // method 2 \u0026amp;BigStruct{} range 遍历 常见的 range 方式通常是如下的方式：\nfor k,v := range slices {log.Println(v) } 或者是省略 k，使用 v 就可以遍历整个 slices。但是，实际上下面这个方法会更快，原因是这里面节省了 v 的拷贝，速度要比拷贝更快：\nfor k,_ := range slices {log.Println(slices[k]) } 更快的运算 除法运算更慢，尝试变成乘法吧。嗯，就是这样。\n更快的结构体 map 方式的结构体，其他的数据方式，性能方式相差不大。效率上，硬编码 \u0026gt; 指针 slice 的 range 循环 \u0026gt; for 循环，需要根据具体环境选择，毕竟相差不大。\n反射会影响性能 除非必要情况下，减少反射可以提升程序的整体性能。\n避免大量重复创建对象 会对 GC 造成负担，目前 Golang 的 GC 会导致程序暂停。连续小内存分配会导致大量的 cpu 消耗在 scanblock 这个函数上；连续 make([]T, size) 来分配 slice 还会额外导致 cpu 消耗在 runtime.memclr 函数上。\n","date":"2014-12-18T23:27:15Z","permalink":"https://www.4async.com/2014/12/2014-12-18-golang-optimizing/","title":"Golang 高效编写（整理）"},{"content":"这个发现是 logger 或者 traceback 是有这个功能的，所以简单的研究了一下，其实很简单：\n#!/usr/bin/env python # -*- coding: utf-8 -*- import sys print sys._current_frames().values()[0].f_code.co_filename, print sys._current_frames().values()[0].f_lineno 执行结果如下：\nprintno.py 7 ","date":"2014-10-22T12:21:15Z","permalink":"https://www.4async.com/2014/10/2014-10-22-python-get-current-line-and-filename/","title":"Python 获取当前执行文件名和行数"},{"content":"这个是在 OS X 上折腾 cubieboard 的笔记。首先，无论是树莓派还是 cubieboard 之类的开发板，都是需要装一个 usb2tty 的驱动，通过 console 连接到开发板上。\n步骤参考：http://pbxbook.com/other/mac-tty.html，简单复述一下：\n首先，需要安装 tty2usb 的驱动，点击下面两个连接下载后，安装 pkg 文件。 PL2303_MacOSX_v1_5_1.zip FTDI Driver 重启之后，将 usb 连接至 Mac，通过执行 ls /dev/cu.* 查看已经连接的设备，默认一般名称是 / dev/cu.usbserial 执行 screen /dev/cu.usbserial 波特率 登录 console，波特率需要查一下官方手册看下具体多少，一般是 9600 或者 115400 的多一些。 如果 screen 进去发现没反应，按按回车看看。 如果想结束此次会话，按 ctrl-a 然后 ctril-\\ 结束会话。 cubieboard 拿到是 Android 系统，建议直接换成官方支持的 Lubuntu 系统吧，毕竟方便一些。\n更换步骤这里不多说了，主要是下载镜像，官方提供工具，直接刷进去就好了。\n","date":"2014-09-10T14:49:15Z","permalink":"https://www.4async.com/2014/09/2014-09-10-trying-cubieboard-on-mac-os-x/","title":"折腾日记 ---cubieboard(Mac OS X 篇)"},{"content":"今天有个项目出了点问题，关于基于 MobileSubstrate（现在叫 Cydia Substrate 了）的 tweak 的问题。\n其实本来问题不大的，主要有几个技巧可以记录一下。\nHook 的日志打印 这个算是个小技巧吧，来自于半仙，是一个 MS 的调试模式。\nextern bool MSDebug MSDebug = true 这个模式作用是专门用来查看一些细节的，比如 MSHookFunction 函数，会打印被 hook 的函数的前 16 字节和被 hook 之后的 10 字节，可以辅助判断这个 hook 的 hook 细节。\nMSHookFunction 函数 old 为 nil 这个原因很简单，函数定义定义不对\u0026hellip;..= =|||\n","date":"2014-08-13T22:00:15Z","permalink":"https://www.4async.com/2014/08/2014-08-13-diagnostics-ios-tweak-based-on-mobile-substrate/","title":"CydiaSubstrate 故障诊断"},{"content":"ollvm 的项目我再这里就不做介绍了。整合进入项目的不过两种：一种是 Theos，另外一种是 Xcode。\n不过值得注意的是 ollvm 主要有效的还是 c 的函数，对 OC 对抗效果不明显。对抗分析的问题，请参考半仙的《软件反分析》。\nTheos 整合 Theos 主要依赖于 Makefile，因此需要 Makefile 文件来实现功能了。首先在文件里添加：\nTARGET_CC = path_to_clang/clang TARGET_CXX = path_to_clang/clang++ TARGET_LD = path_to_clang/clang++ 然后修改 (项目名)_CFLAGS 参数，如果没有则添加：\nxxx_CFLAGS = -fobjc-arc -mllvm -fla 这样就可以在项目中应用 ollvm 了。\nXcode 整合 集成进入 Xcode 需要修改几个文件，根据现有的插件进行修改。\n具体修改步骤如下：\n$ cd /Applications/Xcode.app/Contents/PlugIns/Xcode3Core.ideplugin/Contents/SharedSupport/Developer/Library/Xcode/Plug-ins/ $ sudo cp -r Clang\\ LLVM\\ 1.0.xcplugin/ Obfuscator.xcplugin $ cd Obfuscator.xcplugin/Contents/ $ sudo plutil -convert xml1 Info.plist $ sudo vim Info.plist 修改文件内容（修改前 -\u0026gt; 修改后）\n\u0026lt;string\u0026gt;com.apple.compilers.clang\u0026lt;/string\u0026gt; -\u0026gt; \u0026lt;string\u0026gt;com.apple.compilers.obfuscator\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;Clang LLVM 1.0 Compiler Xcode Plug-in\u0026lt;/string\u0026gt; -\u0026gt; \u0026lt;string\u0026gt;Obfuscator Xcode Plug-in\u0026lt;/string\u0026gt; 接下来修改 Info.plist\n$ sudo plutil -convert binary1 Info.plist $ cd Resources/ $ sudo mv Clang\\ LLVM\\ 1.0.xcspec Obfuscator.xcspec $ sudo vim Obfuscator.xcspec 修改为：\nIdentifier = \u0026#34;com.apple.compilers.llvm.clang.1_0\u0026#34;; -\u0026gt; Identifier = \u0026#34;com.apple.compilers.llvm.obfuscator.3_4\u0026#34;; Name = \u0026#34;Apple LLVM 5.1\u0026#34;; -\u0026gt; Name = \u0026#34;Obfuscator 3.4\u0026#34;; Description = \u0026#34;Apple LLVM 5.1 compiler\u0026#34;; -\u0026gt; Description = \u0026#34;Obfuscator 3.4\u0026#34;; Vendor = Apple; -\u0026gt; Vendor = HEIG-VD; Version = \u0026#34;5.0\u0026#34;; -\u0026gt; Version = \u0026#34;3.4\u0026#34;; ExecPath = \u0026#34;clang\u0026#34;; -\u0026gt; ExecPath = \u0026#34;/path/to/obfuscator_bin/clang\u0026#34;; 接下来修改显示：\n$ cd English.lproj/ $ sudo mv Apple\\ LLVM\\ 5.1.strings \u0026#34;Obfuscator 3.4.strings\u0026#34; $ sudo vim Obfuscator\\ 3.4.strings 修改为：\n\u0026#34;Name\u0026#34; = \u0026#34;Apple LLVM 5.0\u0026#34;; -\u0026gt; \u0026#34;Name\u0026#34; = \u0026#34;Obfuscator 3.4\u0026#34;; \u0026#34;Description\u0026#34; = \u0026#34;Apple LLVM 5.0 Compiler\u0026#34;; -\u0026gt; \u0026#34;Description\u0026#34; = \u0026#34;Obfuscator 3.4\u0026#34;; \u0026#34;Version\u0026#34; = \u0026#34;5.0\u0026#34;; -\u0026gt; \u0026#34;Version\u0026#34; = \u0026#34;3.4\u0026#34;; \u0026#34;Vendor\u0026#34; = \u0026#34;Apple\u0026#34;; -\u0026gt; \u0026#34;Vendor\u0026#34; = \u0026#34;HEIG-VD\u0026#34;; 重启一下你的 Xcode，然后再 Build Options 里面，可以设置 Compiler for C/C++/Objective-C 为 Obfuscator 3.4。\n如果想要添加 cflag，可以在 CustomFlags 中自行添加。\n目前的问题 如果你用起来，现在 ARM64 解决还有问题，我正在研究，如果你知道如何解决，欢迎指出。\n➜ xxx make Making all for tweak xxx... Preprocessing xxx.x... Compiling xxx.x... clang-3.4: error: invalid arch name \u0026#39;-arch arm64\u0026#39; make[2]: *** [xxx.x.585e736e.o] Error 1 make[1]: *** [internal-library-all_] Error 2 make: *** [xxx.all.tweak.variables] Error 2 ","date":"2014-08-01T15:52:15Z","permalink":"https://www.4async.com/2014/08/2014-08-01-migrate-ollvm-into-ios-project/","title":"整合 ollvm 进入项目"},{"content":"Docker 是一个最近非常风行的轻量级应用容器。如果不知道的同学自行搜索去吧，这里不在赘述了。针对于 Docker，有个初创公司推出了适用于 Docker 专用的 Linux：CoreOS。一个号称系统运行内存只占用 161MB 的精简系统。这个 CoreOS 只提供了 Docker 需要的必须环境和管理功能，因此很轻量级，非常适合批量部署和我们这种用来研究学习目的的。\n值得一提的是里面的管理工具也是由 Golang 开发的，提供 json api 接口，有兴趣的话可以自行查看源代码。\n安装必须软件 必须软件需要 Vargrant 和 VirtualBox。Vargrant 可以当作是一个虚拟机和镜像管理器，VirtualBox 我就不用说了吧。安装下载就不说立刻，下载安装地址见下：\nhttp://www.vagrantup.com/downloads.html https://www.virtualbox.org/wiki/Downloads CoreOS 配置 CoreOS 的 Vargrant 脚本可以在 https://github.com/coreos/coreos-vagrant/ 找到，下载下来的方法很简单：\ngit clone https://github.com/coreos/coreos-vagrant/ cd coreos-vagrant 然后执行命令直接开启虚拟机了：\nvagrant up 如果第一次执行，会下载 CoreOS 的镜像，站点在国外，难免会慢一点，不过也没啥关系，稍微等一下吧，如果出现问题了，就重新执行一下。\n如果出现下面错误：\nYou specified a box version constraint with a direct box file path. Box version constraints only work with boxes from Vagrant Cloud or a custom box host. Please remove the version constraint and try again. 注释掉 Vagrantfile 文件中的 config.vm.box_version 这行。\n执行完成上面的指令之后，CoreOS 就运行了，同时你也注意到有一些关于 ssh 的信息。这个时候怎么登陆到 CoreOS 呢？\nvagrant ssh 然后就登录到了 CoreOS 系统了。\n如果你有代理想要下载加速，也是可以的，在命令行中执行 (Windows)：\nset all_proxy=socks5://127.0.0.1:1080 Docker 试用 先来一个简单的 hello world：\ndocker run ubuntu /bin/echo hello world 嗯，据说国内以后会有源，不过现在还没有，所以镜像又得从国外下载，又是一个漫长的等待\u0026hellip;. 经过一天一夜的下载，我的 repo 终于下载成功了\u0026hellip;.\ncore@core-01 / $ docker run ubuntu /bin/echo hello world hello world 这样就表示一个 docker 模版安装成功了。不过这还早，怎么利用 docker 部署 python 应用呢？\n首先是创建一个 Docker 容器：\nsudo docker run -i -t -p 80:80 ubuntu /bin/bash 执行完成之后，你不仅会创建一个容器，并且会登录到这个容器内了。这样就可以当作一个简单的系统来应用了。退出容器的方法是按快捷键 CTRL+P 和 CTRL+Q。列举运行的 docker 可以使用 sudo docker ps 命令，然后 docker attach [id] 方式进入容器。\n根据之前提到的内容，我们现在可以执行 coreos 里面没有的命令了，比如说 apt-get:\napt-get update apt-get upgrade 接下来是安装必须的工具了：\napt-get install -y tar git curl nano wget dialog net-tools vim build-essential zlib1g-dev apt-get install -y python python-dev python-setuptools python-pip libxml2-dev libxslt-dev libffi-dev 接下来安装一些 python 依赖库：\npip install cython gevent flask falcon sqlalchemy lxml cffi somplejson pip install --allow-external mysql-connector-python mysql-connector-python 下面就是开发一些的东西啦，每个人都不同，这里就不多说了。docker 自动化部署之类的东西就放在下个日志来讲好了。\n","date":"2014-06-10T11:52:15Z","permalink":"https://www.4async.com/2014/06/2014-06-01-try-docker-with-coreos/","title":"初识 Docker"},{"content":"好吧，这个问题确实比较蛋疼的一个问题，也是最近经常遇到的问题。具体的原因是这样的，最近使用了一个开源的库实现功能，实现方法是使用的回调方式。这样就出现了一个问题，回调过来的对象是什么格式的我并不知道具体细节。有人说，看看文档不就行了么，问题是这个项目他就没个详细文档。其实读代码也是可以解决这个问题的，但是实际上这是一个时间的工作，并且很容易会把自己也绕进去。那怎么办呢？别急，往下看。\n直接 dir 输出属性 这个很简单了，直接 print dir(对象名) 就可以打印对象的属性名称了。不过这个得来回测试一下，不能一次性把所有的想要的都能得到。\n利用 pdb 快速定位 pdb 是官方库中的 Python Debugger，使用方法也比较简单：在 Python 代码中 import pdb 库，然后在指定的位置添加一句：pdb.set_trace()。以我的代码为例，我需要了解 msg 的结构：\nclass StickyMaster(controller.Master): ...... def handle_request(self, msg): pdb.set_trace() ...... 然后正常执行这个程序，在执行到这句时，得到如下结果：\n✗ python pdbtest.py ...... \u0026gt; pdbtest.py(63)handle_request() -\u0026gt; if allow_check(msg) is False: (Pdb) 是不是看起来很像 gdb 一类的工具？这个时候输入命令查看对象了，输入 h 可以查看帮助：\n(Pdb) h Documented commands (type help \u0026lt;topic\u0026gt;): ======================================== EOF bt cont enable jump pp run unt a c continue exit l q s until alias cl d h list quit step up args clear debug help n r tbreak w b commands disable ignore next restart u whatis break condition down j p return unalias where Miscellaneous help topics: ========================== exec pdb Undocumented commands: ====================== retval rv 接下来查看下当前变量：\n(Pdb) a self = \u0026lt;__main__.StickyMaster instance at 0x107884128\u0026gt; msg = \u0026lt;libmproxy.flow.Request instance at 0x1080d5680\u0026gt; 然后直接 dir 一下这个变量的属性名称，这个时候的好处是方便进行查看属性值：\n(Pdb) dir(msg) [\u0026#39;__doc__\u0026#39;, \u0026#39;__eq__\u0026#39;, \u0026#39;__hash__\u0026#39;, \u0026#39;__init__\u0026#39;, \u0026#39;__module__\u0026#39;, \u0026#39;_assemble\u0026#39;, \u0026#39;_assemble_head\u0026#39;, \u0026#39;_from_state\u0026#39;, \u0026#39;_get_state\u0026#39;, \u0026#39;_load_state\u0026#39;, \u0026#39;_set_replay\u0026#39;, \u0026#39;anticache\u0026#39;, \u0026#39;anticomp\u0026#39;, \u0026#39;client_conn\u0026#39;, \u0026#39;close\u0026#39;, \u0026#39;constrain_encoding\u0026#39;, \u0026#39;content\u0026#39;, \u0026#39;copy\u0026#39;, \u0026#39;decode\u0026#39;, \u0026#39;encode\u0026#39;, \u0026#39;get_content_type\u0026#39;, \u0026#39;get_cookies\u0026#39;, \u0026#39;get_decoded_content\u0026#39;, \u0026#39;get_form_urlencoded\u0026#39;, \u0026#39;get_header_size\u0026#39;, \u0026#39;get_path_components\u0026#39;, \u0026#39;get_query\u0026#39;, \u0026#39;get_transmitted_size\u0026#39;, \u0026#39;get_url\u0026#39;, \u0026#39;headers\u0026#39;, \u0026#39;host\u0026#39;, \u0026#39;httpversion\u0026#39;, \u0026#39;ip\u0026#39;, \u0026#39;is_live\u0026#39;, \u0026#39;is_replay\u0026#39;, \u0026#39;method\u0026#39;, \u0026#39;path\u0026#39;, \u0026#39;port\u0026#39;, \u0026#39;replace\u0026#39;, \u0026#39;reply\u0026#39;, \u0026#39;rfile\u0026#39;, \u0026#39;scheme\u0026#39;, \u0026#39;set_form_urlencoded\u0026#39;, \u0026#39;set_live\u0026#39;, \u0026#39;set_path_components\u0026#39;, \u0026#39;set_query\u0026#39;, \u0026#39;set_url\u0026#39;, \u0026#39;size\u0026#39;, \u0026#39;ssl_setup_timestamp\u0026#39;, \u0026#39;stickyauth\u0026#39;, \u0026#39;stickycookie\u0026#39;, \u0026#39;tcp_setup_timestamp\u0026#39;, \u0026#39;timestamp_end\u0026#39;, \u0026#39;timestamp_start\u0026#39;, \u0026#39;wfile\u0026#39;] (Pdb) msg.path \u0026#39;/\u0026#39; 借助工具直接确定 这里我使用的是 WingIDE，很简单方便。首先下个断点，然后运行下程序，结果如下图：\n就是这么简单！\n","date":"2014-05-30T00:10:15Z","permalink":"https://www.4async.com/2014/05/2014-05-30-quick-identify-python-object-struct/","title":"快速确认 Python 对象结构"},{"content":"因为系统不方便使用包升级，因此使用的源码方式安装，坑基本上都踩过了，做一下记录吧。\nzbar 的安装依赖于 ImageMagick，ImageMagick 则依赖于 jpeg 提供 jpeg 图片支持，因此安装顺序为反序安装。\n安装 jpeg 库 这个安装比较简单，没什么坑来的。先到 http://www.imagemagick.org/download/delegates/ 下载 jpeg 的库，比如我用的是 jpegsrc.v9a.tar.gz，执行下列执行进行安装：\n/.configure make \u0026amp;\u0026amp; make install 安装 ImageMagick 这个安装稍微麻烦一点，还是差不多开始的：\n./configure make \u0026amp;\u0026amp; make install 下面是需要配置一些参数，否则 zbar 安装会不成功：\nexport MAGICK_CFLAGS=\u0026quot;/usr/local/include/ImageMagick-6/\u0026quot; export MAGICK_LIBS=\u0026quot;/usr/local/lib/libMagickWand-6.Q16.so\u0026quot; 安装 zbar 接下来编译 zbar 了，这个之前配置好了会方便一点，同时禁用一些不用的功能：\n./configure --enable-shared --without-gtk --without-qt --without-python CPPFLAGS=-I/usr/local/include/ImageMagick-6 make \u0026amp;\u0026amp; make install 接下来添加 ldconfig 文件让 libzbar.so 生效。\nldconfig 测试安装 测试一下 zbar 是否安装成功：\nwget http://i1.hexunimg.cn/2012-03-13/139256375.jpg /usr/local/bin/zbarimg 139256375.jpg QR-Code:http://www.baidu.com QR-Code:http://apk.hiapk.com/m/downloads?id=com.qq.reader\u0026amp;vcode=11 QR-Code:http://apk.hiapk.com/m/downloads?id=cn.ibuka.manga.ui\u0026amp;vcode=16843778 QR-Code:http://apk.hiapk.com/m/downloads?id=com.kugou.android\u0026amp;vcode=4120 QR-Code:http://apk.hiapk.com/m/downloads?id=com.kandian.vodapp4pad7in\u0026amp;vcode=7 scanned 4 barcode symbols from 1 images in 0.05 seconds ","date":"2014-05-20T15:33:35Z","permalink":"https://www.4async.com/2014/05/2014-05-20-build-zbar-on-linux/","title":"Linux 系统上的 zbar 编译安装"},{"content":"先从一个例子来说，一个斐波那契数列数列例子说起：\n# fib.py #!/usr/bin/env python # -*- coding: utf-8 -*- def fib(i): a = 0 b = 1 while i \u0026gt; 0: i -= 1 a, b = b, a+b return True # try_c.py #!/usr/bin/env python # -*- coding: utf-8 -*- import time from fib import fib startTime = time.time() for i in xrange(1,10): fib(10**6) print time.time() - startTime 在 CPython 下执行效果如下：\n➜ cython python try_c.py 429.003911972 换成 pypy 的话，效率会更高一些：\n(pypy)➜ cython pypy try_c.py 355.186796904 能不能有更快的效率呢？Python 的执行效率确实令人诟病，如果有 Python 的编写速度，C 的执行效率更好了（你说的是 Golang 嘛？）。这就是需要 Cython 出场了。\n执行下面的命令就可以安装 Cython 了：\npip install cython 然后编写一个 setup.py 文件：\n# setup.py #!/usr/bin/env python # -*- coding: utf-8 -*- from distutils.core import setup from Cython.Build import cythonize setup(name =\u0026#39;cython app\u0026#39;, ext_modules = cythonize(\u0026#34;fib.pyx\u0026#34;), ) 将 fib.py 文件重命名为 fib.pyx 文件，然后执行下面命令就可以了：\n(pypy)➜ cython pypy setup.py build_ext --inplace running build_ext building \u0026#39;fib\u0026#39; extension cc -arch x86_64 -O2 -fPIC -Wimplicit -I/Developer/pypy/include -c fib.c -o build/temp.macosx-10.9-x86_64-2.7/fib.o fib.c:58:9: warning: \u0026#39;Py_OptimizeFlag\u0026#39; macro redefined #define Py_OptimizeFlag 0 ^ /Developer/pypy/include/pypy_macros.h:613:9: note: previous definition is here #define Py_OptimizeFlag PyPy_OptimizeFlag ^ 1 warning generated. cc -shared -undefined dynamic_lookup -arch x86_64 build/temp.macosx-10.9-x86_64-2.7/fib.o -o /Desktop/cython/fib.pypy-23.so 具体执行的代码如下：\ncython fib.pyx cc -arch x86_64 -O2 -fPIC -Wimplicit -I/Users/jetlee/Developer/pypy/include -c fib.c -o build/temp.macosx-10.9-x86_64-2.7/fib.o cc -shared -undefined dynamic_lookup -arch x86_64 build/temp.macosx-10.9-x86_64-2.7/fib.o -o /Users/jetlee/Desktop/cython/fib.pypy-23.so 这里有个很奇怪的地方，cython 生成的是 so 文件，我的系统是 mac，shared library 是. dylib 文件，不过 cython 这里只认识 so 文件，所以自己不要自以为是的改名了\u0026hellip;\n(pypy)➜ cython pypy try_c.py 208.884904146 再测试一下 CPython 的结果：\n(pypy)➜ cython python try_c.py 215.461463928 上面的结果不是同样条件测试的，会对结果有一定的影响。具体情况下的结果还请自行测试。\n除了利用 Cython 外，还有其他几种方法用来加速，这个可以自行搜索一下，比如 cffi，具体不在这里赘述。我个人比较倾向于 Cython 的方法，这种方法可以实现无需修改代码的加速（如上），同样也可以实现更高效的代码级别编译，效率更高。但是缺点是绕不开 python 的 GIL 问题，具体的可以看一下 这篇文章 介绍 GIL 问题。\n","date":"2014-05-17T17:41:14Z","permalink":"https://www.4async.com/2014/05/2014-05-17-speed-python-code-with-cython/","title":"利用 Cython 加速 Python 代码"},{"content":"channel 是一个比较神奇的东西，以前很少研究，不过最近的项目有这方面的需求就看了一下。下面主要从 channel 的功能谈起。\nchannel 的读取和写入问题 channel 的读取写入取决于当前的 channel 的状态，大家应该都知道下面的情况是一定会产生死锁：\nch := make(chan int) \u0026lt;-ch 但是针对于 close 掉的 channel，则是一定可以读取成功的：\nch := make(chan int) close(ch) i:=\u0026lt;-ch\t//i=0 对于这种情况，golang 添加了一个结果类型判断 i, succ :=\u0026lt;-ch\t//i=0,succ=false，若结果为 false，说明该 channel 已经关闭了。但是 close 过的 channel 再写入，是会 panic 的。\n有时可能还有一些时间上的要求，比如说判断是否超时。这个时候可以结合上一篇文章中的 select 来解决这个问题：case \u0026lt;- time.After(time.Second*2): 这样就可以设置一个超时时间，解决这个问题。这个问题我们之前有探讨过，select 会判断 channel 的状态是否 ready。超时时，需要取得数据的 channel 应该是未完成的，这个时候就可以进入超时 block。\n但是对于在上一篇文章中的另外一种情况，还需要在解决问题时根据具体情况确定具体的解决方案。\n利用 channel 实现去异步化 比如一个比较经典的生产者消费者模型：\npackage main import (\u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) // 生产者 func Producer(id int, item chan int) { for i:=0; i\u0026lt;10; i++ { item \u0026lt;- i fmt.Printf(\u0026#34;Producer %d produces data: %d\\n\u0026#34;, id, i) time.Sleep(10 * 1e6) } } // 消费者 func Consumer(id int, item chan int) { for i:=0; i\u0026lt;20; i++ { c_item := \u0026lt;-item fmt.Printf(\u0026#34;Consumer %d get data: %d\\n\u0026#34;, id, c_item) time.Sleep(10 * 1e6) } } func main() {item := make(chan int, 6) go Producer(1, item) go Producer(2, item) go Consumer(1, item) time.Sleep(5 * time.Second) } 在写法上，golang 通过 channel 实现了一个忽略异步等待的代码，channel 其实相当于一个队列，当队列中为空时，会阻塞在取队列的操作上，直到可以从中取到内容。而在代码中，生产者和消费者共用了一个 channel item，在生产后放入 item 中，消费者从中读取消费。\n执行结果为：\nProducer 1 produces data: 0 Producer 2 produces data: 0 Consumer 1 get data: 0 Producer 1 produces data: 1 Producer 2 produces data: 1 Consumer 1 get data: 0 Producer 1 produces data: 2 Consumer 1 get data: 1 Producer 2 produces data: 2 .... 再谈 goroutines 的控制 最后一个例子用 golang 自己提供的一个 例子 来探讨下 channel 的使用。\n这个例子也在官方博客中进行了探讨（参考链接 1），其中 result 的是用来保存结果的结构体，sumFiles 函数的功能是遍历目录，计算文件的 MD5 hash 值，MD5All 则是 sumFiles 的上层调用函数，对外提供功能，并且封装结果 channel 数据成字典类型。\nsumFiles 函数传入参数有两个：一个是 done，这个是用于定义结束，另外一个是 root，指定遍历的路径；传出数据为一个 channel 和 error。sumFiles 会将每个文件放到一个 goroutine 中计算 hash 值，并且将结果保存到 result channel 中。 值得注意的是，在检查是否终止遍历时，使用下面的方法来进行了一个检查：\nselect { case \u0026lt;-done: // HL return errors.New(\u0026#34;walk canceled\u0026#34;) default: return nil } 根据 MD5All 中的 defer close(done)，当函数 MD5All 返回时，会关闭 done 这个 channel，这样从 done 中取值就可以始终成功，select 在执行时就可以一直成功，这也对我之前想法中退出 channel 填值有可能比取值少导致阻塞的情况做了一个比较完美的解答。\n参考链接 [1]. Go Concurrency Patterns: Pipelines and cancellation\n","date":"2014-04-29T15:57:46Z","permalink":"https://www.4async.com/2014/04/2014-04-29-more-about-golang-channel/","title":"更多关于 channel 的扯淡"},{"content":"最近有一个需求是在一个常驻内存的程序中结束相关任务。在 Go 中，创建一个 goroutine 非常简单，只需要 go 一下就可以了，但是如果我创建了很多 goroutine，想要结束怎么办？\n比如说我有一个死循环的例子\npackage main import (\u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func main() {queue := make(chan int, 20) var wg sync.WaitGroup for { // 就是一个死循环 queue \u0026lt;- 1 \u0026lt;-queue for i := 0; i \u0026lt;10; i++ {wg.Add(1) go func(i int) {time.Sleep(5 * time.Second) fmt.Println(\u0026#34;Sleep\u0026#34;) wg.Done()}(i) } wg.Wait()} } 如何在一个 goroutine 里面控制所有的 goroutine，让所有的 goroutine 结束呢？这就需要 select 出场了。有人告诉我，这样子实现会更好一些：\npackage main import (\u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func main() { // 定义一个用于指定退出的 channel EXIT := make(chan int, 1) queue := make(chan int, 20) var wg sync.WaitGroup // 启动新的 goroutine go func() {time.Sleep(10 * time.Second) // 休息了之后，该结束了 EXIT \u0026lt;- 1}() for { // 进入死循环 queue \u0026lt;- 1 select { case \u0026lt;-EXIT: // 收到了退出消息 fmt.Println(\u0026#34;KILLED\u0026#34;) return case \u0026lt;-queue: for i := 0; i \u0026lt;10; i++ {wg.Add(1) go func(i int) {time.Sleep(5 * time.Second) fmt.Println(\u0026#34;Sleep\u0026#34;) wg.Done()}(i) } wg.Wait()} } } 但是输出却是比较让人困惑：\nF:\\\u0026gt;go run dada.go Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep Sleep KILLED 看起来停止的时间有 15s，比预想的 10s 时间要长一些，这是为什么呢？\n这里 select 的作用是，在遇到 channel case 时，尝试所有的 channel 是否为 ready；若有一个为 ready，则执行该 case，多个 case 时会随机执行其中一个 case；如果有 default，则会在所有都 not ready 时执行；没有 default 的话就 wait 等待 ready。\n关于 select 的随机性，用一个例子来说明更方便一些：\npackage main import (\u0026#34;time\u0026#34; \u0026#34;fmt\u0026#34; ) func main() { // 定义一个用于指定退出的 channel c1 := make(chan int, 1) c2 := make(chan int, 1) for { c1 \u0026lt;- 1 c2 \u0026lt;- 1 select{ case \u0026lt;-c1: fmt.Println(\u0026#34;c1\u0026#34;) // 防止出现 panic \u0026lt;-c2 time.Sleep(1*time.Second) case \u0026lt;-c2: fmt.Println(\u0026#34;c2\u0026#34;) \u0026lt;-c1 time.Sleep(1*time.Second) } } } 输出结果如下：\ngo run dada.go c1 c1 c2 c1 c1 c2 c1 .... 那这样上面还是会出现那种有可能没退出的情况，这样怎么做呢？下面我个人感觉会是一种更好的做法：\npackage main import (\u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func main() { // 定义一个用于指定退出的 channel EXIT := make(chan int, 1) var wg sync.WaitGroup // 启动新的 goroutine go func() {time.Sleep(10 * time.Second) // 休息了之后，该结束了 EXIT \u0026lt;- 1}() for { select { case \u0026lt;-EXIT: // 收到了退出消息 fmt.Println(\u0026#34;KILLED\u0026#34;) return default: for i := 0; i \u0026lt;10; i++ {wg.Add(1) go func(i int) {time.Sleep(5 * time.Second) fmt.Println(\u0026#34;Sleep\u0026#34;) wg.Done()}(i) } wg.Wait()} } } 还有一个值得注意的事情就是，对 select 来说，整体的运行相当于一个循环分支处理的过程。对 case 来说，过程是一个 block 的过程，比如说在执行 default 过程中，即使收到了来自 EXIT 的信息，也不会中断执行 default 去跳转执行 EXIT，而是在 default 完成之后，进入条件分支选择使优先进入 channel 已经 ready 的 case。\n","date":"2014-04-28T20:57:20Z","permalink":"https://www.4async.com/2014/04/2014-04-28-control-goroutines-by-channel-select/","title":"channel 和 select 控制 goroutines"},{"content":"Github 是个好东西，最早接触 Git，就是因为 Github 接触的。上手简单，丰富的开源库，互动性等等功能都让 Github 充满魅力。\n当然了，一些私密的代码是不适合放在 Github 上的（比如工作代码），这个时候就不方便上传代码到 Github 了。当然了，如果 购买 Github 的 Micro Plan 就可以创建私密的 repo 了。\n不花钱有没有什么好方法？以前有一个 Github 功能大致相同的社区版 Gitlab，基于 Ruby 开发，可是配置实在是太过于复杂，也不适合一个人单独打包使用。不过后来遇到了 gogs，这个问题就解决了。其实刚开始知道 gogs 的时候，还不支持 SQLite3，最新的一个版本已经支持了，不过只有 x64 位版本，基于 golang 构建，因此也没有什么依赖需要安装，只需要简单的：下载、解压、打开、配置，然后你就可以使用 git 服务了。\n下载地址：https://github.com/gogits/gogs/wiki/Install-from-binary\n打开压缩包，以 Windows/SQLite3 为例演示安装步骤：\n解压压缩包，双击执行 start.bat 文件（Linux/Mac 下是 start.sh） 打开浏览器，浏览 http://127.0.0.1:3000 打开自动跳转至安装界面，内容一共只有下图这么多！很简单，我就不说啥了。如果你想多机同步，建议你将 SQLite 数据库和 repo 地址放在网盘内，比如我就放在 Dropbox 下了。 点击 Install Gogs，安装完成～ 比如我现在是自己在用，可能需要一些单独的配置：\n打开 gogs 的目录，custom/conf/app.ini 文件，使用 SublimeText 之类的编辑器打开，不要使用记事本，这是个教训\u0026hellip; 按照下面修改之后就只允许本机访问了（反正只有我自己用），注意 Mac 和 Linux 下开 80 需要 root： [server] PROTOCOL = http DOMAIN = localhost ROOT_URL = `http://localhost/` HTTP_ADDR = 127.0.0.1 HTTP_PORT = 80 重新执行 start.bat，看看自己的成果吧！配合 SourceTreee 之类的程序，效果更佳：）看下我的 gogs 吧。\n如果想修改样式或者翻译之类的，在 template 目录下的为 gogs 的模板文件，可以进行修改，我想要的可是和 Github 同样的效果：）\n","date":"2014-04-22T20:09:46Z","permalink":"https://www.4async.com/2014/04/2014-04-22-build-your-private-git-serivce/","title":"基于 Gogs 实现的私密 Git 服务"},{"content":"正好群里面有人在讨论这个，就单独拿出来记录一下，下次就不用单独跑了。结果只是包含常见的算法性能。\n说到常用的算法，之前 QQ 的协议中最早使用的是 xxTEA 算法 [^1]，运算效率高，但是同样的，被破解的几率随着系统硬件性能的提升和手段的进化破解难度大幅降低，所以后来更换成了 blowfish 的算法 [^2]，从效率上来说可以接受，并且强度也是可以接受的范围。当然了，至于密钥，那是另外一个话题了。\nOpenSSL 1.0.1g 7 Apr 2014 built on: Mon Apr 7 19:24:10 BST 2014 options:bn(64,64) rc4(ptr,char) des(idx,cisc,16,int) aes(partial) idea(int) blowfish(idx) compiler: clang -fPIC -fno-common -DOPENSSL_PIC -DZLIB_SHARED -DZLIB -DOPENSSL_THREADS -D_REENTRANT -DDSO_DLFCN -DHAVE_DLFCN_H -arch x86_64 -O3 -DL_ENDIAN -Wall -DOPENSSL_IA32_SSE2 -DOPENSSL_BN_ASM_MONT -DOPENSSL_BN_ASM_MONT5 -DOPENSSL_BN_ASM_GF2m -DSHA1_ASM -DSHA256_ASM -DSHA512_ASM -DMD5_ASM -DAES_ASM -DVPAES_ASM -DBSAES_ASM -DWHIRLPOOL_ASM -DGHASH_ASM The \u0026#39;numbers\u0026#39; are in 1000s of bytes per second processed. type 16 bytes 64 bytes 256 bytes 1024 bytes 8192 bytes md5 27012.47k 83668.96k 193662.90k 288870.40k 339611.95k hmac(md5) 22436.27k 72562.60k 176476.05k 277375.28k 337510.40k sha1 30214.31k 86946.66k 183855.45k 259266.33k 297429.61k rc4 196157.50k 211073.87k 232483.09k 233261.75k 235103.97k des cbc 36183.51k 37559.07k 38393.90k 38166.10k 38460.60k des ede3 14248.36k 14433.49k 14513.37k 14586.23k 14629.00k idea cbc 29515.76k 30455.99k 30819.81k 30886.49k 30909.05k seed cbc 42818.52k 46288.05k 47289.80k 47466.82k 47201.72k blowfish cbc 61676.43k 64869.07k 66887.52k 66720.19k 66823.31k aes-128 cbc 53756.63k 58966.27k 59666.27k 126935.19k 127531.60k aes-192 cbc 46063.28k 50037.52k 50891.14k 109122.65k 109760.48k aes-256 cbc 40039.57k 42316.96k 43542.15k 93052.84k 93731.72k sha256 24619.03k 53308.64k 90030.81k 109692.87k 117423.88k sha512 19076.43k 74213.38k 114250.08k 162023.40k 183772.75k whirlpool 15034.42k 31777.43k 52066.33k 61904.05k 56250.74k aes-128 ige 53897.17k 56852.04k 57523.48k 57741.98k 57742.06k aes-192 ige 45658.29k 47596.69k 48166.72k 48698.26k 48257.35k aes-256 ige 40110.70k 41541.96k 41726.01k 41792.81k 41839.56k sign verify sign/s verify/s rsa 512 bits 0.000169s 0.000015s 5921.3 64569.3 rsa 1024 bits 0.000554s 0.000035s 1805.6 28181.2 rsa 2048 bits 0.003531s 0.000112s 283.2 8923.1 rsa 4096 bits 0.025068s 0.000405s 39.9 2466.8 sign verify sign/s verify/s dsa 512 bits 0.000145s 0.000153s 6890.8 6518.5 dsa 1024 bits 0.000338s 0.000393s 2961.0 2547.0 dsa 2048 bits 0.001081s 0.001332s 925.1 750.6 刚刚开始那系统自带的去跑，发现干脆升级到最新的 OpenSSL 跑一下吧。结果跑了两遍之后，Air 直接烫手了，这种测试也就做一次就够了吧。\n[^1] http://bbs.pediy.com/showthread.php?t=11312\n[^2] http://blog.csdn.net/qinggebuyao/article/details/7814499\n","date":"2014-04-11T13:19:46Z","permalink":"https://www.4async.com/2014/04/2014-04-11-openssl-algorithm-speed-result/","title":"几种常见加密 / hash 算法效率 (OpenSSL)"},{"content":"最近升级了一下 simplejson，发现 simplejson 的 speedup 模块报了一个错误：\nInstalling collected packages: simplejson Found existing installation: simplejson 3.3.1 Uninstalling simplejson: Successfully uninstalled simplejson Running setup.py install for simplejson building \u0026#39;simplejson._speedups\u0026#39; extension cc -fno-strict-aliasing -fno-common -dynamic -arch x86_64 -arch i386 -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -mno-fused-madd -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32 -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -DENABLE_DTRACE -arch x86_64 -arch i386 -pipe -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c simplejson/_speedups.c -o build/temp.macosx-10.9-intel-2.7/simplejson/_speedups.o clang: error: unknown argument: \u0026#39;-mno-fused-madd\u0026#39; [-Wunused-command-line-argument-hard-error-in-future] clang: note: this will be a hard error (cannot be downgraded to a warning) in the future *************************************************************************** WARNING: The C extension could not be compiled, speedups are not enabled. Failure information, if any, is above. I\u0026#39;m retrying the build without the C extension now. *************************************************************************** *************************************************************************** WARNING: The C extension could not be compiled, speedups are not enabled. Plain-Python installation succeeded. *************************************************************************** Successfully installed simplejson Cleaning up... 注意看 error 这里，有一个 unknown argument: '-mno-fused-madd' 的错误，应该是 Xcode 升级最新的版本之后，clang 替换带来的问题。\n解决方法也很简单，在编译之前 export 两个参数即可：\n$ export CFLAGS=-Qunused-arguments $ export CPPFLAGS=-Qunused-arguments $ pip install simplejson Downloading/unpacking simplejson Downloading simplejson-3.4.0.tar.gz (68kB): 68kB downloaded Running setup.py egg_info for package simplejson Installing collected packages: simplejson Running setup.py install for simplejson building \u0026#39;simplejson._speedups\u0026#39; extension cc -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -Qunused-arguments -Qunused-arguments -arch x86_64 -arch i386 -pipe -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c simplejson/_speedups.c -o build/temp.macosx-10.9-intel-2.7/simplejson/_speedups.o cc -bundle -undefined dynamic_lookup -arch x86_64 -arch i386 -Wl,-F. -Qunused-arguments -Qunused-arguments build/temp.macosx-10.9-intel-2.7/simplejson/_speedups.o -o build/lib.macosx-10.9-intel-2.7/simplejson/_speedups.so Successfully installed simplejson Cleaning up... 后来查了一下具体的原因，在 Xcode5.1 的 release note 中有提及：\nThe Apple LLVM compiler in Xcode 5.1 treats unrecognized command-line options as errors. This issue has been seen when building both Python native extensions and Ruby Gems, where some invalid compiler options are currently specified. Projects using invalid compiler options will need to be changed to remove those options. To help ease that transition, the compiler will temporarily accept an option to downgrade the error to a warning: -Wno-error=unused-command-line-argument-hard-error-in-future 官方推荐的方法是：\n$ ARCHFLAGS=-Wno-error=unused-command-line-argument-hard-error-in-future pip install simplejson Downloading/unpacking simplejson Downloading simplejson-3.4.0.tar.gz (68kB): 68kB downloaded Running setup.py egg_info for package simplejson Installing collected packages: simplejson Running setup.py install for simplejson building \u0026#39;simplejson._speedups\u0026#39; extension cc -fno-strict-aliasing -fno-common -dynamic -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -mno-fused-madd -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32 -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -DENABLE_DTRACE -Wno-error=unused-command-line-argument-hard-error-in-future -pipe -Wno-error=unused-command-line-argument-hard-error-in-future -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c simplejson/_speedups.c -o build/temp.macosx-10.9-intel-2.7/simplejson/_speedups.o clang: warning: unknown argument: \u0026#39;-mno-fused-madd\u0026#39; [-Wunused-command-line-argument-hard-error-in-future] clang: note: this will be a hard error (cannot be downgraded to a warning) in the future clang: warning: argument unused during compilation: \u0026#39;-mno-fused-madd\u0026#39; cc -bundle -undefined dynamic_lookup -Wl,-F. -Wno-error=unused-command-line-argument-hard-error-in-future -Wno-error=unused-command-line-argument-hard-error-in-future build/temp.macosx-10.9-intel-2.7/simplejson/_speedups.o -o build/lib.macosx-10.9-intel-2.7/simplejson/_speedups.so Successfully installed simplejson Cleaning up... 最后来个轻量级的性能测试 (python 2.7.5)：\n#!/usr/bin/env python # -*- coding: utf-8 -*- from timeit import Timer import json import simplejson def by_json(): a = {\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;c\u0026#34;: [1, 2, 3], \u0026#34;d\u0026#34;: [\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;]} b = json.dumps(a) a = json.loads(b) def by_simplejson(): a = {\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;c\u0026#34;: [1, 2, 3], \u0026#34;d\u0026#34;: [\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;]} b = simplejson.dumps(a) a = simplejson.loads(b) if __name__ == \u0026#39;__main__\u0026#39;: t = Timer(stmt=\u0026#34;by_json()\u0026#34;, setup=\u0026#34;from __main__ import by_json\u0026#34;) print \u0026#39;by json: %s seconds\u0026#39; % t.timeit(number=3) t = Timer(stmt=\u0026#34;by_simplejson()\u0026#34;, setup=\u0026#34;from __main__ import by_simplejson\u0026#34;) print \u0026#39;by simplejson: %s seconds\u0026#39; % t.timeit(number=3) 测试结果表明：恩，实际上不用 simplejson 也可以，不会有很大性能差异：\n$ python json_speed.py by json: 0.000191926956177 seconds by simplejson: 0.000169992446899 seconds ","date":"2014-04-08T11:43:52Z","permalink":"https://www.4async.com/2014/04/2014-04-08-fix-build-python-ext-on-mac-os-x-mavericks-with-error-mno-fused-madd/","title":"解决 Mavericks 系统上 Python 库 - mno-fused-madd 错误"},{"content":"之前一直是 Flask 的忠实粉丝，然后嘲讽下 Django 党；结果被 falcon 打脸，恩，下面是我的真机实际测试（i5-3230M 2.6G 8G RAM, SSD。乱码的是什么呢\u0026hellip;.）：\nBenchmarking, Trial 1 of 3......done. Benchmarking, Trial 2 of 3......done. Benchmarking, Trial 3 of 3......done. Results: 1. falcon.........34,165 req/sec or 29.27 渭 s/req (9x) 2. falcon-ext.....25,752 req/sec or 38.83 渭 s/req (7x) 3. bottle.........17,043 req/sec or 58.67 渭 s/req (5x) 4. pecan...........7,915 req/sec or 126.34 渭 s/req (2x) 5. werkzeug........6,241 req/sec or 160.24 渭 s/req (2x) 6. flask...........3,762 req/sec or 265.81 渭 s/req (1x) 这样看起来，完全可以 flask 做前台，api 使用 falcon 来实现（或者 Beego，同时还能保证开发效率）。\nFalcon 实在是太小巧了，没啥好讲的，一个例子就足够了：\nimport falcon from gevent.wsgi import WSGIServer class URLApi(object): def on_get(self, req, resp): \u0026#34;\u0026#34;\u0026#34;Handles GET requests\u0026#34;\u0026#34;\u0026#34; result = {\u0026#34;errno\u0026#34;: 0, \u0026#34;details\u0026#34;: \u0026#34;hello\u0026#34;} resp.status = falcon.HTTP_200 resp.body = json.dumps(result) def on_post(self, req, resp): \u0026#34;\u0026#34;\u0026#34;Handles POST requests\u0026#34;\u0026#34;\u0026#34; data = json.load(req.stream,\u0026#39;utf-8\u0026#39;) result = {\u0026#34;errno\u0026#34;: 0, \u0026#34;details\u0026#34;: \u0026#34;hello\u0026#34;} resp.status = falcon.HTTP_200 resp.body = json.dumps(result) app = falcon.API() api = URLApi() app.add_route(\u0026#39;/url/\u0026#39;, api) if __name__ == \u0026#39;__main__\u0026#39;: server = WSGIServer((\u0026#39;0.0.0.0\u0026#39;, 5000), app) server.serve_forever() 这个框架目标是提供高效的 API 接口，非常合适大量并发的 API 接口开发，同时借助 Python 的开发效率，还是不错的选择。\n","date":"2014-04-02T19:47:51Z","permalink":"https://www.4async.com/2014/04/2014-04-02-falcon-web-framework/","title":"短小精悍 Falcon"},{"content":"Beego 框架不错，个人比较喜欢，关键是还有一些开发工具很实用，封装了很多实现，不需要自己做很多工作，在不考虑效率的前提下，都是可以接受的范畴。\n自定义错误页面 Beego 默认自己带了一些错误页面，在 error.go 文件中进行了定义。以 404 为例：\n// show 404 notfound error. func NotFound(rw http.ResponseWriter, r *http.Request) {t, _ := template.New(\u0026#34;beegoerrortemp\u0026#34;).Parse(errtpl) data := make(map[string]interface{}) data[\u0026#34;Title\u0026#34;] = \u0026#34;Page Not Found\u0026#34; data[\u0026#34;Content\u0026#34;] = template.HTML(\u0026#34;\u0026lt;br\u0026gt;The page you have requested has flown the coop.\u0026#34;+\u0026#34;\u0026lt;br\u0026gt;Perhaps you are here because:\u0026#34;+\u0026#34;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\u0026lt;ul\u0026gt;\u0026#34; + \u0026#34;\u0026lt;br\u0026gt;The page has moved\u0026#34;+\u0026#34;\u0026lt;br\u0026gt;The page no longer exists\u0026#34;+\u0026#34;\u0026lt;br\u0026gt;You were looking for your puppy and got lost\u0026#34;+\u0026#34;\u0026lt;br\u0026gt;You like 404 pages\u0026#34;+\u0026#34;\u0026lt;/ul\u0026gt;\u0026#34;) data[\u0026#34;BeegoVersion\u0026#34;] = VERSION //rw.WriteHeader(http.StatusNotFound) t.Execute(rw, data) } 其实自己做一个修改也很容易：\nfunc page_not_found(rw http.ResponseWriter, r *http.Request){t,_:= template.New(\u0026#34;404.html\u0026#34;).ParseFiles(beego.ViewsPath+\u0026#34;/404.html\u0026#34;) data :=make(map[string]interface{}) data[\u0026#34;content\u0026#34;] = \u0026#34;page not found\u0026#34; t.Execute(rw, data) } func main() {beego.Errorhandler(\u0026#34;404\u0026#34;,page_not_found) beego.Router(\u0026#34;/\u0026#34;, \u0026amp;controllers.MainController{}) beego.Run()} Server 字段的修改 HTTP 返回头中的 Server 总是显示的是 BeegoServer，太不专业了，定义在 config.go 文件中：\nBeegoServerName = \u0026quot;beegoServer\u0026quot; 我最初是 fork 了一个 repo 改，后来发现，只需要在 app.conf 文件中定义一下就可以了：\nBeegoServerName = ACGSOSERVER 这样 Server 这个字段就更改掉了，其他的属性也可以用这种方式定义掉。\n","date":"2014-04-01T18:46:51Z","permalink":"https://www.4async.com/2014/04/2014-04-01-beego-tips/","title":"Beego 的几个小 Tips"},{"content":"之前我在 Python 开发实践 中曾经提到过通过 python distribute 实现包分发。但是一直没有机会研究如何实现 (没错\u0026hellip;)，今天就粗略的通过一个例子简单描述一下一个包的制作：\n# setup.py #!/usr/bin/env python # -*- coding: utf-8 -*- from setuptools import setup, find_packages import sys # 兼容旧版本 setuptools extra = {} if sys.version_info \u0026gt;= (3,): extra[\u0026#39;use_2to3\u0026#39;] = True extra[\u0026#39;convert_2to3_doctests\u0026#39;] = [\u0026#39;src/your/module/README.txt\u0026#39;] extra[\u0026#39;use_2to3_fixers\u0026#39;] = [\u0026#39;your.fixers\u0026#39;] setup( # 包名称 name=\u0026#39;your.module\u0026#39;, # 版本号 version = \u0026#39;1.0\u0026#39;, # 包描述 description=\u0026#39;This is your awesome module\u0026#39;, # 作者名称 author=\u0026#39;You\u0026#39;, # 作者 URL author_email=\u0026#39;your@email\u0026#39;, # 如果有需要 license = \u0026#34;PSF\u0026#34;, # 如果有需要 url = \u0026#34;\u0026#34;, # 依赖包 install_requires=[], # 源代码目录 package_dir = {\u0026#39;\u0026#39;:\u0026#39;src\u0026#39;}, # 包文件 packages = find_packages(), # 包含其他必须文件，前面是文件夹，后面是扩展名，将会按此格式排列文件 package_data = {# include them:\u0026#39;\u0026#39;: [\u0026#39;*.txt\u0026#39;, \u0026#39;*.rst\u0026#39;], }, # 单元测试 test_suite = \u0026#39;your.module.tests\u0026#39;, **extra ) 在 setuptools \u0026gt;= 0.7 的版本上，distribute 的功能已经被整合进入 Setuptools，例子也是适合于 0.7 以上版本的配置。上面这个例子适用于纯 Python 的包，很多时候我们需要做一些对 Python 的性能提升 (比如动态库 binding)，怎么做呢？这是下一篇文要写的了\u0026hellip;\n","date":"2014-03-28T11:33:51Z","permalink":"https://www.4async.com/2014/03/2014-03-28-python-distribute-building-part-one/","title":"Python 分发包制作（一）"},{"content":"话不多说，自己看吧，比较明白。\npackage main import (\u0026#34;crypto\u0026#34; \u0026#34;crypto/rand\u0026#34; \u0026#34;crypto/rsa\u0026#34; \u0026#34;crypto/sha256\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;encoding/base64\u0026#34; \u0026#34;encoding/pem\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; ) func main() {signer, err := loadPrivateKey(\u0026#34;private.pem\u0026#34;) if err != nil {fmt.Errorf(\u0026#34;signer is damaged: %v\u0026#34;, err) } toSign := \u0026#34;date: Thu, 05 Jan 2012 21:31:40 GMT\u0026#34; signed, err := signer.Sign([]byte(toSign)) if err != nil {fmt.Errorf(\u0026#34;could not sign request: %v\u0026#34;, err) } sig := base64.StdEncoding.EncodeToString(signed) fmt.Printf(\u0026#34;Signature: %v\\n\u0026#34;, sig) parser, perr := loadPublicKey(\u0026#34;public.pem\u0026#34;) if perr != nil {fmt.Errorf(\u0026#34;could not sign request: %v\u0026#34;, err) } err = parser.Unsign([]byte(toSign), signed) if err != nil {fmt.Errorf(\u0026#34;could not sign request: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Unsign error: %v\\n\u0026#34;, err) } // loadPrivateKey loads an parses a PEM encoded private key file. func loadPublicKey(path string) (Unsigner, error) {return parsePublicKey([]byte(`-----BEGIN PUBLIC KEY----- MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDCFENGw33yGihy92pDjZQhl0C3 6rPJj+CvfSC8+q28hxA161QFNUd13wuCTUcq0Qd2qsBe/2hFyc2DCJJg0h1L78+6 Z4UMR7EOcpfdUE9Hf3m/hs+FUR45uBJeDK1HSFHD8bHKD6kv8FPGfJTotc+2xjJw oYi+1hqp1fIekaxsyQIDAQAB -----END PUBLIC KEY-----`)) } // parsePublicKey parses a PEM encoded private key. func parsePublicKey(pemBytes []byte) (Unsigner, error) {block, _ := pem.Decode(pemBytes) if block == nil {return nil, errors.New(\u0026#34;ssh: no key found\u0026#34;)} var rawkey interface{} switch block.Type {case\u0026#34;PUBLIC KEY\u0026#34;: rsa, err := x509.ParsePKIXPublicKey(block.Bytes) if err != nil {return nil, err} rawkey = rsa default: return nil, fmt.Errorf(\u0026#34;ssh: unsupported key type %q\u0026#34;, block.Type) } return newUnsignerFromKey(rawkey) } // loadPrivateKey loads an parses a PEM encoded private key file. func loadPrivateKey(path string) (Signer, error) {return parsePrivateKey([]byte(`-----BEGIN RSA PRIVATE KEY----- MIICXgIBAAKBgQDCFENGw33yGihy92pDjZQhl0C36rPJj+CvfSC8+q28hxA161QF NUd13wuCTUcq0Qd2qsBe/2hFyc2DCJJg0h1L78+6Z4UMR7EOcpfdUE9Hf3m/hs+F UR45uBJeDK1HSFHD8bHKD6kv8FPGfJTotc+2xjJwoYi+1hqp1fIekaxsyQIDAQAB AoGBAJR8ZkCUvx5kzv+utdl7T5MnordT1TvoXXJGXK7ZZ+UuvMNUCdN2QPc4sBiA QWvLw1cSKt5DsKZ8UETpYPy8pPYnnDEz2dDYiaew9+xEpubyeW2oH4Zx71wqBtOK kqwrXa/pzdpiucRRjk6vE6YY7EBBs/g7uanVpGibOVAEsqH1AkEA7DkjVH28WDUg f1nqvfn2Kj6CT7nIcE3jGJsZZ7zlZmBmHFDONMLUrXR/Zm3pR5m0tCmBqa5RK95u 412jt1dPIwJBANJT3v8pnkth48bQo/fKel6uEYyboRtA5/uHuHkZ6FQF7OUkGogc mSJluOdc5t6hI1VsLn0QZEjQZMEOWr+wKSMCQQCC4kXJEsHAve77oP6HtG/IiEn7 kpyUXRNvFsDE0czpJJBvL/aRFUJxuRK91jhjC68sA7NsKMGg5OXb5I5Jj36xAkEA gIT7aFOYBFwGgQAQkWNKLvySgKbAZRTeLBacpHMuQdl1DfdntvAyqpAZ0lY0RKmW G6aFKaqQfOXKCyWoUiVknQJAXrlgySFci/2ueKlIE1QqIiLSZ8V8OlpFLRnb1pzI 7U1yQXnTAEFYM560yJlzUpOb1V4cScGd365tiSMvxLOvTA== -----END RSA PRIVATE KEY-----`)) } // parsePublicKey parses a PEM encoded private key. func parsePrivateKey(pemBytes []byte) (Signer, error) {block, _ := pem.Decode(pemBytes) if block == nil {return nil, errors.New(\u0026#34;ssh: no key found\u0026#34;)} var rawkey interface{} switch block.Type {case\u0026#34;RSA PRIVATE KEY\u0026#34;: rsa, err := x509.ParsePKCS1PrivateKey(block.Bytes) if err != nil {return nil, err} rawkey = rsa default: return nil, fmt.Errorf(\u0026#34;ssh: unsupported key type %q\u0026#34;, block.Type) } return newSignerFromKey(rawkey) } // A Signer is can create signatures that verify against a public key. type Signer interface { // Sign returns raw signature for the given data. This method // will apply the hash specified for the keytype to the data. Sign(data []byte) ([]byte, error) } // A Signer is can create signatures that verify against a public key. type Unsigner interface { // Sign returns raw signature for the given data. This method // will apply the hash specified for the keytype to the data. Unsign(data[]byte, sig []byte) error } func newSignerFromKey(k interface{}) (Signer, error) { var sshKey Signer switch t := k.(type) { case *rsa.PrivateKey: sshKey = \u0026amp;rsaPrivateKey{t} default: return nil, fmt.Errorf(\u0026#34;ssh: unsupported key type %T\u0026#34;, k) } return sshKey, nil } func newUnsignerFromKey(k interface{}) (Unsigner, error) { var sshKey Unsigner switch t := k.(type) { case *rsa.PublicKey: sshKey = \u0026amp;rsaPublicKey{t} default: return nil, fmt.Errorf(\u0026#34;ssh: unsupported key type %T\u0026#34;, k) } return sshKey, nil } type rsaPublicKey struct {*rsa.PublicKey} type rsaPrivateKey struct {*rsa.PrivateKey} // Sign signs data with rsa-sha256 func (r *rsaPrivateKey) Sign(data []byte) ([]byte, error) {h := sha256.New() h.Write(data) d := h.Sum(nil) return rsa.SignPKCS1v15(rand.Reader, r.PrivateKey, crypto.SHA256, d) } // Unsign verifies the message using a rsa-sha256 signature func (r *rsaPublicKey) Unsign(message []byte, sig []byte) error {h := sha256.New() h.Write(message) d := h.Sum(nil) return rsa.VerifyPKCS1v15(r.PublicKey, crypto.SHA256, d, sig) } ","date":"2014-03-23T20:25:52Z","permalink":"https://www.4async.com/2014/03/2014-03-23-golang-rsa/","title":"Golang 的 RSA 公私钥方法实现"},{"content":"以前一直写的是后台的业务系统，数据不需要直接的实时返回，经常不会有什么队列方面的需求。最近有个关于交互的演示项目需要实时返回结果，但是中间几个过程都需要大量的交互（网络通讯），因此这个时候就需要引入消息队列。相对来说，最简单的 Python 队列实现就是 Celery。\nCelery 的使用比较简单，但是需要后端的数据库做消息队列存储机制，这个一直没想明白，看文档的时候想当然的以为不需要后端也行的。假如没有后端的话，很有可能会不能实现功能。\n比如我有一个上传文件的要求：\nr = requests.post(url, files=fh) 这个请求可能需要大量的时间实现上传，应用又有体验的原因需要实时返回结果，因此实现的方式就可以选择队列实现。更加普遍的场景还有发送邮件、短信等等的场景。\n首先是编写 Celery 的 Worker 进程。这个进程是作为队列的后端处理程序存在的，我在选择后端时使用的是 Redis，主要是比较方便，amq 安装相对麻烦。有人曾经反应说 Redis 的支持并不是十分完善，但是我看文档中并未提到，因此在实际生产环境的时候注意一下这个坑，如果发现 Redis 有问题，可以果断迁移至 amq，或者直接用它做后端。\n# tasks.py from celery import Celery redis_url = \u0026#39;redis://localhost:6379/0\u0026#39; app = Celery(\u0026#39;tasks\u0026#39;, backend=redis_url, broker=redis_url) # Celery 第一个参数同文件名 # 新异步任务 @app.task def upload_file(url, fh): r = requests.post(url, files=fh) 编写完成后就可以运行 Celery 的 Worker 进程：\ncelery -A tasks worker 就可以执行 Worker 进程，记得同时启动数据库：）\n然后写一下前段处理模块：\nfrom task import upload_file result = upload_file.delay(url, fh) # 延迟执行 print result.ready() # 获取运行状态 print result.get(timeout=10) # 获取运行结果 ","date":"2014-03-23T19:58:42Z","permalink":"https://www.4async.com/2014/03/2014-03-23-python-celery-queue-first-step/","title":"初识 Celery"},{"content":"第一次用 python encode utf-8 的中文，结果发现了一个蛋疼的问题\nimport base64 base64.b64encode(u\u0026#39; 你好世界 \u0026#39;) Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; File \u0026#34;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/base64.py\u0026#34;, line 53, in b64encode encoded = binascii.b2a_base64(s)[:-1] UnicodeEncodeError: \u0026#39;ascii\u0026#39; codec can\u0026#39;t encode characters in position 0-3: ordinal not in range(128) 这个问题在 2008 年就提出来过，这个 Base64 的这个库的实现是按照 RFC 3548 实现的，仅按照 byte 和 ascii 字符，所以会出现这个问题。如果修复这个问题的话，就是把 unicode 字符换成 byte 就可以正常了。\n_str = u\u0026#34;你好世界\u0026#34; encoded = base64.b64encode(_str.encode(\u0026#39;utf-8\u0026#39;)) print encoded print base64.b64decode(encoded).decode(\u0026#39;utf-8\u0026#39;) ","date":"2014-03-13T21:50:29Z","permalink":"https://www.4async.com/2014/03/2014-03-13-python-base64-encode-utf-8error/","title":"Python 中文 UTF-8 编码 base64 报错"},{"content":"在上一篇文章中，我提到过 cProfile 对 Python 进行调优，但是仅仅只是简单的一笔带过，这篇文章就针对这个内容，单独扩展一下。cProfile 是 Python 的性能测试工具，另外一个同类工具是 python 实现的 profile，不过 cProfile 是 C 语言扩展的实现。\n官方文档页面：http://docs.python.org/2/library/profile.html#module-cProfile。\n先从一个例子开始：\n#!/usr/bin/env python # -*- coding: utf-8 -*- import time def delay(): time.sleep(10) def main(): delay() print \u0026#34;123\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: main() 使用下面这个命令就可以快速诊断：\npython -m cProfile profile_run.py 输出如下：\n5 function calls in 10.002 seconds Ordered by: standard name ncalls tottime percall cumtime percall filename:lineno(function) 1 0.002 0.002 10.002 10.002 profile_run.py:11(main) 1 0.000 0.000 10.002 10.002 profile_run.py:4(\u0026lt;module\u0026gt;) 1 0.000 0.000 10.000 10.000 profile_run.py:7(delay) \u0026lt;-- 调用者这里 1 0.000 0.000 0.000 0.000 {method'disable'of'_lsprof.Profiler'objects} 1 10.000 10.000 10.000 10.000 {time.sleep} \u0026lt;--- 元凶在这里 如果想输出到文件，添加一个 - o 参数即可：\npython -m cProfile -o profile_output profile_run.py 若想按照某列排序，则可以添加这个参数：\npython -m cProfile -o profile_output -s tottime profile_run.py 那么等等，这个结果文件如何识别呢：\nncalls: 这个函数被调用的次数\ntottime：函数的具体执行时间，不包含其他函数消耗的时间\npercall：每次调用的消耗（包含其他函数消耗和不包含的时间）\ncumtime：程序消耗的总时间\n然后看一个实际的成果看一下结果如何，具体的诊断数据：\n发现最慢的执行模块是 not_operation 这个函数：\ndef not_operation(operand, dictionary, pfile): \u0026#34;\u0026#34;\u0026#34;Performs the operation `NOT operand`.\u0026#34;\u0026#34;\u0026#34; # A list of all the documents (sorted) all_docs = dictionary.all_docs() # A list of the documents matching `operand` (sorted) results = get_results(operand, dictionary, pfile, force_list=True) return [doc for doc in all_docs if doc not in results] 接下来是怎么修复了：\n# the fix. def not_operation(operand, dictionary, pfile): \u0026#34;\u0026#34;\u0026#34;Performs the operation `NOT operand`.\u0026#34;\u0026#34;\u0026#34; # A list of all the documents (sorted) all_docs = dictionary.all_docs() # A list of the documents matching `operand` (sorted) results = get_results(operand, dictionary, pfile, force_list=True) return list_a_and_not_list_b(all_docs, results) def list_a_and_not_list_b(a, b): \u0026#34;\u0026#34;\u0026#34;Returns `a AND NOT b`. Both a and b are expected to be sorted lists. \u0026#34;\u0026#34;\u0026#34;results = [] idx_a = 0 idx_b = 0 while idx_a \u0026lt;len(a) and idx_b \u0026lt;len(b): if a[idx_a] \u0026lt;b[idx_b]: results.append(a[idx_a]) idx_a += 1 elif b[idx_b] \u0026lt;a[idx_a]: idx_b += 1 else: idx_a += 1 idx_b += 1 while idx_a \u0026lt;len(a): results.append(a[idx_a]) idx_a += 1 对了，说起来，上面图片查看工具是 cprofilev，是针对 cProfile 的查看工具，使用命令安装：\npip install bottle cprofilev cprofilev profile_output 如果在 Windows 安装默认有点问题，你如果想解决，可以自己看一下:)\n","date":"2014-03-09T23:04:00Z","permalink":"https://www.4async.com/2014/03/2014-03-09-python-profile-using-cprofile/","title":"利用 cProfile 进行 Python 程序性能调优"},{"content":"本来还想继续翻译：https://wiki.python.org/moin/PythonSpeed/PerformanceTips，但是事情太多真的没时间，那么就简单的总结一下吧。\n字符串拼接 应该避免使用：\ns = \u0026#34;\u0026#34; for substring in list: s += substring 选择使用：\ns = \u0026#34;\u0026#34;.join(list) 对于大字符串列表，避免使用：\ns = \u0026#34;\u0026#34; for x in list: s += some_function(x) 使用：\nslist = [some_function(elt) for elt in somelist] s = \u0026#34;\u0026#34;.join(slist) 避免使用：\nout = \u0026#34;\u0026lt;html\u0026gt;\u0026#34; + head + prologue + query + tail + \u0026#34;\u0026lt;/html\u0026gt;\u0026#34; 使用：\nout = \u0026#34;\u0026lt;html\u0026gt;%s%s%s%s\u0026lt;/html\u0026gt;\u0026#34; % (head, prologue, query, tail) 当然了，这种更容易读：\nout = \u0026#34;\u0026lt;html\u0026gt;%(head)s%(prologue)s%(query)s%(tail)s\u0026lt;/html\u0026gt;\u0026#34; % locals() 循环 列表里面的每个字符串都变成大写？\nnewlist = [] for word in oldlist: newlist.append(word.upper()) 试试这种方法：\nnewlist = map(str.upper, oldlist)) 或者是：\niterator = (s.upper() for s in oldlist) 避免点号 比如说我们想把一个列表一个大写之后到一个新列表，试试下面的：\nupper = str.upper newlist = [] append = newlist.append for word in oldlist: append(upper(word)) 本地变量 Python 在访问本地变量时的速度要比全局变量速度更快，比如：\ndef func(): upper = str.upper newlist = [] append = newlist.append for word in oldlist: append(upper(word)) return newlist 初始化字典元素 比如你在初始化一个字典，字典中每个元素是单词和标号：\nwdict = {} for word in words: if word not in wdict: wdict[word] = 0 wdict[word] += 1 那么试试 try-except 吧：\nwdict = {} for word in words: try: wdict[word] += 1 except KeyError: wdict[word] = 1 或者再换一种方式：\nwdict = {} get = wdict.get for word in words: wdict[word] = get(word, 0) + 1 头部引入 def doit1(): import string ###### import statement inside function string.lower(\u0026#39;Python\u0026#39;) for num in range(100000): doit1() import string ###### import statement outside function def doit2(): string.lower(\u0026#39;Python\u0026#39;) for num in range(100000): doit2() doit2 要快一些。\n数据聚合 import time x = 0 def doit1(i): global x x = x + i list = range(100000) t = time.time() for i in list: doit1(i) print \u0026#34;%.3f\u0026#34; % (time.time()-t) vs import time x = 0 def doit2(list): global x for i in list: x = x + i list = range(100000) t = time.time() doit2(list) print \u0026#34;%.3f\u0026#34; % (time.time()-t) 对比一下性能，第二种方法要更快一点\nxrange 替代 range 恩，xrage 更快一些。\n善用 cProfile 性能诊断的好工具\n","date":"2014-03-03T20:30:00Z","permalink":"https://www.4async.com/2014/03/2014-02-18-benchmark-python-2/","title":"Python 性能技巧 (二)"},{"content":"翻译自：https://wiki.python.org/moin/PythonSpeed/PerformanceTips\n该页面致力于提供帮助提高你的 Python 程序的性能各种技巧和窍门。无论信息来自何人, 我试图鉴别来源。\nPython 在我 1996 年写过 “fast python” 页面后已经发生了很多重要变化，这意味着一些命令会发生变化。我将这个页面迁移到 Python wiki, 希望其他人会帮助维护它。\n在使用这些技巧时你仍需进行针对特定版本 Python 和你程序的测试，而不是盲目的接受一个方法快于另外一种。在分析时你会看到更多的细节。\n通同样更新的你可以用类似 Cython、Pyrex、Psyco、Weave、Shed Skin 和 PyInline 之类编写包，通过将性能关键代码转换为 C 或机器语言极大提升应用性能。\n概述： 优化你需要优化的 你只能在获取第一个正确结果运行获取输出时知道你的程序是否运行很慢。当发现运行缓慢, 分析可以显示程序的哪些部分消耗的大部分时间。全面的快速测试套件可以确保未来的优化不改变程序的正确性。简单来说：\n正常获取结果 测试是否正确 如果慢则进行分析 优化 重复步骤 2 一些优化方法是良好的编程风格, 因此你应该学习学习他们的方式。一个例子将是不通过循环移动计算的值。\n选择正确的数据结构 TBD\n","date":"2014-02-18T21:07:00Z","permalink":"https://www.4async.com/2014/02/2014-02-18-benchmark-python/","title":"Python 性能技巧 (一)"},{"content":"作为一个土逼，我之前写个程序都是臃肿的，后来还被人嘲笑真不是一个那啥的程序。比如说关于算签名的问题，这个在很多 Oauth 环境下都会很常见的算法我写起来就很挫。后来找了一个更好的方法来解决这个问题：\n# 假设所有的都在一个 dict 中 items = adict.items() items.sort() sha1=hashlib.sha1() map(sha1.update, items) hashcode=sha1.hexdigest() if hashcode == signature: return True 不知道 Map 的用法，真的很挫\u0026hellip;.\n","date":"2014-02-17T17:12:00Z","permalink":"https://www.4async.com/2014/02/2014-02-17-create-sign-quickly-python/","title":"Python 下快速创建签名"},{"content":"之前一直没有太过于关注过单元测试，感觉自己程序代码写了足够的实现逻辑和处理流程就应该完全可以满足需求，但是事实上在实现具体功能时，还是存在一些这样那样的问题，甚至通常情况下都是在代码写完可以跑通运行时发现的问题，通常这个时候已经很晚了。大家可以翻下我在 Github 上开放出来的大部分代码应该都存在一些或多或少的小问题，这些问题虽不致命，但是却是也是很烦躁的事情。\n什么是单元测试 单元测试（unit testing），是指对软件中的最小可测试单元进行检查和验证。对于单元测试中单元的含义，一般来说，要根据实际情况去判定其具体含义，如 C 语言中单元指一个函数，Java 里单元指一个类，图形化的软件中可以指一个窗口或一个菜单等。总的来说，单元就是人为规定的最小的被测功能模块。单元测试是在软件开发过程中要进行的最低级别的测试活动，软件的独立单元将在与程序的其他部分相隔离的情况下进行测试。(via 百度百科)\n我对单元测试的理解，单元测试应该是保证程序功能模块的代码安全性和健壮性的保障性测试，主要是开发人员在对自己开发的代码进行测试。被测试模块可能包括多个功能点，在做测试时，设计测试用例要覆盖这些功能点，以保证这些功能点经过测试。很多文章在介绍单元测试常常会介绍一些 100% 的覆盖，考虑到一些异常情况情况的情况下，我个人觉得应该是覆盖异常的情况更多一点，分支覆盖一般常见参数传入测试功能是否正常，另外还是需要保证一些以前的 Bug 并不会出现的测试用例。\n几种常见语言的 Unit Test 先说即种的，一种是 Python 的，一种是 Golang，还有一种是 Objective-C 的。\nPython Python 提供了unittest 库，用于进行单元测试。这个详细说一下，剩下的两个都有现成的代码 :)。\n下面是一个对 random 模块的单元测试代码，测试了 shuffle, choice 和 sample 函数功能，测试函数以 test 开头，并且没有返回和传入参数。\nimport random #被测试模块 import unittest class TestSequenceFunctions(unittest.TestCase): def setUp(self): ``` 测试初始化 ``` self.seq = range(10) pass def tearDown(self): ``` 测试清理 ``` pass def test_shuffle(self): # make sure the shuffled sequence does not lose any elements random.shuffle(self.seq) self.seq.sort() self.assertEqual(self.seq, range(10)) # should raise an exception for an immutable sequence self.assertRaises(TypeError, random.shuffle, (1,2,3)) def test_choice(self): element = random.choice(self.seq) self.assertTrue(element in self.seq) def test_sample(self): with self.assertRaises(ValueError): random.sample(self.seq, 20) for element in random.sample(self.seq, 5): self.assertTrue(element in self.seq) if __name__ == \u0026#39;__main__\u0026#39;: unittest.main() Golang Go 语言提供了 testing 包 和 go test 命令 用于单元测试。\n值得注意的是 Golang 的单元测试是和被测单元在相同文件夹下，使用 xxxx_test.go 方式命名文件，并且要求测试方法以 Test 开头。\n比如 md5 官方库的单元测试就可以在 http://golang.org/src/pkg/crypto/md5/ 找到。\nObjective-C Xcode 原生内置了一个单元测试工具，在创建工程时勾选即可。只说说这个吧。生成测试用例是也是已 test 开头的函数，在编译时会自动进行。\n另外，也有人分享了一些 单元测试的例子 可供参考。\n","date":"2014-01-26T16:21:00Z","permalink":"https://www.4async.com/2014/01/2014-01-26-unit-test/","title":"关于写程序的一点想法：单元测试"},{"content":"OSX 下面添加开机选项怎么加呢？这是个问题。系统提供了一个 LaunchDaemon 的玩意，用于提供这个功能。相关的命令是 launchd 和 launchctl 两个命令。\nlaunch daemon 的配置文件是保存在 / System/Library/LaunchDaemons / 和 / Library/LaunchDaemons / 文件夹下。如果需要指定特定用户的配置，则可以放在~/Library/LaunchAgents、/Library/LaunchAgents 或者 / System/Library/LaunchAgents 下。区别为，是否用户登录之后执行还是内核初始化完成后执行。\n具体的执行命令很简单\nsudo launchctl load \u0026lt;plist file\u0026gt; #加载配置 sudo launchctl unload \u0026lt;file\u0026gt; sudo launchctl start \u0026lt;bundleid\u0026gt; sudo launchctl stop \u0026lt;bundleid\u0026gt; 下面是一个配置的具体示例，用 shadowsocks 为例：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34;encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC\u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34;\u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;Label\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;com.example.shadowsocks\u0026lt;/string\u0026gt;\u0026lt;!-- 这个名字必须和文件名一致 --\u0026gt; \u0026lt;key\u0026gt;ProgramArguments\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;/usr/local/bin/sslocal\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;-c\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;/Users/charles/Projects/shadowsocks-config/config.json\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;key\u0026gt;KeepAlive\u0026lt;/key\u0026gt;\u0026lt;!-- 后台保持运行 --\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;key\u0026gt;RunAtLoad\u0026lt;/key\u0026gt;\u0026lt;!-- 加载时候运行 --\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;key\u0026gt;UserName\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;charles\u0026lt;/string\u0026gt;\u0026lt;!-- 执行用户 --\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; 这个我们自己用户用的，放在~/Library/LaunchAgents 这个文件夹里。执行\nlaunchctl load ~/Library/LaunchAgents/com.example.shadowsocks.plist 即可。\n如果出现错误，则可以使用 plutil -lint 先判断文件格式是否正确。\n参考文档\n[1] Daemons and Services Programming Guide\n","date":"2014-01-23T15:00:00Z","permalink":"https://www.4async.com/2014/01/2014-01-23-launch-daemons-configure-mac-os-x/","title":"通过 LaunchDaemon 实现 OSX 下的自动启动"},{"content":"[UPDATE 2014-01-23]\n解决 pyOpenSSL 的问题也不一定非得使用升级 OpenSSL 的方式，由于 OpenSSL 关联面太广，实际上可以采用安装新版 OpenSSL，在安装 pyOpenSSL 的时候指定 include 和 libs 的方式来指定使用版本的 OpenSSL。具体改一下 setup.py 中的具体对应参数即可。\nPSS: 重要的放在开头，升级 OpenSSL 对系统影响较大，比如 OpenSSH 无法正常工作，算是得到的一个教训，记得同步更新相关使用到 OpenSSL 的软件\n之前比较挫的很少在 64 位系统上编译东西，经常用的服务器都是 x86 的 VPS。今天因为项目需要安装 mitmproxy。这玩意对 pyOpenSSL 的需求是 0.13 以上版本，隐性要求 OpenSSL 为 0.9.8f 及以上版本。怎奈何服务器上是 0.9.8e 版本。因此需要对 OpenSSL 进行升级。因为各种原因，升级是在源码编译状态下进行的。\n编译倒是挺顺畅的，./config 一下，然后直接 make \u0026amp;\u0026amp; make install，替换下系统的头文件和 lib 文件。\n然后开始安装 pyOpenSSL。这步也没什么错误提示，如果安装出错，那么肯定是 OpenSSL 的版本不对。\n然后执行以下 mitmproxy，提示 pyOpenSSL 的 SSL.so 无法找 “SSL_get_servername” 的引用。What？编译没问题运行却提示出错了，orz。\nldd SSL.so 看下该文件的引用关系，发现引用了 lib64 的 libssl.so.6 和 libcrypto.so.6 文件，这两个文件是 openssl 的 shared library。切换到默认的 OpenSSL 目录下 / usr/local/ssl/lib / 文件下，发现并没有生成 so 文件。原因很明显了，是因为生成 OpenSSL 的方式是采用默认的 static 方式。\n./config shared 这样就没有问题了。\n另外还有一个值得注意的地方是在 64 位系统下默认的 lib 目录是 lib64 而不是 lib，最开始我还替换错了，还好及时发现了。\nPS: 每次 Markdown 写文章，带下划线的总是忘记该，要回来修一下，蛋疼。\n","date":"2014-01-15T18:05:00Z","permalink":"https://www.4async.com/2014/01/2014-01-15-openssl-and-pyopenssl/","title":"OpenSSL 和 pyOpenSSL 那点事"},{"content":"权限的那点事儿 检查程序问题，除了配置文件有没有错以外，还需要检查一下权限什么的原因。除了用户组以外，还需要关注文件描述的配置。两个命令分别是\nchown chmod 多留点心眼总没坏处\nldconfig 的问题 经常会编译一些常用的库文件供其他的文件使用。比如 luajit，今天安装完成之后，编译了一个依赖的程序，但是执行程序却发现提示对应的 so 文件无法找到。\n后来我灵机一动，虽然 so 文件在 ldcache 中了，但是重新使用 ldconfig 命令就重新 recache 一下就行了。\n看来不见得所有的都那么靠谱就是了。之前编译 zmap 也遇到了这个问题，一直很奇怪 json 库的 so 文件在 ldconf 文件中，为什么执行 zmap 却提示无法找到，看来也是相同的原因了。\n","date":"2014-01-12T00:20:00Z","permalink":"https://www.4async.com/2014/01/2014-01-12-something-details/","title":"今天学到的几件事"},{"content":"现在来写 cross_fuzz 似乎是有点晚了，毕竟基本上很多时候 cross_fuzz 已经广泛应用于 DOM fuzz 中了。比如 Chromium 中就内置了 cross_fuzz 工具。不过说回来，毕竟就是 Google 自己人做的不是。\n最新的代码 (2011 年更新) 可以在：http://lcamtuf.coredump.cx/cross_fuzz/ 找到。\n基本设计逻辑 cross_fuzz 就是一个 HTML 文件，因此看起来其实并不麻烦：简单的说，就是不停的打开两个窗口显示不同的 HTML/XHTML/SVG/swf 等等内容（当然，可以自己添加），通过不同的 DOM 操作（访问对象、回收操作）等等尝试出发 UAF 之类的问题。\n1. Open two windows with documents of any (DOM-enabled) type. Simple HTML, XHTML, and SVG documents are randomly selected as targets by default – although any other, possibly plugin-supported formats could be targeted instead. 2. Crawl DOM hierarchy of the first document, collecting encountered object references for later reuse. Visited objects and collected references are tagged using an injected property to avoid infinite recursion; a secondary blacklist is used to prevent navigating away or descending into the master window. Critically, random shuffling and recursion fanout control are used to ensure good coverage. 3. Repeat DOM crawl, randomly tweaking encountered object properties by setting them to a one of the previously recorded references (or, with some probability, to one of a handful of hardcoded “interesting” values). 4. Repeat DOM crawl, randomly calling encountered object methods. Call parameters are synthesized using collected references and “interesting” values, as noted above. If a method returns an object, its output is subsequently crawled and tweaked in a similar manner. 5. Randomly destroy first document using one of the several possible methods, toggle garbage collection. 6. Perform the same set of crawl \u0026amp; tweak operations for the second document, but use references collected from the first document for overwriting properties and calling methods in the second one. 7. Randomly destroy document windows, carry over a percentage of collected references to the next fuzzing cycle. 使用方法 使用很简单，以 chrome 为例：\nchrome --disable-popup-blocking --no-first-run --user-data-dir=$TEMPDIR --allow-file-access-from-files --noerrdialogs --disable-hang-monitor file://`pwd`/third_party/cross_fuzz/cross_fuzz_randomized_20110105_seed.html#1234 其中 1234 是随机的 seed，可以自行更换。ie 也是相同的使用方法，记得允许弹出窗口即可。\n实际运行效果 简单拿着跑了一下 XP/IE8，还是有一些少量 crash，并不是单纯的 IE 问题，但是说明发现效果还是不错的，对比了一下之前的 IE7 xday，恩，效果还是差一些才是。\n","date":"2014-01-07T15:00:00Z","permalink":"https://www.4async.com/2014/01/2014-01-07-web-browser-cross-fuzz/","title":"浏览器 fuzz 工具 cross_fuzz"},{"content":"前面看过的 NSConcreteGobalBlock 其实在实际应用里是并不是太多的情况，相对来说，接下来要提到的 NSConcreteStackBlock 则是会经常遇到的情况。\n从源码看 NSConcreteStackBlock 还是先从代码来看：\n#include \u0026lt;stdio.h\u0026gt; int main(){ int a = 1; int (^block)(void) = ^{printf(\u0026#34;%d\\n\u0026#34;, a); return a; }; int b = block(); printf(\u0026#34;%d\\n\u0026#34;, b); return 0; } 翻译成 C++ 代码则是：\nstruct __block_impl { void *isa; int Flags; int Reserved; void *FuncPtr; }; struct __main_block_impl_0 { struct __block_impl impl; struct __main_block_desc_0* Desc; int a; __main_block_impl_0(void *fp, struct __main_block_desc_0 *desc, int _a, int flags=0) : a(_a) { impl.isa = \u0026amp;_NSConcreteStackBlock; impl.Flags = flags; impl.FuncPtr = fp; Desc = desc; } }; static int __main_block_func_0(struct __main_block_impl_0 *__cself) {int a = __cself-\u0026gt;a; // bound by copy printf(\u0026#34;%d\\n\u0026#34;, a); return a; } static struct __main_block_desc_0 { size_t reserved; size_t Block_size; } __main_block_desc_0_DATA = {0, sizeof(struct __main_block_impl_0)}; int main(){ int a = 1; int (*block)(void) = (int (*)())\u0026amp;__main_block_impl_0((void *)__main_block_func_0, \u0026amp;__main_block_desc_0_DATA, a); int b = ((int (*)(__block_impl *))((__block_impl *)block)-\u0026gt;FuncPtr)((__block_impl *)block); printf(\u0026#34;%d\\n\u0026#34;, b); return 0; } 代码具体和 NSConcreteGobalBlock 相比还是有一些变化，如，由于使用局部变量 a，在 block 中则是对 a 变量进行了拷贝，也就是说，在 block 中修改某一变量并不会影响 block 之外的相同变量。另外一个是关于返回值的定义，对函数的强制类型转化而来，关于这一块的东西，会在逆向分析时在进行对比。对于调用传入的指针 cself，则是在对 main_block_impl_0 结构的转化，注意在定义时，添加了 a 的传入参数，从而获取到参数内容。\n从逆向看 NSConcreteStackBlock x86_64:\n__text:0000000100000E60 _main proc near __text:0000000100000E60 __text:0000000100000E60 var_4C = dword ptr -4Ch __text:0000000100000E60 var_48 = qword ptr -48h __text:0000000100000E60 var_3C = dword ptr -3Ch __text:0000000100000E60 var_38 = qword ptr -38h __text:0000000100000E60 var_30 = dword ptr -30h __text:0000000100000E60 var_2C = dword ptr -2Ch __text:0000000100000E60 var_28 = qword ptr -28h __text:0000000100000E60 var_20 = qword ptr -20h __text:0000000100000E60 var_18 = dword ptr -18h __text:0000000100000E60 var_10 = qword ptr -10h __text:0000000100000E60 var_8 = dword ptr -8 __text:0000000100000E60 var_4 = dword ptr -4 __text:0000000100000E60 __text:0000000100000E60 push rbp __text:0000000100000E61 mov rbp, rsp __text:0000000100000E64 sub rsp, 50h __text:0000000100000E68 lea rdi, aD ; \u0026#34;%d\\n\u0026#34; __text:0000000100000E6F lea rax, [rbp+var_38] __text:0000000100000E73 lea rcx, ___block_descriptor_tmp __text:0000000100000E7A lea rdx, ___main_block_invoke __text:0000000100000E81 mov rsi, cs:__NSConcreteStackBlock_ptr __text:0000000100000E88 mov [rbp+var_4], 0 __text:0000000100000E8F mov [rbp+var_8], 1 __text:0000000100000E96 mov [rbp+var_38], rsi __text:0000000100000E9A mov [rbp+var_30], 40000000h __text:0000000100000EA1 mov [rbp+var_2C], 0 __text:0000000100000EA8 mov [rbp+var_28], rdx __text:0000000100000EAC mov [rbp+var_20], rcx __text:0000000100000EB0 mov r8d, [rbp+var_8] __text:0000000100000EB4 mov [rbp+var_18], r8d __text:0000000100000EB8 mov [rbp+var_10], rax __text:0000000100000EBC mov rax, [rbp+var_10] __text:0000000100000EC0 mov rcx, rax __text:0000000100000EC3 mov [rbp+var_48], rdi __text:0000000100000EC7 mov rdi, rcx __text:0000000100000ECA call qword ptr [rax+10h] __text:0000000100000ECD mov [rbp+var_3C], eax __text:0000000100000ED0 mov esi, [rbp+var_3C] __text:0000000100000ED3 mov rdi, [rbp+var_48] ; char * __text:0000000100000ED7 mov al, 0 __text:0000000100000ED9 call _printf __text:0000000100000EDE mov esi, 0 __text:0000000100000EE3 mov [rbp+var_4C], eax __text:0000000100000EE6 mov eax, esi __text:0000000100000EE8 add rsp, 50h __text:0000000100000EEC pop rbp __text:0000000100000EED retn __text:0000000100000EED _main endp ARM:\n__text:00002F0C _main __text:00002F0C __text:00002F0C var_34 = -0x34 __text:00002F0C var_30 = -0x30 __text:00002F0C var_2C = -0x2C __text:00002F0C var_28 = -0x28 __text:00002F0C var_24 = -0x24 __text:00002F0C var_20 = -0x20 __text:00002F0C var_1C = -0x1C __text:00002F0C var_18 = -0x18 __text:00002F0C var_14 = -0x14 __text:00002F0C var_10 = -0x10 __text:00002F0C var_C = -0xC __text:00002F0C var_8 = -8 __text:00002F0C __text:00002F0C PUSH {R4,R7,LR} __text:00002F0E ADD R7, SP, #4 __text:00002F10 SUB SP, SP, #0x30 __text:00002F12 MOV R0, #(aD - 0x2F1E) ; \u0026#34;%d\\n\u0026#34; __text:00002F1A ADD R0, PC ; \u0026#34;%d\\n\u0026#34; __text:00002F1C ADD R1, SP, #0x34+var_28 __text:00002F1E MOV R2, #(___block_descriptor_tmp - 0x2F2A) ; ___block_descriptor_tmp __text:00002F26 ADD R2, PC ; ___block_descriptor_tmp __text:00002F28 MOV R3, #(___main_block_invoke+1 - 0x2F34) ; ___main_block_invoke __text:00002F30 ADD R3, PC ; ___main_block_invoke __text:00002F32 MOV R9, #0 __text:00002F3A MOV R12, #0x40000000 __text:00002F42 MOV LR, #(__NSConcreteStackBlock_ptr - 0x2F4E) ; __NSConcreteStackBlock_ptr __text:00002F4A ADD LR, PC ; __NSConcreteStackBlock_ptr __text:00002F4C LDR.W LR, [LR] ; __NSConcreteStackBlock __text:00002F50 MOVS R4, #1 __text:00002F56 STR.W R9, [SP,#0x34+var_8] __text:00002F5A STR R4, [SP,#0x34+var_C] __text:00002F5C STR.W LR, [SP,#0x34+var_28] __text:00002F60 STR.W R12, [SP,#0x34+var_24] __text:00002F64 STR.W R9, [SP,#0x34+var_20] __text:00002F68 STR R3, [SP,#0x34+var_1C] __text:00002F6A STR R2, [SP,#0x34+var_18] __text:00002F6C LDR R2, [SP,#0x34+var_C] __text:00002F6E STR R2, [SP,#0x34+var_14] __text:00002F70 STR R1, [SP,#0x34+var_10] __text:00002F72 LDR R1, [SP,#0x34+var_10] __text:00002F74 MOV R2, R1 __text:00002F76 LDR R1, [R1,#0xC] __text:00002F78 STR R0, [SP,#0x34+var_30] __text:00002F7A MOV R0, R2 __text:00002F7C BLX R1 __text:00002F7E STR R0, [SP,#0x34+var_2C] __text:00002F80 LDR R1, [SP,#0x34+var_2C] __text:00002F82 LDR R0, [SP,#0x34+var_30] ; char * __text:00002F84 BLX _printf __text:00002F88 MOVS R1, #0 __text:00002F8E STR R0, [SP,#0x34+var_34] __text:00002F90 MOV R0, R1 __text:00002F92 ADD SP, SP, #0x30 __text:00002F94 POP {R4,R7,PC} __text:00002F94 ; End of function _main ","date":"2014-01-03T15:00:00Z","permalink":"https://www.4async.com/2014/01/2014-01-03-something-about-block-part-two/","title":"谈谈 block Part 2"},{"content":"Block 概述 Block 是 Apple 提供的一种闭包实现，比较方便实现一些函数嵌套实现的功能。Block 分为三种类型：\nNSConcreteGlobalBlock NSConcreteStackBlock NSConcreteMallocBlock 这三种类型分别对应了三种不同的 Block 类型，值得注意的是，在启用了 ARC 之后，NSConcreteStackBlock 会转换类型为 NSConcreteMallocBlock。\n换种比较容易的理解的说法：\n在非 ARC 下，LLVM 编译下没有访问局部变量的 Block 应该是 NSConcreteGlobalBlock 类型的，访问了局部变量的 Block 是 NSConcreteStackBlock 类型的。\n在 ARC 下，访问了局部变量的 Block 是 NSConcreteMallocBlock 类型的，未访问局部变量的 Block 是 NSConcreteGlobalBlock 类型的。\n具体的实现代码可以参考 llvm 的 BlockRuntime。\n对于研究 block 的具体代码翻译，则可以使用 clang 的 rewrite-objc 功能，将 OC 文件转换成 cpp 文件。比如：\nclang -rewrite-objc blocktest.c 这样就可以生成对应的 blocktest.cpp 文件。\n从源码看 NSConcreteGlobalBlock 首先先看一下 NSConcreteGlobalBlock 的代码，从简单的开始：\n#include \u0026lt;stdio.h\u0026gt; int main() {^{printf(\u0026#34;Hello World!\\n\u0026#34;);}(); return 0; } 翻译之后的代码（精简仅包含所有必须代码）：\nstruct __block_impl { void *isa; int Flags; int Reserved; void *FuncPtr; }; struct __main_block_impl_0 { struct __block_impl impl; struct __main_block_desc_0* Desc; __main_block_impl_0(void *fp, struct __main_block_desc_0 *desc, int flags=0) { impl.isa = \u0026amp;_NSConcreteStackBlock; impl.Flags = flags; impl.FuncPtr = fp; Desc = desc; } }; static void __main_block_func_0(struct __main_block_impl_0 *__cself) {printf(\u0026#34;Hello World!\\n\u0026#34;);} static struct __main_block_desc_0 { size_t reserved; size_t Block_size; } __main_block_desc_0_DATA = {0, sizeof(struct __main_block_impl_0)}; int main() {(void (*)())\u0026amp;__main_block_impl_0((void *)__main_block_func_0, \u0026amp;__main_block_desc_0_DATA)(); return 0; } 主要的 Block 代码被翻译成为了一个指针函数调用，__main_block_impl_0 是一个定义过的结构，每个 Block 的类型是固定的。其中 isa 是指明的 Block 类型，FuncPtr 则是函数指针。值得注意的是，虽然 impl.isa 填写的是 NSConcreteStackBlock，但是实际在编译过程中，这里还是会被处理成为 NSConcreteGlobalBlock。\n从反汇编看 NSConcreteGlobalBlock (x86_64):\n__text:0000000100000EC0 _main proc near __text:0000000100000EC0 __text:0000000100000EC0 var_8 = dword ptr -8 __text:0000000100000EC0 var_4 = dword ptr -4 __text:0000000100000EC0 __text:0000000100000EC0 push rbp __text:0000000100000EC1 mov rbp, rsp __text:0000000100000EC4 sub rsp, 10h __text:0000000100000EC8 mov eax, 0 __text:0000000100000ECD lea rcx, ___block_literal_global __text:0000000100000ED4 mov [rbp+var_4], 0 __text:0000000100000EDB mov rdi, rcx __text:0000000100000EDE mov [rbp+var_8], eax __text:0000000100000EE1 call cs:off_100001050 __text:0000000100000EE7 mov eax, [rbp+var_8] __text:0000000100000EEA add rsp, 10h __text:0000000100000EEE pop rbp __text:0000000100000EEF retn __text:0000000100000EEF _main endp (ARM):\n__text:00002F70 PUSH {R7,LR} __text:00002F72 MOV R7, SP __text:00002F74 SUB SP, SP, #8 __text:00002F76 MOVS R0, #0 __text:00002F7C MOV R1, #(___block_literal_global - 0x2F88) ; ___block_literal_global __text:00002F84 ADD R1, PC ; ___block_literal_global __text:00002F86 MOV R2, R1 __text:00002F88 STR R0, [SP,#0x10+var_C] __text:00002F8A LDR R1, [R1,#(off_3028 - 0x301C)] __text:00002F8C STR R0, [SP,#0x10+var_10] __text:00002F8E MOV R0, R2 __text:00002F90 BLX R1 ; ___main_block_invoke __text:00002F92 LDR R0, [SP,#0x10+var_10] __text:00002F94 ADD SP, SP, #8 __text:00002F96 POP {R7,PC} 会发现在调用 block 时是采用的直接调用的方式 (call/blx)，由于 NSConcreteGlobalBlock 没有传入参数，因此这个也就是关键在参数处理方式上。\n在 X64 平台上，off_100001050 中保存的就是 main_block_invoke（也就是我们使用的 block）的地址，而在 ARM 平台上，R1（main_block_invoke）地址是在 LDR R1, [R1,#(off_3028 - 0x301C)] 这一句赋值而来，其中 off_3028 指向的就是 main_block_invoke 的地址。\n\u0026lt;未完待续\u0026gt;\n","date":"2014-01-02T14:31:00Z","permalink":"https://www.4async.com/2014/01/2014-01-02-something-about-block-part-one/","title":"谈谈 block Part 1"},{"content":"很早之前就写过一个 lnmp 的配置，后来因为发现了更好的 lnmp.org，因此就改用了 lnmp 工具。不过已经很长时间军哥豆没更新这个工具了，很多需要进行更新，因此这也是本文诞生的原因了。全当做自己的记录吧。\n{% gist 8206547 %}\n适用于 Debian 系统，没准 Ubuntu 系统也是适用的。\n","date":"2014-01-01T18:31:00Z","permalink":"https://www.4async.com/2014/01/2014-01-01-build-lnmp-on-debian/","title":"Debian 系统下配置 lnmp"},{"content":"一直以来，看着 SecureCRT 有一个十分有用的功能，clone session 可以免输入帐号密码重新开启一个 ssh 窗口，这样就在不方便使用公钥登录的时候非常有用。\n在 Mac 下面如何实现这个功能呢？也是十分的简单：\n进入~/.ssh 目录，创建名为 config 的文件（有则编辑）：\n添加以下内容\nhost * ControlMaster auto ControlPath ~/.ssh/master-%r@%h:%p 这样在创建一个连接之后，再次连接就可以实现免输入帐号了。\n但是缺点是，第一个连接一旦关闭，所有的连接都会被关闭。\n具体的 config 文件内容，可以参考 http://linux.die.net/man/5/ssh_config。\n","date":"2013-12-27T19:06:00Z","permalink":"https://www.4async.com/2013/12/2013-12-27-mac-ssh-scurecrt-clone-session/","title":"Mac 下实现类 SecureCRT 的 session clone 功能"},{"content":"第一次试用 Github Pages 来做页面，感觉挺不错的，先试试咯。\ndef print_hi(name) print \u0026#34;hi\u0026#34; print_hi() ","date":"2013-12-27T15:28:30Z","permalink":"https://www.4async.com/2013/12/2013-12-27-first-post/","title":"测试页面"}]