<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content='如果你是第一次开始研究如何使用开源大语言模型(LLM)测试 GenerativeAI 时，一开始所有的信息一股脑在你的眼前，令人望而生畏。互联网上存在着来自许多不同来源的大量碎片信息，使得快速启动项目变得困难。
这篇文章的目标是提供一篇简易上手的文章，帮助你使用名为 Ollama 的模型的启动程序在本地设置和运行开源 AI 模型，当然，这些步骤也同样可以让你在你的服务器上运行它。
什么是 Ollama Ollama 是一款帮助我们在本地机器上运行大型语言模型的工具，它让你在本地测试 LLM 变得更加容易。Ollama 提供了本地命令行和 API 调用多种方式进行交互。如果你是想快速测试，那么 CLI 则是一个非常不错的方式；如果你是想开始一个产品，你也可以选择试用/api/chat进行开发一个应用。
Ollama 分为两个部分：一个是运行 LLM 的服务端，另外一个则是可选组件：用于和服务端和 LLM进行交互的 CLI。
安装 Ollama Ollama 官方提供了安装包用于 MacOS/Linux/Windows 下载。其中 Windows 支持截止到目前(2024/02/27)为止，还是预览支持，可能存在问题。因此这里我们演示使用 MacOS 安装这个应用。
下载安装 下载完成后，解压这个 zip 文件即可得到Ollama 的应用程序，你可以把它拖到系统的应用程序文件夹中，双击打开：
如果是第一次打开，会遇到安全提示，选择打开即可。
接下来需要安装 Ollama 的命令行工具，这样你就可以在命令行中访问LLM 了。
命令行安装 当然，如果你安装了[homebrew](https://brew.sh/)包管理工具，也可以使用下面的方式快速安装 Ollama：
brew install ollama 命令行方式安装时，需要额外注意我们无法像图形界面进行后台服务启动。你需要施工执行下面的命令启动Ollama 服务端：
ollama serve 试用 Ollama 现在你已经安装完成 Ollama 了，接下来让我们运行一个基础的模型试用一下。Ollama 支持了很多的开源模型，包括 Google 最新开源的 Gemma。不过这里我推荐使用Mistral 7B 模型。我们可以使用下面的命令快速安装对应模型：
ollama pull mistral 这会安装 Mistral 7B 模型，这是一个效率很高，效果不错的模型。这个命令通常会下载几个 G 到几十个 G 的模型到本地，请注意确保网速和本地磁盘空间足以安装模型。'><title>使用 Ollama 快速部署本地开源大语言模型</title>
<link rel=canonical href=https://www.4async.com/2024/02/deploy-local-llm-using-ollama/><link rel=stylesheet href=../../../scss/style.min.8191399262444ab68b72a18c97392f5349be20a1615d77445be51e974c144cff.css><meta property='og:title' content='使用 Ollama 快速部署本地开源大语言模型'><meta property='og:description' content='如果你是第一次开始研究如何使用开源大语言模型(LLM)测试 GenerativeAI 时，一开始所有的信息一股脑在你的眼前，令人望而生畏。互联网上存在着来自许多不同来源的大量碎片信息，使得快速启动项目变得困难。
这篇文章的目标是提供一篇简易上手的文章，帮助你使用名为 Ollama 的模型的启动程序在本地设置和运行开源 AI 模型，当然，这些步骤也同样可以让你在你的服务器上运行它。
什么是 Ollama Ollama 是一款帮助我们在本地机器上运行大型语言模型的工具，它让你在本地测试 LLM 变得更加容易。Ollama 提供了本地命令行和 API 调用多种方式进行交互。如果你是想快速测试，那么 CLI 则是一个非常不错的方式；如果你是想开始一个产品，你也可以选择试用/api/chat进行开发一个应用。
Ollama 分为两个部分：一个是运行 LLM 的服务端，另外一个则是可选组件：用于和服务端和 LLM进行交互的 CLI。
安装 Ollama Ollama 官方提供了安装包用于 MacOS/Linux/Windows 下载。其中 Windows 支持截止到目前(2024/02/27)为止，还是预览支持，可能存在问题。因此这里我们演示使用 MacOS 安装这个应用。
下载安装 下载完成后，解压这个 zip 文件即可得到Ollama 的应用程序，你可以把它拖到系统的应用程序文件夹中，双击打开：
如果是第一次打开，会遇到安全提示，选择打开即可。
接下来需要安装 Ollama 的命令行工具，这样你就可以在命令行中访问LLM 了。
命令行安装 当然，如果你安装了[homebrew](https://brew.sh/)包管理工具，也可以使用下面的方式快速安装 Ollama：
brew install ollama 命令行方式安装时，需要额外注意我们无法像图形界面进行后台服务启动。你需要施工执行下面的命令启动Ollama 服务端：
ollama serve 试用 Ollama 现在你已经安装完成 Ollama 了，接下来让我们运行一个基础的模型试用一下。Ollama 支持了很多的开源模型，包括 Google 最新开源的 Gemma。不过这里我推荐使用Mistral 7B 模型。我们可以使用下面的命令快速安装对应模型：
ollama pull mistral 这会安装 Mistral 7B 模型，这是一个效率很高，效果不错的模型。这个命令通常会下载几个 G 到几十个 G 的模型到本地，请注意确保网速和本地磁盘空间足以安装模型。'><meta property='og:url' content='https://www.4async.com/2024/02/deploy-local-llm-using-ollama/'><meta property='og:site_name' content="ipfans's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='GenerativeAI'><meta property='article:tag' content='LLM'><meta property='article:tag' content='Python'><meta property='article:published_time' content='2024-02-28T17:39:00+00:00'><meta property='article:modified_time' content='2024-02-28T17:39:00+00:00'><meta property='og:image' content='https://www.4async.com/2024/02/deploy-local-llm-using-ollama/cover.png'><meta name=twitter:site content="@janxin"><meta name=twitter:creator content="@janxin"><meta name=twitter:title content="使用 Ollama 快速部署本地开源大语言模型"><meta name=twitter:description content="如果你是第一次开始研究如何使用开源大语言模型(LLM)测试 GenerativeAI 时，一开始所有的信息一股脑在你的眼前，令人望而生畏。互联网上存在着来自许多不同来源的大量碎片信息，使得快速启动项目变得困难。
这篇文章的目标是提供一篇简易上手的文章，帮助你使用名为 Ollama 的模型的启动程序在本地设置和运行开源 AI 模型，当然，这些步骤也同样可以让你在你的服务器上运行它。
什么是 Ollama Ollama 是一款帮助我们在本地机器上运行大型语言模型的工具，它让你在本地测试 LLM 变得更加容易。Ollama 提供了本地命令行和 API 调用多种方式进行交互。如果你是想快速测试，那么 CLI 则是一个非常不错的方式；如果你是想开始一个产品，你也可以选择试用/api/chat进行开发一个应用。
Ollama 分为两个部分：一个是运行 LLM 的服务端，另外一个则是可选组件：用于和服务端和 LLM进行交互的 CLI。
安装 Ollama Ollama 官方提供了安装包用于 MacOS/Linux/Windows 下载。其中 Windows 支持截止到目前(2024/02/27)为止，还是预览支持，可能存在问题。因此这里我们演示使用 MacOS 安装这个应用。
下载安装 下载完成后，解压这个 zip 文件即可得到Ollama 的应用程序，你可以把它拖到系统的应用程序文件夹中，双击打开：
如果是第一次打开，会遇到安全提示，选择打开即可。
接下来需要安装 Ollama 的命令行工具，这样你就可以在命令行中访问LLM 了。
命令行安装 当然，如果你安装了[homebrew](https://brew.sh/)包管理工具，也可以使用下面的方式快速安装 Ollama：
brew install ollama 命令行方式安装时，需要额外注意我们无法像图形界面进行后台服务启动。你需要施工执行下面的命令启动Ollama 服务端：
ollama serve 试用 Ollama 现在你已经安装完成 Ollama 了，接下来让我们运行一个基础的模型试用一下。Ollama 支持了很多的开源模型，包括 Google 最新开源的 Gemma。不过这里我推荐使用Mistral 7B 模型。我们可以使用下面的命令快速安装对应模型：
ollama pull mistral 这会安装 Mistral 7B 模型，这是一个效率很高，效果不错的模型。这个命令通常会下载几个 G 到几十个 G 的模型到本地，请注意确保网速和本地磁盘空间足以安装模型。"><meta name=twitter:card content="summary"><meta name=twitter:image content='https://www.4async.com/2024/02/deploy-local-llm-using-ollama/cover.png'><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-33841232-2","auto"),ga("send","pageview"))</script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=../../../><img src=../../../img/avatar_huae056372e08c63f80862e9295e80f934_12087_300x0_resize_q75_box.jpeg width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=../../../>ipfans's Blog</a></h1><h2 class=site-description></h2></div></header><ol class=menu id=main-menu><li><a href=../../../><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=../../../about><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=../../../archives><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=../../../atom.xml><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-rss" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="5" cy="19" r="1"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg>
<span>Feed</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://www.4async.com/ selected></option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#什么是-ollama>什么是 Ollama</a><ul><li><a href=#安装-ollama>安装 Ollama</a></li></ul></li><li><a href=#试用-ollama>试用 Ollama</a><ul><li><a href=#关于-mistral-的一些分享>关于 Mistral 的一些分享</a></li></ul></li><li><a href=#使用ollama-进行开发>使用Ollama 进行开发</a></li><li><a href=#结语>结语</a></li></ul></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=../../../2024/02/deploy-local-llm-using-ollama/><img src=../../../2024/02/deploy-local-llm-using-ollama/cover_hua4e12490b5b2c9e61f4b88388eb5eb25_619303_800x0_resize_box_3.png srcset="../../../2024/02/deploy-local-llm-using-ollama/cover_hua4e12490b5b2c9e61f4b88388eb5eb25_619303_800x0_resize_box_3.png 800w, ../../../2024/02/deploy-local-llm-using-ollama/cover_hua4e12490b5b2c9e61f4b88388eb5eb25_619303_1600x0_resize_box_3.png 1600w" width=800 height=477 loading=lazy alt="Featured image of post 使用 Ollama 快速部署本地开源大语言模型"></a></div><div class=article-details><header class=article-category><a href=../../../categories/genai/>GenAI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=../../../2024/02/deploy-local-llm-using-ollama/>使用 Ollama 快速部署本地开源大语言模型</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Feb 28, 2024</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 2 分钟</time></div></footer></div></header><section class=article-content><p>如果你是第一次开始研究如何使用开源大语言模型(LLM)测试 GenerativeAI 时，一开始所有的信息一股脑在你的眼前，令人望而生畏。互联网上存在着来自许多不同来源的大量碎片信息，使得快速启动项目变得困难。</p><p>这篇文章的目标是提供一篇简易上手的文章，帮助你使用名为 Ollama 的模型的启动程序在本地设置和运行开源 AI 模型，当然，这些步骤也同样可以让你在你的服务器上运行它。</p><h2 id=什么是-ollama>什么是 Ollama</h2><p>Ollama 是一款帮助我们在本地机器上运行大型语言模型的工具，它让你在本地测试 LLM 变得更加容易。Ollama 提供了本地命令行和 API 调用多种方式进行交互。如果你是想快速测试，那么 CLI 则是一个非常不错的方式；如果你是想开始一个产品，你也可以选择试用<code>/api/chat</code>进行开发一个应用。</p><p>Ollama 分为两个部分：一个是运行 LLM 的服务端，另外一个则是可选组件：用于和服务端和 LLM进行交互的 CLI。</p><h3 id=安装-ollama>安装 Ollama</h3><p><a class=link href=https://ollama.com/ target=_blank rel=noopener>Ollama</a> 官方提供了<a class=link href=https://ollama.com/download target=_blank rel=noopener>安装包</a>用于 MacOS/Linux/Windows 下载。其中 Windows 支持截止到目前(2024/02/27)为止，还是预览支持，可能存在问题。因此这里我们演示使用 MacOS 安装这个应用。</p><h4 id=下载安装>下载安装</h4><p>下载完成后，解压这个 zip 文件即可得到Ollama 的应用程序，你可以把它拖到系统的应用程序文件夹中，双击打开：</p><p><img src=https://res.craft.do/user/full/1adcdc5f-9fec-35eb-9cb6-9d25a1858140/doc/AC77186B-279D-42E3-94E6-1210D21445CF/7F776093-3862-4581-8084-8B3BA0E87381_2/xtp78WzaqBigmWTxPFklpEhcyiwIjyJ6WBYdsoYkN6oz/Image.png loading=lazy alt=image></p><p>如果是第一次打开，会遇到安全提示，选择打开即可。</p><p><img src=https://res.craft.do/user/full/1adcdc5f-9fec-35eb-9cb6-9d25a1858140/doc/AC77186B-279D-42E3-94E6-1210D21445CF/47F99D14-8F4F-4C2F-AEED-71C8F8894B2C_2/glL5swE2IRE6DUeLCVYDYywm0ksnNLB1FxhstNkbAGwz/2024-02-27%2011.44.33.png loading=lazy alt=image></p><p>接下来需要安装 Ollama 的命令行工具，这样你就可以在命令行中访问LLM 了。</p><p><img src=https://res.craft.do/user/full/1adcdc5f-9fec-35eb-9cb6-9d25a1858140/doc/AC77186B-279D-42E3-94E6-1210D21445CF/043C01CF-0F92-4B75-A566-43CFDAB1E541_2/rOmbRmEOsdZek1JwIw5fDvoMAy6eUli2mziQFEEWTKwz/2024-02-27%2011.46.18.png loading=lazy alt=image></p><h4 id=命令行安装>命令行安装</h4><p>当然，如果你安装了<code>[homebrew](https://brew.sh/)</code>包管理工具，也可以使用下面的方式快速安装 Ollama：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>brew install ollama
</span></span></code></pre></div><p>命令行方式安装时，需要额外注意我们无法像图形界面进行后台服务启动。你需要施工执行下面的命令启动Ollama 服务端：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ollama serve
</span></span></code></pre></div><h2 id=试用-ollama>试用 Ollama</h2><p>现在你已经安装完成 Ollama 了，接下来让我们运行一个基础的模型试用一下。Ollama 支持了<a class=link href=https://ollama.com/library target=_blank rel=noopener>很多的开源模型</a>，包括 Google 最新开源的 <a class=link href=https://ollama.com/library/gemma target=_blank rel=noopener>Gemma</a>。不过这里我推荐使用<a class=link href=https://ollama.com/library/mistral target=_blank rel=noopener>Mistral</a> 7B 模型。我们可以使用下面的命令快速安装对应模型：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ollama pull mistral
</span></span></code></pre></div><p>这会安装 <a class=link href=https://mistral.ai/news/announcing-mistral-7b/ target=_blank rel=noopener>Mistral 7B 模型</a>，这是一个效率很高，效果不错的模型。这个命令通常会下载几个 G 到几十个 G 的模型到本地，请注意确保网速和本地磁盘空间足以安装模型。</p><p>我们现在假设你已经安装模型成功了，我们可以使用下面的命令，开始进行试用：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ollama run mistral
</span></span></code></pre></div><p>如果你执行这个命令遇到了以下报错，说明你的服务端还没有启动：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ollama run mistral
</span></span><span style=display:flex><span>Error: could <span style=color:#f92672>not</span> connect to ollama app, <span style=color:#f92672>is</span> it running<span style=color:#960050;background-color:#1e0010>?</span>
</span></span></code></pre></div><p>你可以双击打开应用，或者使用下面的命令快速启动服务端：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ollama serve
</span></span></code></pre></div><p>执行试用模型命令之后，可能需要一段时间加载模型。这可能需要一些时间，具体的时间和你的电脑配置相关。</p><p>加载模型成功之后，你可以看到这样的界面：</p><p><img src=https://res.craft.do/user/full/1adcdc5f-9fec-35eb-9cb6-9d25a1858140/doc/AC77186B-279D-42E3-94E6-1210D21445CF/05F167A4-A2FC-467E-9205-4584C52860F6_2/sgvhzmm1SdwZ4mXXcfDegUhcZBbIspt3e8n9aXMp7u4z/2024-02-27%2012.07.31.png loading=lazy alt=image></p><p>让我们提个问题，让它回答一下吧：</p><p><img src=https://res.craft.do/user/full/1adcdc5f-9fec-35eb-9cb6-9d25a1858140/doc/AC77186B-279D-42E3-94E6-1210D21445CF/D0F6AC6A-E212-4E70-B287-EFABA993412E_2/80PEcIxkqC7LXg3sePiYauXNrODYrOFVdIu6ShyauQwz/2024-02-27%2012.11.07.png loading=lazy alt=image></p><p>这个问题可能太简单了，我们让 AI 使用 Python 编写一个快速排序的实现吧：</p><p><img src=https://res.craft.do/user/full/1adcdc5f-9fec-35eb-9cb6-9d25a1858140/doc/AC77186B-279D-42E3-94E6-1210D21445CF/9405054C-4A06-4050-9147-7BCD47CD42D0_2/MN2Qxubvs4GnLsrXw6p308V6GaEsC3cx1jaH6NmClgkz/2024-02-27%2012.10.19.png loading=lazy alt=image></p><h3 id=关于-mistral-的一些分享>关于 Mistral 的一些分享</h3><p>开源的模型有很多，为什么会推荐 Mistral 呢？Mistral 有几个不错的点：虽然是一个 7B 模型，根据一些 benchmark 显示可以堪比 LLaMA 13B 模型；同时因为是 7B 模型，因此在 CPU 推理时性能也还可以接受，如果使用 GPU 推理，则速度非常快速。</p><p><img src=https://res.craft.do/user/full/1adcdc5f-9fec-35eb-9cb6-9d25a1858140/doc/AC77186B-279D-42E3-94E6-1210D21445CF/446F1171-B23D-4553-9D9B-D15074003697_2/LJGEL3OGOyp8I074ggWQRpAPiPgOmBiSQDBNbnlbrhwz/Image.png loading=lazy alt=image></p><p>当然它也有很多缺点，比如因为参数体量的原因，Mistral 的信息量其实非常有限，这意味着如果在实际项目或者把它当作 AI 助手，你可能还需要补充很多的信息给模型它才可能知道。这让 Mistral 可以作为一个非常合适的扩展开发工作的模型。</p><h2 id=使用ollama-进行开发>使用Ollama 进行开发</h2><p>为了快速开发应用，我们选择使用 Python 进行开发。</p><p>让我们从最开始的问答开始：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># llm_chat.py</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>res <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>post(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;http://127.0.0.1:11434/api/chat&#34;</span>,  <span style=color:#75715e># 服务器地址</span>
</span></span><span style=display:flex><span>    json<span style=color:#f92672>=</span>{
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;model&#34;</span>: <span style=color:#e6db74>&#34;mistral&#34;</span>,  <span style=color:#75715e># 使用 mistral 模型</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;stream&#34;</span>: <span style=color:#66d9ef>False</span>,  <span style=color:#75715e># 禁用流式输出</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;messages&#34;</span>: [
</span></span><span style=display:flex><span>            {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;你是谁？&#34;</span>},  <span style=color:#75715e># 用户输入</span>
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>print(res<span style=color:#f92672>.</span>json())
</span></span></code></pre></div><p>接下来，我们执行一下这个 Python 脚本：</p><p><img src=https://res.craft.do/user/full/1adcdc5f-9fec-35eb-9cb6-9d25a1858140/doc/AC77186B-279D-42E3-94E6-1210D21445CF/A42501D2-52B2-4A1E-B27B-2B062D83C639_2/UvnnE5W50UVamcMarYtlC0IifU1WpZqq4tjPau6iJw0z/2024-02-27%2018.15.30.png loading=lazy alt=image></p><p>如果仅仅满足问答需求，可能不足以满足我们在实际项目中的需求。我们通常在程序中集成格式化输出的内容，以方便程序进行解析和后续处理。因此，我们需要通过 Prompt Engineering 方式实现这种目标：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># llm_chat.py</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>res <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>post(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;http://localhost:11434/api/chat&#34;</span>,
</span></span><span style=display:flex><span>    json<span style=color:#f92672>=</span>{
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;model&#34;</span>: <span style=color:#e6db74>&#34;mistral&#34;</span>,  <span style=color:#75715e># 使用 mistral 模型</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;stream&#34;</span>: <span style=color:#66d9ef>False</span>,  <span style=color:#75715e># 禁用流式输出</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;messages&#34;</span>: [
</span></span><span style=display:flex><span>            {
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;&#34;&#34;你是谁？使用yaml格式进行回答，yaml格式如下：
</span></span></span><span style=display:flex><span><span style=color:#e6db74>name: string
</span></span></span><span style=display:flex><span><span style=color:#e6db74>language: string
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>, <span style=color:#75715e># 用户输入</span>
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>print(res<span style=color:#f92672>.</span>json())
</span></span></code></pre></div><p>注意我们在这里要求 LLM 按照 <a class=link href=https://yaml.org/ target=_blank rel=noopener>YAML格式</a>进行输出。这是一种对人和机器都友好的格式。我们提供了具体的格式定义和字段名与类型，防止 LLM 的自我发挥：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>name</span>: <span style=color:#ae81ff>string</span>
</span></span><span style=display:flex><span><span style=color:#f92672>language</span>: <span style=color:#ae81ff>string</span>
</span></span></code></pre></div><p>我们再执行一下这个文件：</p><p><img src=https://res.craft.do/user/full/1adcdc5f-9fec-35eb-9cb6-9d25a1858140/doc/AC77186B-279D-42E3-94E6-1210D21445CF/B96BAF8B-81E5-4882-8586-2ED5AB41CB38_2/CQjf7qvgCQDtXBrgvcGQ6JcDTxxZam1brDX359zx8c0z/2024-02-27%2018.30.07.png loading=lazy alt=image></p><p>我们可以看到 LLM 按照我们的格式要求输出了对应的内容：让我们按照 YAML 格式展示一下输出的内容：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>name</span>: <span style=color:#e6db74>&#34;AI Assistant&#34;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>language</span>: <span style=color:#e6db74>&#34;English&#34;</span>
</span></span></code></pre></div><p>这样就可以借助 LLM 把这些非格式化的内容转化为格式化内容方便我们的应用进行进一步处理。</p><h2 id=结语>结语</h2><p>正如你所看到的，使用 Ollama 快速部署本地开源大语言模型就是那么简单。借助Ollama提供的能力，我们可以轻松地在本地测试开源 AI 模型。这是创建使用真正人工智能的应用程序的第一步。关于后续真正进入真正的 AI 应用开发，后续再更新一些系列文章进一步讲解一下。</p></section><footer class=article-footer><section class=article-tags><a href=../../../tags/generativeai/>GenerativeAI</a>
<a href=../../../tags/llm/>LLM</a>
<a href=../../../tags/python/>Python</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//s1mbily.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2013 -
2024 ipfans's Blog</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.16.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=../../../ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>